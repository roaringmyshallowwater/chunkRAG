[{"content": "大数据技术精  品 系列教材 “1+X” 职业技能等级证书配套系列教材 大 数 据 应 用 开 发(Python)\n\nPython\n\n中文自然语言处理\n\n基础与实战\n\nFundamentals and Practice of\n\nChinese Natural Language Processing with Python\n\n肖刚张良均◎主编\n\n郑鑫标罗惠琳陈晓娜◎副主编\n\n中国工信出版集团   \n\n大数据技术精品系列教材\n\n“1+X” 职业技能等级证书配套系列教材\n\n大数据应用开发(Python)\n\nPython\n\n中文自然语言处理\n\n基础与实战\n\nFun(                                ce of\n\nChinese Natura                               ing with Python\n\n肖刚张良均◎主编\n\n郑鑫标罗惠琳陈晓娜◎副主编\n\n人 民 邮 电 出 版 社\n\n北 京\n\n图书在版编目(ClP)    数据\n\nPython中文自然语言处理基础与实战/肖刚，张良\n\nI.①P…Ⅱ.①      肖 … ②张 … Ⅲ . ①软件工具一程序\n\n设计一高等学校一教材 IV.①TP311.561\n\n中国版本图书馆CIP数据核字(2021)第113523号\n\n内 容 提 要\n\n本书以 Python  自然语言处理的常用技术与真实案例相结合的方式，深入浅出地介绍 Python 自然 语言处理的重要内容。全书共12章，内容包括绪论、语料库、正则表达式、中文分词技术、词性标注 与命名实体识别、关键词提取、文本向量化、文本分类与文本聚类、文本情感分析、NLP 中的深度学 习技术、智能问答系统，以及基于TipDM 大数据挖掘建模平台实现垃圾短信分类。本书包含课后习题\n\n和实训，帮助读者通过练习和操作实践，巩固所学内容。\n\n本书可作为“1+X”  证书制度试点工作中“大数据应用开发 (Python)”   职业技能等级证书的教学 和培训用书，也可以作为高校数据科学或人工智能相关专业的教材，还可作为机器学习爱好者的自学\n\n用书。\n\n◆ 主    编  肖  刚  张良均\n\n副主编   郑鑫标 罗惠琳  陈晓娜\n\n责任编辑  初美呈\n\n责任印制  王郁  彭志环\n\n人民邮电出版社出版发行         北京市丰台区成寿寺路11号\n\n邮编  100164  电子邮件315@ptpress.com.cn\n\n网址https://www.ptpress.com.cn\n\n天津翔远印刷有限公司印刷\n\n◆开本：787×10921/16\n\n印张：15.5\n\n字数：371千字\n\n2022年1月第1版\n\n2022年5月天津第2次印刷\n\n定价：59.80元\n\n读者服务热线：(010)81055256 印装质量热线：(010)81055316\n\n反盗版热线：(010)81055315\n\n广告经营许可证：京东市监广登字20170147号\n\n大数据技术精品系列教材\n\n专家委员会\n\n专委会主任：   郝志峰(汕头大学)\n\n专委会副主任(按姓氏笔画排列):\n\n冯国灿(中山大学)\n\n余明辉(广州番禺职业技术学院)\n\n张良均(广东泰迪智能科技股份有限公司)\n\n聂 哲(深圳职业技术学院) ·\n\n曾 斌(人民邮电出版社有限公司)\n\n蔡志杰(复旦大学)\n\n专委会成员 ( 按姓氏笔画排列):\n\n王 丹(国防科技大学)\n\n化存才(云南师范大学)\n\n孔 原(江苏信息职业技术学院) 史小英(西安航空职业技术学院)\n\n边馥萍(天津大学)\n\n吕跃进(广西大学)\n\n刘保东(山东大学)\n\n刘艳飞(中山职业技术学院)\n\n孙云龙(西南财经大学)\n\n花 强(河北大学)\n\n李明革(长春职业技术学院)\n\n杨 虎(重庆大学)\n\n杨治辉(安徽财经大学)\n\n王  津(成都航空职业技术学院)\n\n方海涛(中国科学院)\n\n邓明华(北京大学)\n\n冯伟贞(华南师范大学)\n\n戎海武(佛山科学技术学院)\n\n朱元国(南京理工大学)\n\n刘彦姝(湖南大众传媒职业技术学院)\n\n刘深泉(华南理工大学)\n\n阳永生(长沙民政职业技术学院)\n\n杜  恒(河南工业职业技术学院)\n\n杨 坦(华南师范大学)\n\n杨志坚(武汉大学)\n\n肖  刚(韩山师范学院)\n\nPython 中文自然语言处理基础与实战\n\n2\n\n吴孟达(国防科技大学)\n\n邱炳城(广东理工学院)\n\n沈  洋(大连职业技术学院)\n\n宋汉珍(承德石油高等专科学校)\n\n吴阔华(江西理工大学)\n\n余爱民(广东科学技术职业学院) 沈凤池(浙江商业职业技术学院)\n\n宋眉眉(天津理工大学)\n\n张 敏(广东泰迪智能科技股份有限公司)\n\n张尚佳(广东泰迪智能科技股份有限公司)\n\n张治斌(北京信息职业技术学院)\n\n张雅珍(陕西工商职业学院)\n\n武春岭(重庆电子工程职业学院)\n\n官金兰(广东农工商职业技术学院)\n\n胡支军(贵州大学)\n\n张积林(福建工程学院)\n\n陈 . 永(江苏海事职业技术学院)\n\n林智章(厦门城市职业学院)\n\n赵  强(山东师范大学)\n\n胡国胜(上海电子信息职业技术学院)\n\n施  兴(广东泰迪智能科技股份有限公司)\n\n秦宗槐(安徽商贸职业技术学院) 韩宝国(广东轻工职业技术学院) 蔡 铁(深圳信息职业技术学院)\n\n薛  毅(北京工业大学)\n\n韩中庚(信息工程大学)\n\n蒙  飚(柳州职业技术学院)\n\n谭  忠(厦门大学)\n\n魏毅强(太原理工大学)\n\n序    FOREWORD\n\n着大数据时代的到来，移动互联网和智能手机迅速普及，多种形态的移动互\n\n联网应用蓬勃发展，电子商务、云计算、互联网金融、物联网、虚拟现实、 智能机器人等不断渗透并重塑传统产业，而与此同时，大数据当之无愧地成为新的产\n\n业革命核心。\n\n2019年8月，联合国教科文组织以联合国6种官方语言正式发布《北京共识—— 人工智能与教育》。其中提出，各国要制定相应政策，推动人工智能与教育系统性融合， 利用人工智能加快建立开放、灵活的教育体系，促进全民享有公平、高质量、适合每\n\n个人的终身学习机会。这表明基于大数据的人工智能和教育均进入了新的阶段。\n\n高等教育是教育系统中的重要组成部分，高等院校作为人才培养的重要载体，肩 负着为社会培育人才的重要使命。2018年6月21日的新时代全国高等学校本科教育 工作会议首次提出了“金课”的概念。“金专”“金课”“金师”迅速成为新时代高等 教育的热词。如何建设具有中国特色的大数据相关专业，以及如何打造世界水平的\n\n“金专”“金课”“金师”“金教材”是当代教育教学改革的难点和热点。\n\n实践教学是在一定的理论指导下，通过实践引导，使学习者获得实践知识、掌握 实践技能、锻炼实践能力、提高综合素质的教学活动。实践教学在高校人才培养中有 着重要的地位，是巩固和加深理论知识的有效途径。目前，高校大数据相关专业的教 学体系设置过多地偏向理论教学，课程设置冗余或缺漏，知识体系不健全，且与企业 实际应用契合度不高，学生无法把理论转化为实践应用技能。为了有效解决该问题， “泰迪杯”数据挖掘挑战赛组委会与人民邮电出版社共同策划了“大数据技术精品系列 教材”,这恰与2019年10月24日教育部发布的《教育部关于一流本科课程建设的实 施意见》(教高〔2019〕8号)中提出的“坚持分类建设”“坚持扶强扶特”\"提升高阶\n\n性”“突出创新性”“增加挑战度”原则完全契合。\n\n“泰迪杯”数据挖掘挑战赛自2013年创办以来， 一直致力于推广高校数据挖掘实 践教学，培养学生数据挖掘的应用和创新能力。挑战赛的赛题均为经过适当简化和加 工的实际问题，来源于各企业、管理机构和科研院所等，非常贴近现实热点需求。赛 题中的数据只做必要的脱敏处理，力求保持原始状态。竞赛围绕数据挖掘的整个流程， 从数据采集、数据迁移、数据存储、数据分析与挖掘，到数据可视化，涵盖了企业应 用中的各个环节，与目前大数据专业人才培养目标高度一致。“泰迪杯”数据挖掘挑战 赛不依赖于数学建模，甚至不依赖传统模型的竞赛形式，使得“泰迪杯”数据挖掘挑\n\n战赛在全国各大高校反响热烈，且得到了全国各界专家学者的认可与支持。2018年，\n\nPython 中文自然语言处理基础与实战\n\n“泰迪杯”数据挖掘挑战赛增加了子赛项——数据分析职业技能大赛，为高职和中职技 能型人才培养提供理论、技术和资源方面的支持。截至2019年", "metadata": {}}, {"content": "，与目前大数据专业人才培养目标高度一致。“泰迪杯”数据挖掘挑战 赛不依赖于数学建模，甚至不依赖传统模型的竞赛形式，使得“泰迪杯”数据挖掘挑\n\n战赛在全国各大高校反响热烈，且得到了全国各界专家学者的认可与支持。2018年，\n\nPython 中文自然语言处理基础与实战\n\n“泰迪杯”数据挖掘挑战赛增加了子赛项——数据分析职业技能大赛，为高职和中职技 能型人才培养提供理论、技术和资源方面的支持。截至2019年，全国共有近800所高 校，约1万名研究生、5万名本科生、2万名高职生参加了“泰迪杯”数据挖掘挑战赛\n\n和数据分析职业技能大赛。\n\n本系列教材的第一大特点是注重学生的实践能力培养，针对高校实践教学中的痛 点，首次提出“鱼骨教学法”的概念。以企业真实需求为导向，学生学习技能时紧紧 围绕企业实际应用需求，将学生需掌握的理论知识，通过企业案例的形式进行衔接， 达到知行合一、以用促学的目的。第二大特点是以大数据技术应用为核心，紧紧围绕 大数据应用闭环的流程进行教学。本系列教材涵盖了企业大数据应用中的各个环节， 符合企业大数据应用真实场景，使学生从宏观上理解大数据技术在企业中的具体应用\n\n场景及应用方法。\n\n在教育部全面实施“六卓越一拔尖”计划2.0的背景下，对如何促进我国高等教 育人才培养体制机制的综合改革，以及如何重新定位和全面提升我国高等教育质量， 本系列教材将起到抛砖引玉的作用，从而加快推进以新工科、新医科、新农科、新文 科为代表的一流本科课程的“双万计划”建设；落实“让学生忙起来，管理严起来和 教学活起来”措施，让大数据相关专业的人才培养质量有一个质的提升；借助数据科 学的引导，在文、理、农、工、医等方面全方位发力，培养各个行业的卓越人才及未 来的领军人才。同时本系列教材将根据读者的反馈意见和建议及时改进、完善，努力\n\n成为大数据时代的新型“编写、使用、反馈”螺旋式上升的系列教材建设样板。\n\n汕头大学校长\n\n教育部高校大学数学课程教学指导委员会副主任委员\n\n“泰迪杯”数据挖掘挑战赛组织委员会主任\n\n“泰迪杯”数据分析职业技能大赛组织委员会主任\n\n2021年7月于粤港澳大湾区\n\n2\n\n前 言    PREFACE\n\n然语言处理(Natural   Language   Processing,NLP)作为人工智能的一个重要分\n\n支，它的应用需求越来越大，并且在数据处理领域占有越来越重要的地位， 如今被大多数人熟知和应用。中文和英文虽然整体上在NLP 的算法中差异不大，但是 在细节处理上还是存在很多差异。大部分NLP 的相关资料都是以英文为基础的， 一般 初学者先学习英文的处理，然后再学习中文的处理。这样中文 NLP学习者不仅走了弯 路，也浪费了大量时间和精力。此外，国内纯中文，并结合理论和实践的 NLP书籍较 少，这让中文NLP 学习者学习起来有一定难度。本书为中文NLP 初学者边学边实战 的入门级教程，针对中文语料的小数据量实例，通过理论结合实践的形式，带领初学\n\n者快速掌握NLP 在中文方面的基本开发方法。\n\n本书特色\n\n本书内容契合“1+X”  证书制度试点工作中“大数据应用开发(Python)”  职业技能 等级证书(高级)考核标准，将理论与实践结合，注重任务案例的学习。本书设计思路 以应用为导向，从知识点背景到原理分析，再到任务案例，让读者明确如何利用所学知 识来解决现实问题；通过实训和课后习题巩固所学知识，让读者真正理解并能够应用所\n\n学知识。本书大部分章节围绕任务需求展开，着重于思路的启发与解决方案的实施。\n\n本书适用对象\n\n开 设NLP 相关课程的院校的学生。\n\nNLP应用的开发人员。\n\n进行NLP 应用研究的科研人员。\n\n“1+X” 证书制度试点工作中“大数据应用开发(Python)”职业技能等级证书\n\n(高级)考生。\n\n代码下载及问题反馈\n\n为了帮助读者更好地使用本书，泰迪云课堂提供了配套的教学视频。如需获取书 中的原始数据文件和程序代码，读者可以从“泰迪杯”数据挖掘挑战赛网站免费下载， 也可登录人民邮电出版社教育社区(www.ryjiaoyu.com)  下载。为方便教师授课，本 书还提供了PPT 课件、教学大纲、教学进度表和教案等教学资源，教师可扫描下方二\n\n维码下载申请表，填写后发送至指定邮箱申请所需资料。同时欢迎教师加入 QQ 交流\n\nPython 中文自然语言处理基础与实战\n\n群“人邮大数据教师服务群”(669819871)进行交流探讨。\n\n由于编者水平有限，书中难免出现一些疏漏和不足之处。如果读者有宝贵意见， 欢迎在泰迪学社微信公众号 (TipDataMining)  回复“图书反馈”进行反馈。更多本系\n\n列教材的信息可以在“泰迪杯”数据挖掘挑战赛网站查阅。\n\n编  者\n\n2021年5月\n\n2\n\n泰迪云课堂\n\n“泰迪杯”数据挖掘\n\n挑战赛网站\n\n申请表下载\n\n目 (录 NTENTS\n\nPython  中文自然语言处理基础与实战\n\n2\n\n目    录\n\n3\n\n实训 1 基于朴素贝叶斯的新闻分类…… 144\n\n实训2 食品种类安全问题聚类分析……  145\n\n9.4.1  基于朴素贝叶斯分类的情感分析… 154\n\n9.4.2  基于SnowNLP库的情感分析…… 156\n\n9.5    任务：基于LDA  主题模型的\n\n实训 2 基于朴素贝叶斯算法的豆瓣评论\n\n10.2.3  非等长结构(Seq2Seq模型)…… 169\n\n10.3.2   基于TensorFlow 的深度学习库\n\n实训1 实现基于LSTM模型的新闻分类…195\n\nPython  中文自然语言处理基础与实战\n\n4\n\n第12章 基 于TipDM  大数据挖掘建模\n\n平台实现垃圾短信分类 ……  220\n\n12.1     平 台 简 介……………………………220\n\n 第①章绪论\n\n所谓自然语言，即人们日常使用的语言。人类的多种智能活动都与语言有着密切的 关系，并且绝大部分的知识是通过语言文字的形式记载和流传下来的。自从有了计算机， 人们就希望用自然语言与计算机进行交流，其中核心的任务就是将人的语言转换成计算 机可以执行的命令，也就是让计算机读懂人的语言。自然语言处理是指将人类交流沟通 所用的语言经过处理转化为计算机能理解的机器语言，是一种研究语言能力的模型和算 法框架，是一门语言学和计算机科学的交叉学科。作为人工智能的一个重要分支，自然 语言处理在数据处理领域占有越来越重要的地位，如今被大多数人熟知和应用。本章主 要介绍自然语言处理的发展历程、研究内容、应用场景、技术应用，以及基本流程等\n\n内容。\n\n学习目标\n\n(1)了解自然语言处理的发展历程、研究内容和应用场景。\n\n(2)熟悉自然语言处理的基本流程。\n\n(3)熟悉 Anaconda的安装流程和虚拟环境的创建。\n\n(4)掌握 Jupyter Notebook 和 Spyder 应用功能的操作方法。\n\n1.1   自然语言处理概述\n\n自然语言是指汉语、英语、法语等人们日常使用的语言，是自然而然地随着人类社会 发展演变而来的语言。它是人类学习和生活中的重要工具。概括来说，自然语言是指人类\n\n社会约定俗成的，并且区别于人工语言(如计算机程序语言)的语言。\n\n自然语言处理 (Natural  Language  Processing,NLP) 是指利用计算机对自然语言的形、 音、义等信息进行处理，即对字、词、句、篇章的输入、输出、识别、分析、理解、生成  等进行操作和加工的过程。NLP 是计算机科学领域和人工智能领域的一个重要研究方向，  是一门融语言学、计算机科学、数学和统计学于一体的科学。NLP 的具体表现形式包括机\n\n器翻译、文本摘要、文本分类、文本校对、信息抽取、语音合成、语音识别等。\n\nNLP 机制涉及两个流程：自然语言理解和自然语言生成。自然语言理解研究的是计算 机如何理解自然语言文本中包含的意义，自然语言生成研究的是计算机如何生成自然语言\n\n文本表达给定的意图、思想等。因为NLP 的目的是让计算机“理解”自然语言，所以NLP\n\nPython 中文自然语言处理基础与实战\n\n有时又被称为自然语言理解 (Natural    Language     Understanding,NLU)。\n\n1.1.1  NLP 的发展历程\n\n1946年世界上第一台通用电子计算机诞生时，英国人布思和美国人韦弗就提出了利用 计算机进行机器翻译。从这个时间点开始算起， NLP 技术已经历70多年的发展历程。NLP\n\n的整个发展历程可以归纳为“萌芽期”“发展期”“繁荣期”3个阶段。\n\n1. 萌芽期(1960年以前)\n\n20世纪40年代到50年代之间", "metadata": {}}, {"content": "，英国人布思和美国人韦弗就提出了利用 计算机进行机器翻译。从这个时间点开始算起， NLP 技术已经历70多年的发展历程。NLP\n\n的整个发展历程可以归纳为“萌芽期”“发展期”“繁荣期”3个阶段。\n\n1. 萌芽期(1960年以前)\n\n20世纪40年代到50年代之间，除了当时给世界带来极大震撼的计算机技术外，在美 国还有两个人在进行着重要的研究工作。其中一位是乔姆斯基，他的主要工作为对形式语 言进行研究；另一位是香农，他的主要工作是基于概率和信息论模型进行研究。香农的信 息论是在概率统计的基础上对自然语言和计算机语言进行的研究。1956年，乔姆斯基提出 了上下文无关语法，并将它运用到NLP 中。他们的工作直接导致了基于规则和基于概率这 两种不同的 NLP 技术的产生，而这两种不同的 NLP 技术又引发了数十年有关基于规则方\n\n法和基于概率方法孰优孰劣的争执。\n\n2. 发展期(1960年—1999年)\n\n20世纪60年代，法国格勒诺布尔大学的著名数学家沃古瓦开始了自动翻译系统的开 发。在这一时期，很多国家和组织对机器翻译都投入了大量的人力、物力和财力。然而在 机器翻译系统的开发过程中，出现了各种各样的问题，并且这些问题的复杂度远远超过了 原来的预期。为了解决这些问题，当时人们提出了各种各样的模型和解决方案。虽然最后 的结果并不如人意，但是却为后来的各个相关分支领域的发展奠定了基础，如统计学、逻\n\n辑学、语言学等。\n\n20世纪90年代后，在计算机技术的快速发展下，基于统计的 NLP 取得了相当大的成 果，开始在不同的领域里大放异彩。例如，由于机器翻译领域引入了许多基于语料库的方 法，因此率先取得了突破。1990年，第13 届国际计算机语言学大会的主题是“处理大规 模真实文本的理论、方法与工具”,从而将研究的重心开始转向大规模真实文本，传统的基\n\n于语言规则的 NLP 开始显得力不从心。\n\n20世纪90年代中期，有两件事促进了NLP 研究的复苏与发展。 一件事是计算机的运 行速度和存储量的大幅提高，这为NLP 改善了物质基础，使得语言处理的商品化开发成为 可能；另一件事是1994年万维网协会成立，在互联网的带动下，产生了很多原来没有的计 算模型，大数据和各种统计模型应运而生。这段时间，在大数据和概率统计模型的影响下，\n\nNLP 得到了飞速的发展。\n\n3. 繁荣期(2000年至今)\n\n21世纪之后， 一大批互联网公司的涌现对 NLP 的发展起到了很大的推动作用，如早期 的雅虎搜索、后来的谷歌和百度。大量的基于万维网的应用和各种社交工具在不同的方面 促进了NLP 的发展进步。在这个过程中，各种数学算法和计算模型越来越显现出重要性。 机器学习、神经网络和深度学习等技术都在不断地消除人与计算机之间的交流限制。特别\n\n是深度学习技术，它将会在NLP 领域发挥越来越重要的作用。也许在不久的将来，在互联\n\n2\n\n第①章  绪论\n\n网的基础上，现今 NLP 中遇到的问题将不再是问题。使用不同语言的人们可以畅通无阻地\n\n沟通交流，人与计算机之间的沟通也没有阻碍。\n\n1.1.2  NLP 研究内容\n\nNLP 研究内容包括很多的分支领域，如文本分类、信息抽取、信息检索、信息过滤、 自动文摘、智能问答、话题推荐、机器翻译、主题词识别、知识库构建、深度文本表示、 命名实体识别、文本生成、文本分析(词法、句法和语法)、舆情分析、自动校对、语音识\n\n别与合成等。部分常见的NLP 分支领域的简介如下。\n\n1. 机器翻译\n\n机器翻译又称自动翻译，是利用计算机将一种自然语言转换为另一种自然语言的过 程。机器翻译是计算机语言学的一个分支，是人工智能的终极目标之一，具有重要的科\n\n学研究价值。\n\n2. 信息检索\n\n信息检索又称情报检索，是指利用计算机系统从海量文档中找到符合用户需求的相 关信息。狭义的信息检索仅指信息查询，广义的信息检索是指将信息按一定的方式进行 加工、整理、组织并存储起来，再根据信息用户特定的需求将相关信息准确地查找出来\n\n的过程。\n\n3. 文本分类\n\n文本分类又称文档分类或信息分类，其目的是利用计算机系统对大量的文档按照一定 的标准进行分类。文本分类技术具有广泛的用途，公司可以利用该技术了解用户对产品的 评价，政府部门也可以利用该技术分析人们对某一事件、政策法规或社会现象的评论，实\n\n时了解百姓的态度。\n\n4. 智能问答\n\n智能问答是指问答系统能以一问一答的形式，正确回答用户提出的问题。智能问答 可以精确地定位用户所提问的知识，通过与用户进行交互，为用户提供个性化的信息\n\n服务。\n\n5. 信息过滤\n\n信息过滤是指信息过滤系统对网站信息发布、公众信息公开申请和网站留言等内容实 现提交时的自动过滤处理。例如，发现谩骂、诽谤等非法言论或有害信息时实现自动过滤， 并给用户友好的提示，同时向管理员提交报告。信息过滤技术目前主要用于信息安全防护、\n\n网络内容管理等。\n\n6. 自动文摘\n\n文摘是指能够全面准确地反映某一文献中心内容的简单连贯的短文，自动文摘则是指  利用计算机自动地从原始文献中提取文摘。互联网每天都会产生大量的文本数据，文摘是 文本的主要内容，用户想查询和了解关注的话题需要花费大量时间和精力进行选择和阅读，\n\n单靠人工进行文摘是很难实现的。为了应对这种状况，学术界尝试使用计算机技术实现对\n\n3\n\nPython 中文自然语言处理基础与实战\n\n文献的自动处理。自动文摘主要应用于 Web搜索引擎、问答系统的知识融合和舆情监督系\n\n统的热点与专题追踪。\n\n7. 信息抽取\n\n信息抽取是指从文本中抽取出特定的事件或事实信息。例如，从时事新闻报道中抽取 出某一恐怖袭击事件的基本信息，如时间、地点、事件制造者、受害人、袭击目标、伤亡 人数等。信息抽取与信息检索有着密切的关系，信息抽取系统通常以信息检索系统的输出\n\n作为输入，此外，信息抽取技术可以用于提高信息检索系统的性能。\n\n8. 舆情分析\n\n舆情分析是指根据特定问题的需要，对舆情进行深层次的思维加工和分析研究，得 到相关结论的过程。网络环境下舆情信息的主要来源有新闻评论、网络论坛、聊天室、 博客、新浪微博、聚合新闻和 QQ 等。由于网上的信息量十分巨大，仅仅依靠人工的方 法难以应对海量信息的搜集和处理，因此需要加强相关信息技术的研究，形成一套自动 化的网络舆情分析系统，以及时应对网络舆情，由被动防堵变为主动梳理、  引导。舆情 分析是一项十分复杂、涉及问题众多(包括网络文本挖掘、观点挖掘等各方面的问题)\n\n的综合性技术。\n\n9. 语音识别\n\n语音识别又称自动语音识别，是指对输入计算机的语音信号进行识别并将其转换成文 字表示出来。语音识别技术涉及的领域众多，其中包括信号处理、模式识别、概率论和信\n\n息论、发声机理和听觉机理、人工智能等。\n\n10. 自动校对\n\n自动校对是指对文字拼写、用词、语法或文档格式等进行自动检查、校对和编排的过 程。电子信息的形成可通过多种途径实现，最常用的方法是用键盘输入，但键盘输入不免 会造成一些输入错误，利用计算机进行文本自动校对的研究就由此产生了。自动校对系统\n\n可应用于出版、打字业等需要进行文本校对的行业。\n\n1.1.3 NLP  的几个应用场景\n\nNLP 不仅是一种新兴的商业技术，更是一种广泛使用的流行技术。几乎所有涉及语言\n\n的功能都包含NLP 算法。 NLP 在人们的日常生活中有广泛的应用，常见应用场景如下。\n\n1. 百度翻译\n\n百度翻译是百度公司发布的在线翻译服务，其依托互联网数据资源和NLP 技术的优势， 致力于帮助用户跨越语言鸿沟、方便快捷地获取信息和服务。百度翻译是一款比较成熟的 机器翻译产品。此外，还有支持语音输入的多国语言互译的产品，如科大讯飞的机器翻译\n\n产品。\n\n2. 图灵机器人\n\n智能问答在一些电商网站中具有非常实用的价值，如代替人工充当客服角色。人工\n\n客服有时会遇到很多基本而且重复的问题，此时就可以通过智能问答系统筛选掉大量重\n\n4\n\n第①章  绪论\n\n复的问题，使人工客服能更好地服务客户。图灵机器人是以语义技术为核心驱动力的人 工智能产品，其三大核心功能之一就是智能问答。图灵机器人提供超过500种实用生活 服务技能，涵盖生活、出行、学习、金融、购物等多个领域，能提供一站式服务以满足\n\n用户的需求。\n\n3. 微信语音转文字\n\n微信中有一个将语音转化成文字的功能，其原理就是利用NLP 、语音识别等技术，在 基于语言模型和声学模型的转写引擎下", "metadata": {}}, {"content": "，使人工客服能更好地服务客户。图灵机器人是以语义技术为核心驱动力的人 工智能产品，其三大核心功能之一就是智能问答。图灵机器人提供超过500种实用生活 服务技能，涵盖生活、出行、学习、金融、购物等多个领域，能提供一站式服务以满足\n\n用户的需求。\n\n3. 微信语音转文字\n\n微信中有一个将语音转化成文字的功能，其原理就是利用NLP 、语音识别等技术，在 基于语言模型和声学模型的转写引擎下，将持续语流转写成文字。此功能的好处之一是方 便快速阅读和理解，另一个好处是方便对内容进行二次推广和多次利用。成年人正常的语 速为160字/分钟，比绝大多数人打字的速度快，微信语音转文字这个功能，可以极大地节\n\n省输入文字的时间，提高工作效率。\n\n4. 新闻自动分类\n\n新闻自动分类是文本自动分类最常见的一个应用。随着网络信息技术的迅速发展和传 统纸媒向信息化媒体的逐渐转型，网络中存在着越来越多的新闻信息积累，传统的手动新 闻分类存在耗费大量人力和物力等诸多的弊端。为了提高新闻分类的准确率和速度，新闻 自动分类顺理成章地成为发展方向。新闻自动分类有助于实现新闻的有序化管理，以及新 闻的挖掘分析。百度就实现了新闻的自动分类，它涵盖军事、财经、娱乐、游戏等多个分\n\n类，可以实现每隔一段时间自动获取更新、自动分类等操作。\n\n1.1.4  NLP 与人工智能技术\n\n人工智能技术是让计算机能够通过模仿人类自动化完成智能任务的技术，其关键在于 智能和自动化。NLP 是人工智能(Artificial Intelligence,AI)研究的一个子领域，也是人\n\n工智能中最困难的问题之一。\n\nAI 技术的发展大致经历了3次浪潮。20世纪70年代第一次 AI 浪潮泡沫破灭之后，相 关研究者转而研究机器学习、数据挖掘、NLP 等方向。20世纪90年代 AI 迎来第二个黄金 时代，但是 AI 并未真正进入人们日常生活，AI 再次进入沉寂期。2008年左右，由于数据 量的大幅度增长和计算机性能的大幅度提升，深度学习开始引领AI 进入第三次浪潮。人们 也逐渐开始将深度学习技术引入NLP 领域中，并在机器翻译、问答系统、自动摘要等方向 取得成功。深度学习能在NLP 中取得这样的成绩的原因可以归结为海量数据的获取和深度\n\n学习算法的革新。\n\n互联网的快速发展，使得很多应用积累了足够多的数据用于学习。当数据量增大之后， 以支持向量机 (Support    Vector    Machine,SVM)、条件随机场 (Conditional   Random  Field,    CRF)  为代表的传统浅层模型由于模型过浅，无法对海量数据中的高维非线性映射建模， 因此不能带来性能的提升。然而，以循环神经网络 (Recurrent     Neural    Network,RNN) 为 代表的深度学习模型，可以随着模型复杂度的增大而增强，能更好地贴近数据的本质映射\n\n关系，达到更优的效果。\n\n深度学习的词向量模型 Word2Vec,   可以将词表示为更低维度的向量空间。这既缓\n\n解了语义鸿沟问题，又降低了输入特征的维度，从而降低了输入层的维度。深度学习\n\n5\n\nPython 中文自然语言处理基础与实战\n\n模型非常灵活，这使得之前的很多任务可以通过端到端的方式进行训练，从而提升了\n\n模型的性能。\n\nNLP 在过去几十年的发展中，从基于简单的规则方法到基于统计学方法，再到现在的 基于深度学习神经网络的方法，技术越来越成熟，在很多领域都取得了巨大的成就。展望 未来10年，随着数据的积累，云计算、芯片技术和人工智能技术的发展等，自然语言必将 越来越贴近人工智能。除此之外，随着人工智能各领域的研究细化，跨领域的研究整合将 是未来的发展方向。可预见的是，NLP 将会和计算机视觉、听觉、触觉等领域高度融合， 反映在人工智能技术上就是语音识别和图像识别，实现包含语言、知识和推理的真正意义\n\n上的智能。\n\nNLP 研究与应用已经取得较为丰硕的成果，但同时也面临着许多新的挑战。实际上对 于NLP 的很多问题，人们本身也不能非常准确、满意地解决。当然，并不是说人们不应该 对某项技术提出更高的要求和希望，但更重要的是应该如何建立有效的理论模型和实现方\n\n法，这也是 NLP 这门学科所面临的问题和挑战。\n\n1.1.5  学习 NLP 的难点\n\nNLP 的发展已经进入了一个相当繁荣的时期，各行各业中越来越多的内容涉及NLP,    这使得 NLP 的学习成为一种迫切的需要。在实际的学习应用中，自然语言的复杂性和多 变性使得学习NLP 变得困难。 一是多学科交叉的困难。NLP 是一门融语言学、计算机科 学、数学、统计学于一体的交叉学科。由于教学时间与学习精力是有限的，人们无法做 到对每一学科的学习和研究都面面俱到，只能对每个学科浅尝辄止，无法进一步深入学 习，因此多学科融合就成为一个学习难点。二是理论学习的困难。NLP 运用了多种复杂 难懂的数学模型，如概率图模型、隐马尔可夫模型 (Hidden     Markov      Model,HMM)、最 大熵模型、条件随机场模型等，这些理论的理解对初学者来说有一定的难度。三是语料 获取的困难。在 NLP 的实际项目中，通常要使用大量的语言数据或者语料，而初学者要\n\n获取这些语料是比较困难的。\n\n1.2 NLP 基本流程\n\n中文 NLP 的基本流程和英文相比有一些特殊性，主要表现在文本预处理环节。首先， 中文文本没有像英文单词那样用空格隔开，因此不能像英文一样直接用最简单的空格和标 点符号完成分词， 一般需要用分词算法完成分词。其次，中文的编码不是utf-8,  而是 Unicode,   因此在预处理的时候，有编码处理的问题。中文NLP 的基本流程由语料获取、语料预处理、\n\n文本向量化、模型构建、模型训练和模型评价6部分组成。\n\n1.2.1 语料获取\n\n在进行 NLP 之前，人们需要得到文本语料。文本语料的获取一般有以下几种方法。\n\n(1)利用已经建好的数据集或第三方语料库，这样可以省去很多处理成本。\n\n(2)获取网络数据。很多时候要解决的是某种特定领域的应用，仅靠开放语料库无法\n\n满足需求，这时就需要通过爬虫技术获取需要的信息。\n\n(3)制订数据搜集策略搜集数据。可以通过制订数据搜集策略，从业务的角度搜集所\n\n6\n\n第①章 绪论\n\n需要的数据。\n\n(4)与第三方合作获取数据。通过购买的方式获取部分需求文本数据。\n\n1.2.2  语料预处理\n\n获取语料后还需要对语料进行预处理，常见的语料预处理如下。\n\n(1)去除数据中非文本内容。大多数情况下，获取的文本数据中存在很多无用的内容 如爬取的一些 HTML 代码、CSS 标签和不需要的标点符号等，这些都需要分步骤去除。少 量非文本内容可以直接用 Python 的正则表达式删除，复杂的非文本内容可以通过 Python\n\n的 Beautiful Soup 库去除。\n\n(2)中文分词。常用的中文分词软件有很多，如jieba 、FoolNLTK 、HanLP 、THULAC、 NLPIR 、LTP 等，本书使用jieba 作为分词工具。jieba  是使用 Python 语言编写的，其安装\n\n方法很简单，使用 “pip install jieba” 命令即可完成安装。\n\n(3)词性标注。词性标注指给词语打上词类标签，如名词、动词、形容词等，常用的\n\n词性标注方法有基于规则的算法、基于统计的算法等。\n\n(4)去停用词。停用词就是句子中没必要存在的词，去掉停用词后对理解整个句子的 语义没有影响。中文文本中存在大量的虚词、代词或者没有特定含义的动词、名词，在文\n\n本分析的时候需要去掉。\n\n1.2.3  文本向量化\n\n文本数据经过预处理去除数据中非文本内容、中文分词、词性标注和去停用词后，基 本上是干净的文本了。但此时还是无法直接将文本用于任务计算，需要通过某些处理手段， 预先将文本转化为特征向量。 一般可以调用一些模型来对文本进行处理，常用的模型有词 袋模型(Bag of Words Model)、独热表示、TF-IDF 表示、 n 元语法(n-gram)   模型和 Word2Vec\n\n模型等。\n\n1.2.4  模型构建\n\n文本向量化后，根据文本分析的需求选择合适的模型进行模型构建，同类模型也需要 多准备几个备选用于效果对比。过于复杂的模型往往不是最优的选择，模型的复杂度与模 型训练时间呈正相关，模型复杂度越高，模型训练时间往往也越长", "metadata": {}}, {"content": "，常用的模型有词 袋模型(Bag of Words Model)、独热表示、TF-IDF 表示、 n 元语法(n-gram)   模型和 Word2Vec\n\n模型等。\n\n1.2.4  模型构建\n\n文本向量化后，根据文本分析的需求选择合适的模型进行模型构建，同类模型也需要 多准备几个备选用于效果对比。过于复杂的模型往往不是最优的选择，模型的复杂度与模 型训练时间呈正相关，模型复杂度越高，模型训练时间往往也越长，但结果的精度可能与 简单的模型相差无几。NLP 中使用的模型包括机器学习模型和深度学习模型两种。常用的 机器学习模型有KNN 、SVM 、Naive   Bayes、决策树、K-means  等。常用的深度学习模型有\n\nRNN 、CNN 、LSTM 、Seq2Seq 、FastText 、TextCNN 等。\n\n1.2.5  模型训练\n\n模型构建完成后，需要进行模型训练，其中包括模型微调等。  训练时可先使用小批量 数据进行试验，这样可以避免直接使用大批量数据训练导致训练时间过长等问题。在模型 训练的过程中要注意两个问题： 一个为在训练集上表现很好，但在测试集上表现很差的过 拟合问题；另一个为模型不能很好地拟合数据的欠拟合问题。同时，还要避免出现梯度消\n\n失和梯度爆炸问题。\n\n7\n\nPython 中文自然语言处理基础与实战\n\n仅训练一次的模型往往无法达到理想的精度与效果，还需要进行模型调优迭代，提升 模型的性能。模型调优往往是一个复杂、冗长且枯燥的过程，需要多次对模型的参数做出 修正；调优的同时需要权衡模型的精度与泛用性，在提高模型精度的同时还需要避免过拟 合。在现实生产与生活中，数据的分布会随着时间的推移而改变，有时甚至会变化得很急 剧，这种现象称为分布漂移(Distribution Drift)。当一个模型随着时间的推移，在新的数据 集中的评价不断下降时，就意味着这个模型无法适应新的数据的变化，此时模型需要进行\n\n重新训练。\n\n1.2.6  模型评价\n\n模型训练完成后，还需要对模型的效果进行评价。模型的评价指标主要有准确率 (Accuracy) 、 精确率 (Precision) 、 召回率、F1  值、ROC 曲线、AUC 曲线等。针对不同类 型的模型，所用的评价指标往往也不同，例如分类模型常用的评价方法有准确率、精确率、 AUC曲线等。同一种评价方法也往往适用于多种类型的模型。在实际的生产环境中，模型 性能评价的侧重点可能会不一样，不同的业务场景对模型的性能有不同的要求，如可能造\n\n成经济损失的预测结果会要求模型的精度更高。\n\n1.3  NLP 的开发环境\n\nPython 以其清晰简洁的语法、易用、可扩展性和丰富庞大的库深受广大开发者喜爱。 Python  内置的非常强大的机器学习代码库和数学库，使其理所当然地成为 NLP 的开发工 具。同时 Python 是开源且免费的，这意味着开发人员不需要花费资金即可进行开发。因此， 采用 Python 进行 NLP 是再好不过的选择。但这个强大的编程软件对初学者来说往往会有 设置环境变量的困扰，为此本书推荐已经集成了 Python 开发环境且自带多种常用数据科学\n\n库的软件 Anaconda。\n\n1.3.1 Anaconda  安装\n\nAnaconda 是一个开源的 Python 发行版本，其包含了 conda 、Python 等180多个科学包 及其依赖项。其中 conda  是一个开源的环境管理器，它可以在同一个机器上安装不同版本 的软件包及其依赖项，并能够在不同的环境之间切换。Anaconda 包含大量的科学包，安装 文件比较大。如果只需要某些包，或者需要节省带宽或存储空间，可以使用较小的发行版\n\nMiniconda (仅包含 conda  和 Python)。\n\nAnaconda 可以应用于多种系统，不管是 Windows 、Linux 还是 Mac OS X, 都可以找到 对应系统类型的版本。Anaconda 可以同时管理不同版本的 Python 环境，包括 Python  2 和 Python  3。本书推荐使用Python3,    因为 Python2  已停止更新维护，并且本书中所有的程序\n\n代码都是基于Python  3进行编写的。\n\n在Windows 环境下， Anaconda 的安装比较简单。按照默认选项进行安装，在选择完路 径后，可勾选图1-1所示的 “Add Anaconda3 to the system PATH environment variable” ( 添 加 Anaconda 至系统环境变量路径中)复选框。勾选此复选框的好处是方便后续创建多种版\n\n本的 Python,   坏处是可能会影响其他程序的使用。\n\n8\n\n第①章 绪论\n\n图1-1 添加Anaconda 至系统环境变量路径中\n\n1.3.2 Anaconda   应用介绍\n\nAnaconda 安装完成后，开始菜单栏中会出现几个应用，分别为Anaconda Navigator、\n\nAnaconda Prompt 、Jupyter Notebook 和 Spyder。\n\n1.Anaconda  Navigator\n\nAnaconda     Navigator 是 Anaconda   发行包中包含的桌面图形界面，可以在不使用命令的 情况下，方便地启动应用程序，管理 conda 包、环境和频道。单击 “Anaconda Navigator”  后会打开页面，页面中会出现CMD.exe Prompt、JupyterLab、Jupyter Notebook、Powershell Prompt 、Qt Console 、Spyder 、Glueviz 、Orange 3 、RStudio 等应用，如图1-2所示。如果要\n\n运行 Spyder,  直接在 “Home”  页面中单击 “Spyder”  即可。\n\n图1-2 “Anaconda Navigator”页面\n\n9\n\nPython 中文自然语言处理基础与实战\n\n2.Anaconda   Prompt\n\nAnaconda Prompt 相当于命令行窗口，与命令行窗口不同的是， Anaconda Prompt 已经 配置好了环境变量。初次安装 Anaconda 的包版本一般比较老，为了避免之后使用时报错， 可以先单击 “Anaconda    Prompt”,然后输入 “conda     update-all” 命令更新所有包的版本，\n\n在提示是否更新的时候输入“y”( 即 Yes), 然后等待更新完成即可。\n\n在当前环境下可以直接运行 Python 文件(如输入 “python  hello.py” 命令),或者在命\n\n令行窗口中输入“python”命令进入交互模式，如图1-3 所示。\n\n图1-3 通过 Anaconda Prompt 运行 Python 文件和进入交互模式\n\n(1)创建 NLP 虚拟环境。在开发过程中，很多时候不同的项目会需要用到不同版本的 包，甚至是不同版本的 Python, 使用虚拟环境可轻松解决这些问题。虚拟环境通过创建一 个全新的 Python 开发环境，实现不同项目间的隔离。打开 Anaconda Prompt后，可以利用 Anaconda自带的 conda 包管理不同的Python 环境。刚开始学习NLP 的读者可以利用conda\n\n包创建一个 NLP 虚拟环境。\n\n先查看 Python 版本，然后创建一个名为 “NLP”  的虚拟环境，并指定 Python 版本，如\n\n代码1-1所示。\n\n代码1-1 查看 Python 版本并创建虚拟环境\n\n(2)进入NLP 虚拟环境。虚拟环境创建完成之后，使用 “activate”  命令进入这个虚拟\n\n环境，并在NLP 虚拟环境中查看配置的编译环境信息，如代码1-2所示。\n\n代码1-2 进入虚拟环境并查看配置信息\n\n运行代码1-2后，结果如图1-4所示。\n\n管理员：Anaconda Prompt\n\nxk\n\nKbaee>C:Wleera Mldninfatratar>aetivate NLP\n\n《NLP>C:sers  dninistrator>conda  into-\n\nw*@\n\nNLP\n\nKNLP>C:Nlaers Adninistrator>\n\n图1-4  进入虚拟环境并查看配置信息\n\n10\n\n第①章  绪论\n\n图1-4 中展示了刚创建的 NLP 虚拟环境的所在路径，路径显示该环境位于 Anaconda 安装路径下的 envs 文件夹中。在刚创建好的虚拟环境中，除了 Python  自带的包之外，没\n\n有其他的包。查看当前环境下的所有包，如代码1-3所示。\n\n代码1-3 查看当前环境下的所有包\n\nconda     env     list     # 查看当前环境下的所有包\n\n( 3 ) 在NLP 虚拟环境中安装或卸载程序包。在学习过程中，可以根据需要安装不同的 程序包。可通过pip 命令或者conda 命令两种方式安装程序包，即“pip install package_name” 和 “conda  install  package_name” 命令，其中 “package_name”  是指程序包的名称。在虚拟 环境中， “pip  install” 命令只会安装需要安装的那个包本身", "metadata": {}}, {"content": "，可以根据需要安装不同的 程序包。可通过pip 命令或者conda 命令两种方式安装程序包，即“pip install package_name” 和 “conda  install  package_name” 命令，其中 “package_name”  是指程序包的名称。在虚拟 环境中， “pip  install” 命令只会安装需要安装的那个包本身，而 “conda   install” 命令除了会 安装需要安装的包，还会自动安装这个包的依赖项。在虚拟环境中，安装程序包如代码1-4\n\n所示，升级和卸载程序包如代码1-5所示。\n\n代码1-4 安装程序包\n\n(4)退出虚拟环境。退出当前的虚拟环境如代码1-6所示。\n\n代码1-6  退出当前的虚拟环境\n\nconda       deactivate    # 退出当前环境，回到最开始的环境\n\n(5)删除环境。删除创建的NLP 虚拟环境如代码1-7所示。\n\n代码1-7 删除创建的 NLP 虚拟环境\n\nconda  remove  --name  NLP  --all  # 删除创建的NLP 环境\n\nAnaconda 能够管理不同环境下的包，使这些包在不同环境下互不影响。在 NLP 的学 习过程中，会使用到很多的程序包， Anaconda 的这种功能无疑为我们的学习提供了很大的\n\n便利。\n\n3.Jupyter   Notebook\n\nJupyter Notebook 是一个在浏览器中使用的交互式的代码编辑器，可以将代码、文字结 合起来。它的受众大多是从事数据科学相关领域(机器学习、数据分析等)的人员。在撰 写含有程序的内容时，有时会展示一大段代码，这样不便于读者阅读，而使用 Jupyter\n\nNotebook 则可以一边编写代码一边解释代码，非常适合用于交互场景。\n\n打开 Jupyter  Notebook 有3种方式：第一种是直接在开始菜单栏中单击 “Anaconda”   下的 “Jupyter   Notebook”; 第二种是在Anaconda  Prompt中输入 “jupyter    notebook”, 浏览 器会自动打开并且显示当前的目录；第三种方式为首先打开某个文件夹，然后按住 “Shift” 键并单击鼠标右键，在菜单中单击“在此处打开 Powershell 窗口”命令，如图1-5所示， 这时会弹出命令行窗口，接着输入 “jupyter  notebook” 命令即可。打开 Jupyter Notebook 后 单击右上角的 “New”→“Python3”       命令，便可创建新笔记，如图1-6所示。使用Jupyter\n\nNotebook 运行 Python 程序时的界面如图1-7所示。\n\n11\n\nPython 中文自然语言处理基础与实战\n\n代码 共享 查着 类型 JetBrains PyChar 大小 1KB > 代 码 名称 修改日期\n\n图1-5 单击“在此处打开 Powershell 窗口”命令\n\nCJupyter\n\n12\n\nFiles       Running\n\nSelect items to perform actions on them.\n\n0-/\n\nhello.py\n\nUpload   New     C\n\nPython 3\n\nOmek\n\nText File\n\nFolder\n\nTerminal\n\n图1-6 创建新笔记\n\n图1-7 运行 Python 程序时的界面\n\nJupyter Notebook 有编辑模式和命令模式两种输入模式。当单元框的边框线是绿色时， Jupyter  Notebook 处于编辑模式，此时允许在单元框中输入代码或者文本，按 “Esc”  键可 切换为命令模式。命令模式下单元框的边框线是灰色的，可以输入运行程序的命令，按 “Enter” 键可切换为编辑模式。在编辑文档时，是以cell 为一个单元框的。cell 有3种类型，\n\n不同的类型有不同的意义， cell的类型及说明如表1- 1所示。\n\n第①章 绪论\n\n表1 - 1 cell 的类型及说明\n\n类型 说明 --code 表示内容可以运行 --heading 表示此单元框的内容是标题(一级、二级、三级标题) --markdown 表示可以用markdown的语法编辑文本\n\n代码编写完成之后可以按快捷键 “Shift+Enter”   或者单击页面上方的“运行”按钮执 行 cell 中的代码。文档编辑完成后，默认保存文件为 “ipynb”   格式，也可以保存为“.py”\n\n“.md”“.html”等格式。\n\n4.Spyder\n\nSpyder 是一款囊括了代码编辑器、编译器、调试器和图形用户界面工具的集成开发环 境 (Integrated   Development   Environment,IDE), 与 Jupyter Notebook一样是用于编写代码 的 IDE 工具。为了方便读者编写或修改代码，本书的代码使用Spyder 进行编写和调试。\n\nSpyder 的界面如图1-8所示。\n\n13\n\nSpyder (Python 3.8)\n\nFle    Edit    Search   Sourse    Run    Debug    Cansoles    Projects    Iools    View\n\n电 三 @▶    jG                                        ▶\n\nC:AUsers\\Adninistrator\\untitled0.py\n\n□ mtitled0.py*区\n\n#* 。cading:utf.0\n\nprint (产生一个9到1之间的随机数* +str(random()))\n\n工作区\n\n菜单栏\n\nHelp\n\nF\n\nC\n\nName  Type  Size\n\n查看栏\n\nHelp      Variable  explorer\n\n□| Console  1/A\n\nIPython 7.16.1 --An enhanced Interactive Python.\n\nIn        [1]:\n\n...:from       random       import       random\n\n产生 个:p1之t间('的产随生0θ 119之9 9的12641\n\nIn  [2]:\n\n输出栏\n\n状态栏\n\n?LSP    Python:ready           conda:base   Cython   3.8.3)\n\nIPython  console     ftstory\n\nLine 3,Col 39      UTF-8     CRLF        RW\n\n      图1-8  Spyder 的界面\n\n根据图1-8所示的标注， Spyder   的界面可分为菜单栏、工具栏、工作区、查看栏、输\n\n出栏和状态栏。各个区域的介绍如下。\n\n(1)菜单栏：放置所有功能和命令。\n\n(2)工具栏：放置快捷菜单，可通过菜单栏中“View”   的 “Toolbars”  复选框设置\n\n其内容。\n\n(3)工作区：编写代码的地方。\n\nPython 中文自然语言处理基础与实战\n\n(4)查看栏：查看文件、调试时的对象和变量。\n\n(5)输出栏：查看程序的输出信息并可作为 shell 终端输入Python 语句。\n\n(6)状态栏：用于显示当前文件权限、编码、鼠标指针指向位置和系统内存。\n\n菜单栏中的常用命令及说明如表1-2所示。\n\n表1 - 2 菜单栏中的常用命令及说明\n\n命令 说明 --File 文件的新建、打开、保存、关闭操作 --Edit 文件内容的编辑，如撤销、重复、复制、剪切等操作 --Run 运行命令，可选择分块运行或运行整个文件 --Consoles 可打开新的输出栏 --Tools→preferences→ IPython console “Display”可以调整字号和背景颜色；在“Graphic”下勾选“Automatical load Pylab and NumPy modules”可在IPython界面直接编写plot函数 作图；“Startup”可设置启动执行的脚本，写入要导入的程序包 --Tools→preferences→Editor “Display”主要设置背景、行号和高亮等；“Code Analysis”可以设置 代码提示\n\n小结\n\n本章主要介绍了一些与 NLP 相关的基础知识和基本概念。首先介绍了NLP 的基本概 念和发展历程；其次讲解了NLP 的研究内容和几个常见应用场景，帮助读者了解正在发展 NLP 的几个领域和 NLP 技术应用场景；接着宏观地探讨了NLP 与人工智能的关系和学习\n\nNLP 会遇到的困难；最后介绍了NLP 的基本流程和虚拟环境的创建。\n\n课后习题\n\n1. 选择题\n\n(1)政府部门利用NLP 技术分析人们对某一事件、政策法规或社会现象的评论，实时\n\n了解百姓的态度", "metadata": {}}, {"content": "，帮助读者了解正在发展 NLP 的几个领域和 NLP 技术应用场景；接着宏观地探讨了NLP 与人工智能的关系和学习\n\nNLP 会遇到的困难；最后介绍了NLP 的基本流程和虚拟环境的创建。\n\n课后习题\n\n1. 选择题\n\n(1)政府部门利用NLP 技术分析人们对某一事件、政策法规或社会现象的评论，实时\n\n了解百姓的态度，这属于NLP 研究内容的(    )。\n\nA.  信息检索    B.   文本分类    C.   信息过滤         D.   自动文摘\n\n(2)不属于NLP 应用场景的是(    )。\n\nA.  百度翻译    B.  图灵机器人  C.   微信语音转文字  D.   数据挖掘\n\n(3)中文 NLP 的基本流程由语料获取、(    )、文本向量化、模型构建、模型训练和\n\n模型评价6部分组成。\n\nA.  语料预处理  B.   中文分词    C.  去停用词        D.   词性标注\n\n( 4 ) 在 NLP  虚拟环境中安装需要的程序包，并自动安装这个包的依赖项需要用到\n\n(    )。\n\nA.pip  install  package_name                B.conda  install  package_name\n\nC.conda   package_name                      D.pip package_name\n\n14\n\n第①章 绪论\n\n(5)不属于打开 Jupyter Notebook方式的是(    )。\n\nA. 直接在开始菜单栏中单击 “Anaconda” 下的 “Jupyter Notebook” B.  在 Anaconda Prompt 中输入 “jupyter notebook”\n\nC.  单击桌面上的Jupyter Notebook 图标\n\nD. 首先打开某个文件夹，然后按住“Shift”键并单击鼠标右键，在菜单中单击 “在此处打开Powershell 窗口”命令，这时会弹出命令行窗口，接着输入 “jupyter\n\nnotebook” 命令即可\n\n2. 操作题\n\n(1)创建一个 NLP的虚拟环境。\n\n( 2 ) 用Jupyter Notebook  创建一个输出 “Hello  world!”  的文件。\n\n( 3 ) 用 Spyder 创建一个输出 “Hello  world!” 的文件。\n\n15\n\n 第②章 语料库\n\n20 世纪80 年代以来，随着计算机应用技术的不断发展，世界上的主要语言都建立了 许多对应的不同规模、不同类型的语料库。语料库的加工程度越来越深，应用范围也越来 越广，并且在NLP  中发挥着越来越重要的作用。语料库已经成为 NLP 的重要基础。本章 将介绍语料库的基本概念和语料库的种类与构建原则，并通过实例介绍涵盖大量数据集的\n\nNLP 工具NLTK 的使用方法。\n\n学习目标\n\n(1)了解语料库的基本概念和用途。\n\n(2)了解语料库的种类和构建原则。\n\n(3)熟悉NLTK 的安装步骤。\n\n(4)掌握NLTK 中函数的使用和语料库的获取。\n\n(5)掌握语料库的构建和分析流程。\n\n2.1   语料库概述\n\n语料库是为某一个或多个应用而专门收集的、有一定结构的、有代表性的、可以被计\n\n算机程序检索的、具有一定规模的语料集合。\n\n2.1.1 语料库简介\n\n语料库的实质是经过科学取样和加工的大规模电子文本库。语料库具备以下3个显著\n\n的特征。\n\n(1)语料库中存放的是真实出现过的语言材料。\n\n(2)语料库是以计算机为载体、承载语言知识的基础资源。\n\n(3)语料库是对真实语料进行加工、分析和处理的资源。\n\n语料库不仅是原始语料的集合，还是有结构并且标注了语法、语义、语音、语用等语\n\n言信息的语料集合。\n\n任何一个信息处理系统都离不开数据和知识库的支持，使用NLP 技术的系统也不例外。 在NLP 的实际项目中，通常要使用大量的语言数据或者语料。语料作为最基本的资源，尽  管在不同的 NLP 系统中所起到的作用不同，但是在不同层面上共同构成了各种 NLP 方法\n\n赖以实现的基础。\n\n第②章 语料库\n\n2.1.2 语料库的用途\n\n语料库的产生起始于语言研究，后来随着语料库功能的增强，它的用途变得越来越广。\n\n语料库的用途包括以下4个方面。\n\n(1)用于语言研究。语料库为语言学的研究提供了丰富真实的语言材料，在句法分析、 词法分析、语言理论和语言史研究中都起到了巨大的作用。如今，人们对语料库内的语料  进行了更深层次的加工处理，使得语料库为语义学、语用学、会话分析、语言变体、语音\n\n科学和心理学等研究领域提供了大量支持。\n\n(2)用于编写工具参考书籍。 一些对语言教学有重要影响的词典和语法书均是在语 料库的基础上编写的。例如，《朗曼当代英语词典》第3版的编写利用了3个大型的语料 库，分别是包含上亿词的英国国家语料库 (British   National   Corpus,BNC)、包含3000万 词的朗曼兰开斯特语料库和朗曼学习者语料库。该词典中常用词及其频率、成语、搭配\n\n和例句等都是根据这3个语料库统计出来的。\n\n(3)用于语言教学。在语言教学中，语料库可以帮助缩小课堂上学习的语言与实际使 用的语言之间的差距，发现过去被忽略的语言规律；使学习者能够更准确地理解一些词语 在实际交际中的意义和用法，发现使用语言时的一些问题。此外，语料库还可以用于语言\n\n测试、分析语言错误等。\n\n(4)用于 NLP。语料库按照一定的要求加工处理后，可以应用到 NLP 的各个层面的研 究中。语料库在词层面上进行分词、词性标注后，可以用于词法分析、拼写检查、全文检 索、词频统计、名词短语的辨识和逐词机器翻译等。语料库在句层面上进行句法标注、语 义标注后，可以用于语法检查、词义排歧、名词短语辨识的改进、机器翻译等。语料库在 语篇层面上进行语用层的处理后，可以用于解决指代问题、时态分析、目的识别、文本摘\n\n要和文本生成等。\n\n语料库包含的语言词汇、语法结构、语义和语用信息为语言学研究和 NLP研究提供了 大量的资料来源。语料库既是时代的产物，也是科技进步的成果，让处于大数据时代的人 们得以拥有和享受语料库带来的便利。语料库的产生，既丰富了语言研究中词汇的数量、 语法的形态和语句的结构，又让学习和研究语言的方式产生了巨大的变化。各种随时代而\n\n兴起的技术也有了更为准确的语言研究基础。\n\n2.2   语料库的种类与构建原则\n\n语料库的种类主要依据它的研究目的和用途进行划分。根据不同的划分标准，语料库\n\n可以分为多个种类。随着语料库不断发展，构建语料库时还需要考虑一些构建原则。\n\n2.2.1 语料库的种类\n\n以语料库结构进行划分，可将语料库分为平衡结构语料库与自然随机结构语料库；以 语料库用途进行划分，可将语料库分为通用语料库与专用语料库；以语料选取时间进行划\n\n分，可将语料库分为共时语料库与历时语料库。\n\n17\n\nPython 中文自然语言处理基础与实战\n\n1. 平衡结构语料库与自然随机结构语料库\n\n平衡结构语料库的着重点是语料的代表性和平衡性，需要预先设计语料库中语料的类 型，定义好每种类型所占的比例并按这种比例去采集语料，组成语料库。例如，历史上第 一个机读语料库——布朗语料库就是平衡结构语料库的典型代表，它的语料按3层分类， 严格设计了每一类语料所占的比例。自然随机结构语料库则是按照某个原则随机采集语料 组成语料库，如狄更斯著作语料库、英国著名作家作品语料库、北京大学开发的《人民日\n\n报》语料库等。\n\n2. 通用语料库与专用语料库\n\n通用语料库与专用语料库是基于语料库的用途划分而来。通用语料库的语料不做特殊 限定，而专用语料库的语料通常只限于某一领域或为了某种专门的目的而采集。只采集某 一特定领域、特定地区、特定时间、特定类型的语料所构成的语料库即为专用语料库", "metadata": {}}, {"content": "，如狄更斯著作语料库、英国著名作家作品语料库、北京大学开发的《人民日\n\n报》语料库等。\n\n2. 通用语料库与专用语料库\n\n通用语料库与专用语料库是基于语料库的用途划分而来。通用语料库的语料不做特殊 限定，而专用语料库的语料通常只限于某一领域或为了某种专门的目的而采集。只采集某 一特定领域、特定地区、特定时间、特定类型的语料所构成的语料库即为专用语料库，如 新闻语料库、科技语料库、中小学语料库、北京口语语料库等。通用领域与专用领域是一\n\n对相对的概念。\n\n3. 共时语料库与历时语料库\n\n共时语料库是为了对语言进行共时研究而建立的语料库，即无论所采集语料的时间段 有多长，只要研究的是一个时间平面上的元素或元素的关系，就是共时研究。共时研究所 建立的语料库就是共时语料库，如中文地区汉语共时语料库(Linguistic Variation in Chinese Speech     Communities,LIVAC) 采用共时性视窗模式，剖析来自中文地区具有代表性的定量 中文媒体语料，是一个典型的共时语料库。历时语料库是为了对语言进行历时研究而建立 的语料库，即研究一个历时切面中元素与元素关系的演化。例如，国家语言文字工作委员 会建设的国家语委现代汉语语料库，其收录的是1919年至今的现代汉语的代表性语料，是 一个典型的历时语料库。根据历时语料库得到的统计结果，是依据时间轴的等距离抽样得\n\n到的若干频次变化形成的走势图。\n\n2.2.2  语料库的构建原则\n\n从事语言研究和机器翻译研究的学者逐渐认识到语料库的重要性，国内外很多研究机 构致力于各种语料库的构建，并且正朝着不断扩大库容量、深化加工和不断拓展新的领域\n\n等方向发展。构建语料库的时候， 一般需要考虑以下4个原则。\n\n(1)代表性。在一定的抽样框架范围内采集的样本语料应尽可能多地反映真实语言现\n\n象和特征。\n\n(2)结构性。收集的语料必须是计算机可读的电子文本形式的语料集合。语料集合 结构包括语料库中语料记录的代码、元数据项、数据类型、数据宽度、取值范围、完整\n\n性约束。\n\n(3)平衡性。平衡性是指语料库中的语料要考虑不同内容或指标的平衡性，如学科， 年代，文体，地域，使用者的年龄、性别、文化背景、阅历，以及语料的用途(公函、私 信、广告)等指标。 一般在构建语料库时，需要根据实际情况选取其中的一个或者几个重\n\n要的指标作为平衡因子。\n\n18\n\n第②章  语料库\n\n(4)规模性。大规模的语料库对语言研究(特别是 NLP 研究)具有不可替代的作用， 但随着语料库的增大，垃圾语料带来的统计垃圾问题也越来越严重。此外，当语料库达到 一定的规模后，语料库的功能不一定会变得更加丰富。因此在构建语料库时，应根据实际\n\n需要决定语料库的规模。\n\n2.3   NLTK\n\nNLTK(Natural    Language   Toolkit,  自然语言处理工具包)是一个用于处理自然语言数\n\n据的Python 应用开源平台，也是基于Python  编程语言实现的NLP 库。\n\n2.3.1 NLTK 简介\n\nNLTK 是当前最为流行的自然语言编程与开发工具之一。在进行NLP 研究和应用时， 利用NLTK  中的函数可以大幅度地提高效率。NLTK  提供了超过50个素材库和词库资源的 接口，涵盖分词、词性标注、命名实体识别、句法分析等各项NLP 领域的功能。NLTK 支 持 NLP 和教学研究，它收集的大量公开数据集和文本处理库可用于文本分类、符号化、提\n\n取词根、贴标签、解析和语义推理等。 NLTK 的部分模块和功能描述如表2-1所示。\n\n表 2 - 1 、NLTK  的部分模块和功能描述\n\n模块 功能 描述 nltk.corpus 获取语料库 语料库和词典的标准化切口 nltk.tokenize、nltk.stem 字符串处理 分词、分句和提取主干 nltk.tag 词性标注 HMM、n-gram、backoff nltk.classify、nltk.cluster 分类、聚类 朴素贝叶斯、决策树、K-means nltk.chunk 分块 正则表达式、命名实体、n-gram nltk.metrics 指标评测 准确率、召回率和协议系数 nltk.probability 概率与评估 频率分布\n\n2.3.2 安装步骤\n\n本书1.3节已经介绍了 Python 开发环境的安装和环境变量的配置，以及如何在 Anaconda Prompt 里创建一个名为 “NLP”  的虚拟环境，本小节不再重复介绍。在成功安装 Python 开\n\n发环境和创建 NLP  虚拟环境的条件下， NLTK 的安装步骤如下。\n\n( 1 ) 进 入NLP 虚拟环境。在 Anaconda Prompt 命令行激活 NLP 虚拟环境，如代码2-1\n\n所示。\n\n代码2-1 激活 NLP 虚拟环境\n\nactivate               NLP\n\n当路径显示根目录由“<base>”  转变为“<NLP>”  时，说明成功进入NLP 虚拟环境。\n\n(2)安装NLTK。在 Anaconda Prompt 的 NLP 虚拟环境里安装 NLTK,  如代码2-2所示。\n\n代码2-2 安装 NLTK\n\nconda             install             nltk\n\n19\n\nPython 中文自然语言处理基础与实战\n\n当显示 “Successfully built nltk” 时，则说明 NLTK安装完成。\n\n(3)检查NLTK 是否安装成功，如代码2-3所示。\n\n代码2-3  检查NLTK 是否安装成功\n\nconda              list\n\n在显示列表中检查是否存在 “nltk”,   若存在，则说明NLTK已成功安装。\n\n(4)下载 NLTK 数据包。在成功安装 NLTK后，打开 Spyder,  新建一个文件，编写代\n\n码，下载 NLTK 数据包，如代码2-4所示。\n\n代码2-4  下载 NLTK 数据包\n\n执行代码2-4中的命令后，会显示可供下载的 NLTK数据包的对话框，如图2-1所示。\n\n图2-1 可供下载的 NLTK 数据包\n\n首先选择需要下载的数据包，如选择 “all”“all-corpora”“all-nltk”“book”“popular”\n\n“tests”“third-party”    中 的 “book”,    然后在 “Download   Directory” 中修改下载路径。下载 路径可选择为 Anaconda3 的安装位置，将NLTK 数据包放置于Anaconda3  的下级目录，如 “C:\\Anaconda3\\nltk  data”(注意：需要先在“C:\\Anaconda3”  目录下新建一个名为“nltk  data”\n\n的文件夹)。最后单击 “Download”  按钮(下载需要一些时间，需耐心等待)。\n\n下载完数据包以后，还需要进行环境变量的配置，具体步骤为：右击计算机图标，依 次单击“属性”→ “高级系统设置”→ “高级”→ “环境变量”,在“系统变量”里双击“Path”,\n\n在输入框中输入下载路径 “C:\\Anaconda3\\nltk_data”。\n\n检查 NLTK 数据包是否安装成功，如代码2-5所示。\n\n代码2-5 检查 NLTK 数据包是否安装成功\n\nfrom nltk.book import *\n\n若运行代码2-5后，输出结果出现如下内容，则表示NLTK 数据包安装成功。\n\n***Introductory                Examples                 for                the                 NLTK                 Book                ***\n\n20\n\n第②章 语料库\n\n在成功安装 NLTK 数据包之后，界面会显示NLTK 中 “book”   数据包的示例文本，其\n\n中 “text1”  的中文名为《白鲸》。\n\n2.3.3  NLTK  中函数的使用\n\n本小节以《白鲸》为例介绍NLTK 中基本函数的使用方法。在使用NLTK 中的各种函\n\n数前，需要先导入NLTK 模块并加载 book 模块下的全部文件", "metadata": {}}, {"content": "，界面会显示NLTK 中 “book”   数据包的示例文本，其\n\n中 “text1”  的中文名为《白鲸》。\n\n2.3.3  NLTK  中函数的使用\n\n本小节以《白鲸》为例介绍NLTK 中基本函数的使用方法。在使用NLTK 中的各种函\n\n数前，需要先导入NLTK 模块并加载 book 模块下的全部文件，如代码2-6所示。\n\n代码2-6 导 入 NLTK 模块并加载 book 模块下的全部文件\n\nfrom nltk.book import *\n\n(1)使用similar 函数搜索相似词语。similar  函数可以识别和搜索与指定对象相似的词 语，即查找近义词。该函数可以用在搜索引擎的相关度识别功能中。例如，在《白鲸》中\n\n搜索与 “pretty”  一词相似的词语，如代码2-7所示。\n\n代码2-7  搜索与“pretty”相似的词语\n\ntext1.similar('pretty')\n\n(2)使用 concordance  函数搜索指定内容。concordance  函数一方面可以展示全文中所 有出现指定内容的文本的位置及其上下文，另一方面可以以对齐的方式输出指定内容的上\n\n下文，便于对比分析。例如，在《白鲸》中搜索指定词语 “danger”,  如代码2-8所示。\n\n代码2-8 搜索指定词语 “danger”\n\ntext1.concordance(word='danger')\n\n(3)使用collocations  函数搜索搭配词语。collocations  函数可展示文本中多次出现的搭\n\n配词语，如代码2-9所示。\n\n代码2-9 展示文本中多次出现的搭配词语\n\ntext1.collocations()\n\n(4)使用common_contexts  函数搜索词的共同上下文。 common   contexts 函数可搜索两\n\n个或两个以上词的共同上下文，如代码2-10所示。\n\n代码2- 10  搜索词的共同上下文\n\ntext1.common_contexts(['monstrous','very'])\n\n注意，此处的多个单词需要使用中括号“[]”括起来。\n\n( 5 ) 使 用len 函数统计文本的长度，如代码2-11所示。\n\nlen(text1)                     ~代码2- 11 统计文本的长度\n\n( 6 ) 使 用set  函数获取文本的词汇表，如代码2-12所示。\n\n代码2-12 获取文本的词汇表\n\nset(text1)\n\n(7)使用sorted  函数对词汇表按照英文字母排序，如代码2-13所示。\n\n代码2-13 对词汇表按照英文字母排序\n\nsorted(set(text1))            #  词汇表排序\n\n21\n\nPython 中文自然语言处理基础与实战\n\n(8)使用 FreqDist 函数查询文本中的词汇频数分布，如代码2-14所示。\n\n代码2-14  查询文本中的词汇频数分布\n\n(9)使用 sorted 函数对词汇表中的词按照英文字母进行排序，如代码2-15所示。\n\n代码2-15 对词汇表中的词按照英文字母进行排序\n\n(10)使用 dispersion_plot 函数可绘制出指定词的分布及其在文本中出现的位置，如代\n\n码2-16所示。\n\n代码2-16  绘制词汇分布离散图\n\n(11)使用 fdist.plot 函数可绘制指定的常用词累计频率图。例如，绘制文本 text1 中常\n\n用词累计频率图，如代码2-17所示。得到的文本中常用词累计频率图如图2-2所示。\n\n代码2- 17  绘制文本中常用词累计频率图\n\n在图2-2中，横坐标表示的是文本中出现的词(按出现次数从小到大排序),纵坐标表\n\n示词在文本中出现的频次，如 “that”  这个词在文本中出现了70000多次。\n\n22\n\n第 ② 章  语料库\n\n常用词累计频率图\n\n常用词\n\n图2-2 文本中常用词累计频率图\n\n本小节给出了NLTK 中几个基本函数的示例，但NLTK 中的函数远不止此，若读者还\n\n想尝试更多的函数，可以查阅相关资料进行学习。\n\n2.4   语料库的获取\n\n除了自行构建语料库之外，还有许多已经构建好的语料库可以直接获取使用。NLTK\n\n中就集成了多个文本语料库。除此之外，还有许多在线语料库被共享出来供人们使用。\n\n2.4.1  获 取 NLTK 语料库\n\nNLTK 中有多个文本语料库，其中包含古腾堡项目(数字图书馆)电子文档的一小部分 文本、网络聊天文本、即时消息聊天会话语料库、布朗语料库、路透社语料库、就职演说语 料库、标注文本语料库和其他语言语料库等。NLTK 中定义了许多基本语料库函数，如表2-2\n\n所示。\n\n表2 - 2 基本语料库函数\n\n函数 说明 fileids() 获取语料库中的文件 fileids([categories] 分类对应的语料库中的文件 categories() 语料库中的分类 categories([fileids]) 文件对应的语料库中的分类 raw() 语料库的原始内容 raw([fileids=[fl,f2,f3]) 指定文件的原始内容 raw(categories=[c1,c2]) 指定分类的原始内容 words() 查找整个语料库中的词汇 words(fileids=[fl,f2,f3]) 指定文件中的词汇 words(categories=[cl,c2]) 指定分类中的词汇\n\n23\n\nPython 中文自然语言处理基础与实战\n\n续表\n\n函数 说明 sents) 指定分类中的句子 sents(fileids=[fl,f2,f3]) 指定文件中的句子 sents(categories=[cl,c2]) 指定分类中的句子 abspath(fileid) 指定文件在磁盘上的位置 encoding(fileid) 文件编码 open(fileid) 打开指定语料库文件的文件流 root() 到本地安装的语料库根目录的路径 readme() 语料库中的README文件的内容\n\n1. 获取古腾堡语料库\n\nNLTK 包含古腾堡项目电子文档的一小部分文本，该项目电子文档大约包含36000 本 免费电子书。获取古腾堡语料库文本需要先加载NLTK,   然后调用fileids  函数获取文本，\n\n如代码2-18所示。\n\n代码2-18 获取古腾堡语料库文本\n\n运行代码2-18后，输出结果如下。\n\n输出结果是NLTK 包含的古腾堡语料库文本，可以对其中的任意文本进行如下操作。\n\n(1)打开文件并统计词数，如代码2-19所示。\n\n代码2- 19  打开文件并统计词数\n\n24\n\n第 ② 章  语料库\n\nlen(emma)     # 统计词数\n\n运行代码2-19后，输出结果如下。\n\n['[','Emma','by','Jane','Austen','1816',']',\n\n192427\n\n(2)索引文本，如代码2-20所示。\n\n代码2-20 索引文本\n\n运行代码2-20后，输出结果如下。\n\n(3)获取文本的标识符、词、句。使用 nltk.corpus.gutenberg.fileids 方法获取古腾堡语\n\n料库的所有文本，然后获取这些文本的统计信息，如代码2-21所示。\n\n代码2-21  获取古腾堡语料库中的所有文本及其统计信息\n\n运行代码2-21 后，输出结果如下。\n\n25\n\nPython 中文自然语言处理基础与实战\n\n2. 获取网络聊天文本\n\nNLTK 包含很多的网络文本小集合，如 Firefox交流论坛、在纽约无意中听到的对话、《加\n\n勒比海盗》电影剧本、个人广告和葡萄酒的评论等。访问网络聊天文本内容的步骤如下。\n\n(1)获取网络聊天文本，如代码2-22所示。\n\n代码2-22 获取网络聊天文本\n\n运行代码2-22后，部分输出结果如下。\n\n(2)查看网络聊天文本信息，如代码2-23所示。\n\n代码2-23 查看网络聊天文本信息\n\n运行代码2-23后，输出结果如下。\n\n3. 获取即时消息聊天会话语料库\n\n即时消息聊天会话语料库是为研究自动检测互联网入侵者而构建的，其包含的帖子超 过1000个", "metadata": {}}, {"content": "，如代码2-22所示。\n\n代码2-22 获取网络聊天文本\n\n运行代码2-22后，部分输出结果如下。\n\n(2)查看网络聊天文本信息，如代码2-23所示。\n\n代码2-23 查看网络聊天文本信息\n\n运行代码2-23后，输出结果如下。\n\n3. 获取即时消息聊天会话语料库\n\n即时消息聊天会话语料库是为研究自动检测互联网入侵者而构建的，其包含的帖子超 过1000个，被分成了15个文件，每个文件包含几百个从特定日期和特定年龄的聊天室收 集而来的帖子。文件名包含了日期、聊天室和帖子的数量。获取即时消息聊天会话语料库，\n\n如代码2-24所示。\n\n代码2-24  获取即时消息聊天会话语料库\n\n运行代码2-24后，输出结果如下。\n\n26\n\n第②章 语料库\n\n4. 获取布朗语料库\n\n布朗语料库是第一个百万词级的英语电子语料库，其中包含500个不同来源的文本， 如新闻、社论和科学小说等。该语料库主要用于研究文体之间的系统性差异(因此又叫作\n\n文体学的语言学研究),可以将该语料库作为词链表或者句子链表进行访问。\n\n(1)查看语料，按特定类别或文件阅读，如代码2-25所示。\n\n代码2-25 查看语料\n\n运行代码2-25后，输出结果如下。\n\n(2)比较不同文体之间情态动词的用法，如代码2-26所示。\n\n代码2-26  比较不同文体之间情态动词的用法\n\n运行代码2-26后，输出结果如下。\n\ncan:94\n\ncould:87\n\nmay:93\n\nmight:38\n\nmust:53\n\nwill:389\n\n5. 获取路透社语料库\n\n路透社语料库包含10788个新闻文档，共计130万字。该类型文档可以分成90个主题， 按照训练和测试分为两组，这样分组是为了方便运用训练和测试算法自动检测文档。与布\n\n朗语料库不同，路透社语料库的类别是相互重叠的，原因是它的新闻报道往往涉及多个主\n\n27\n\nPython 中文自然语言处理基础与实战\n\n题。此外，还可以查找某个或多个文档涵盖的主题，以及查找包含在一个或者多个类别中\n\n的文档。\n\n(1)查看语料文档编号，如代码2-27所示。\n\n代码2-27  查看语料文档编号\n\n运行代码2-27后，输出结果如下。\n\n['test/14826','test/14828','test/14829','test/14832','test/14833']\n\n(2)查看某编号下的语料信息，如代码2-28所示。\n\n代码2-28 查看某编号下的语料信息\n\nreuters.categories('test/14832')\n\n运行代码2-28后，输出结果如下。\n\n['corn','grain','rice','rubber','sugar','tin','trade']\n\n(3)查看指定类别下的编号文件，如代码2-29所示。\n\n代码2-29 查看指定类别下的编号文件\n\nprint(reuters.fileids(['barley','corn']))\n\n运行代码2-29后，部分输出结果如下。\n\n6. 获取就职演说语料库\n\n就职演说语料库是由55个文本组成的集合，每个文本都是一个领导人的演说内容。该\n\n集合的一个显著特征是时间维度。\n\n(1)查看语料信息，如代码2-30所示。\n\n代码2-30 查看语料信息\n\n运行代码2-30后，部分输出结果如下。\n\n28\n\n第②章 语料库\n\n(2)查看演说语料的年份。每个文本的年份都会出现在它的文件名中，若要从文件名中\n\n提取年份，则需要使用fileid[:4]提取文件名的前4个字符，如代码2-31所示。\n\n代码2-31 提取年份\n\n(3)绘制指定单词的时间分布图，如代码2-32所示。得到的 “free”  和 “citizen”  的时\n\n间分布图如图2-3所示。\n\n代码2-32 绘制指定单词的时间分布图\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.sans-serif']='SimHei!#                                           设置中文显示\n\nplt.grid()\n\ntt                                     =['citizen','free']\n\nstyle                        =[¹--','-']\n\nfor            fjd,kist  =idn ict(czfipd(t,tl)e):\n\ni []=sorted(fdist.items(),key  =lambda  x:x[0])\n\ny=[]\n\nfor  i  in  range(len(fdist)):\n\nx.append(fdist[i][0])\n\ny.append(fdist[i][1])\n\nle=k,label=j)\n\nplt.ylabel(       '计数')\n\nplt.legend()\n\nplt.show()\n\n图2-3所示为单词 “free”  和 “citizen”  随时间变化的使用情况，在就职演说语料库中， 所有以 “free”  或 “citizen”  开头的单词都将被计数。将每个演讲单独计数并绘制出图形，\n\n能够观察出这些单词随时间变化的使用情况变化。\n\n29\n\nPython 中文自然语言处理基础与实战\n\n年份\n\n图2-3 “free” 和 “citizen” 的时间分布图\n\n2.4.2  获取网络在线语料库\n\nNLP 研究技术少不了语料库的支持。国内外已有一些著名的在线语料库网站，如搜狗\n\n实验室数据资源、中文 NLP 语料库项目、古腾堡语料库等。\n\n1. 查阅网络在线语料库的内容\n\n以在线古腾堡语料库为例，打开古腾堡语料库网页，单击左侧 “Browse Catalog” 选项， 进入 “Online Book  Catalog-Overview” 网页，在 “Languages with more than 50 books”  中选 择 “Chinese”,   进入 “Browse  By  Language:Chinese”  网页，可以发现该网页中包含大量的 中文在线语料库。这些语料以作者姓名(英文)排序，如《红楼梦》作者 Cao Xueqin、《西\n\n游记》作者 Wu  Cheng'en。可以通过姓名查找需要的语料。\n\n查看《西游记》网页内容，如代码2-33所示。\n\n代码2-33 查看《西游记》网页内容\n\n运行代码2-33后，输出的结果如下。\n\n30\n\n第②章 语料库\n\n查看《红楼梦》网页内容，如代码2-34所示。\n\n代码2 - 34  查看《红楼梦》网页内容\n\n运行代码2-34后，输出结果如下。\n\n2. 获取网络在线语料库文本\n\n打开古腾堡语料库网页，进入 “Browse By Language:Chinese”  网页，分别找到《三国\n\n志》和《窦娥冤》,单击后进入，在 “Download This eBook”  中选择 “Plain Text UTF-8”,\n\n分别获得《三国志》和《窦娥冤》的文本网址。\n\n查看《三国志》文本，如代码2-35所示。\n\n代码2-35 查看《三国志》文本\n\n运行代码2-35后，输出结果如下。\n\n31\n\nPython 中文自然语言处理基础与实战\n\n运行代码2-36后，输出结果如下。\n\n2.5    任务：语料库的构建与应用\n\n本节构建作品集语料库，并在构建完之后对该语料库进行简单的分析。\n\n2.5.1 构建作品集语料库\n\n在构建作品集料库之前，需要下载作品集的文本(如金庸先生部分作品),完成数据采\n\n集和预处理工作，获取保存的文件列表，如代码2-37所示。\n\n代码2-37 获取保存的文件列表\n\n运行代码2-37后，输出的结果如下。\n\n构建完语料库之后，可以利用NLTK 中的基本函数进行搜索相似词语、指定内容、搭\n\n配词语", "metadata": {}}, {"content": "，并在构建完之后对该语料库进行简单的分析。\n\n2.5.1 构建作品集语料库\n\n在构建作品集料库之前，需要下载作品集的文本(如金庸先生部分作品),完成数据采\n\n集和预处理工作，获取保存的文件列表，如代码2-37所示。\n\n代码2-37 获取保存的文件列表\n\n运行代码2-37后，输出的结果如下。\n\n构建完语料库之后，可以利用NLTK 中的基本函数进行搜索相似词语、指定内容、搭\n\n配词语，查询文本词汇频数分布等相应操作。\n\n32\n\n第②章 语料库\n\n2.5.2  武侠小说语料库分析\n\n利用下载的《神雕侠侣》语料构建武侠小说语料库并进行分析，具体实现步骤如下。\n\n1. 读取本地语料\n\n导入《神雕侠侣》语料，在没有重复词的条件下，统计《神雕侠侣》语料中总用词量\n\n和平均每个词的使用次数，如代码2-38所示。\n\n代码2-38 统计《神雕侠侣》语料中总用词量和平均每个词的使用次数\n\n运行代码2-38后，输出的结果如下。\n\n可看到《神雕侠侣》语料总共使用了4109个词，平均每个词使用248次。\n\n2. 查询词频\n\n分别查看《神雕侠侣》语料中的“小龙女”“杨过”“雕”“侠”的使用次数，如代\n\n码2-39所示。\n\n代码2-39 查看《神雕侠侣》语料中的“小龙女”“杨过”“雕”“侠”的使用次数\n\n运行代码2-39后，输出的结果如下。\n\n3. 查看《神雕侠侣》语料中的部分文本\n\n查看《神雕侠侣》语料中的部分文本，如代码2-40所示。\n\n代码2-40 查看《神雕侠侣》语料中的部分文本\n\nstr[5394:6008]    # 查看《神雕侠侣》语料中的部分文本\n\n运行代码2-40后，输出的结果如下。\n\n33\n\nPython 中文自然语言处理基础与实战\n\n4. 统计并输出前30个高频词和高频标识符次数\n\n统计并输出前30个高频词和高频标识符次数，如代码2-41所示。\n\n代码2-41 统计并输出前30个高频词和高频标识符次数\n\n运行代码2-41后，输出的结果如下。\n\n5. 查询词频在指定区间内的词数量\n\n查询词频在指定区间内的词数量，如代码2-42所示。\n\n代码2-42 查询词频在指定区间内的词数量\n\n运行代码2-42后，输出的结果如下。\n\n6. 使 用jieba 进行分词\n\nNLTK 虽自带了很多统计的功能，但是部分函数只能处理英文语料，对中文语料并不 适用。为了使用这些函数，需要对中文进行预处理。首先对中文进行分词，然后将分词的 文本封装成 NLTK 的 “text”  对象，最后再使用 NLTK 中的函数进行处理。分词的目的是 为 NLTK 的 “text”   对象提供封装的语料，这里使用jieba 的 lcut  函数进行分词 (jieba  的\n\n使用将在第4章介绍),如代码2-43 所示。\n\n代码2-43  使 用Icut 函数进行分词\n\n34\n\n第②章  语料库\n\n运行代码2-43后，输出的结果如下。\n\n<Text: 全本全集精校小说尽在更多 . . .>\n\n7. 查看指定单词上下文\n\n查看指定单词上下文，如代码2-44所示。\n\n代码2-44  查看指定单词上下文\n\ntext.concordance(word='侠',width=30,lines=3)\n\n运行代码2-44后，输出的结果如下。\n\n8. 搜索相似词语\n\n搜索相似词语，如代码2-45所示。\n\n代码2-45  搜索相似词语\n\ntext.similar(word=        '李莫愁',num=10)\n\n运行代码2-45后，输出的结果如下。\n\n他杨过她小龙女我你国师郭靖周伯通陆无双\n\n9. 绘制词汇离散图\n\n绘制词汇离散图，如代码2-46所示。\n\n代码2-46  绘制词汇离散图\n\nimport matplotlib as mpl\n\nmpl.rcParams['font.sans-serif']=['SimHei']\n\nwords=[ '小龙女','杨过','郭靖',‘黄蓉']\n\nnltk.draw.dispersion.dispersion_plot(text,words,title=\n\n小结\n\n本章主要介绍语料库的相关知识和 NLTK 中的部分函数。首先是语料库的基本概述， 包括其用途和意义；然后对语料库的种类和构建原则进行逐点阐述；接着详细介绍 NLTK 的安装、使用和获取语料库的方法；最后实现语料库的构建与应用，构建金庸先生作品集\n\n语料库并对语料进行分析。\n\n实训\n\n实 训 1 构建语料库\n\n1.  训练要点\n\n掌握语料库中的文本的获取方法。\n\n35\n\nPython 中文自然语言处理基础与实战\n\n2. 需求说明\n\n语料库是标注了语法、语义、语音、语用等语言信息的语料集合，在对语料库进行分\n\n析之前需要先构建语料库，获取保存的文件列表。\n\n3. 实现思路与步骤\n\n(1)设置本地存放作品集文本的目录。\n\n(2)调用 fileids 函数获取文本。\n\n实训2  《七剑下天山》语料库分析\n\n1.   训练要点\n\n(1)掌握读取本地语料的方法。\n\n(2)掌握查询词频方法。\n\n(3)掌握查看语料中部分文本的方法。\n\n(4)掌握统计并输出高频词和高频标识符次数的方法。\n\n(5)掌握查询词频在指定区间内的词数量的方法。\n\n(6)掌握使用jieba 进行分词的方法。\n\n(7)掌握查看指定单词上下文的方法。\n\n(8)掌握搜索相似词语的方法。\n\n(9)掌握绘制词汇离散图的方法。\n\n2. 需求说明\n\n根据构建的作品集语料库，对语料库进行分析，查看文本、查询词频、统计并输出高\n\n频词和高频标识符次数、查看指定单词上下文、搜索相似词语和绘制词汇离散图。\n\n3. 实现思路与步骤\n\n(1)使用read 函数读取作品集文本。\n\n(2)使用count 函数查询词频。\n\n(3)使用字符串切片方法查看部分文本。\n\n(4)使用FreqDist 函数查询文本中的词汇频数分布。\n\n(5)使用len 函数统计文本的长度。\n\n(6)使用jieba进行分词。\n\n(7)使用 concordance 函数查看指定单词的上下文\n\n(8)使用 similar 函数搜索相似词语。\n\n(9)使用dispersion_plot 函数绘制指定词分布情况离散图。\n\n课后习题\n\n1. 选择题\n\n(1)语料库以语料库结构进行划分可分为(    )。\n\nA.  通用语料库与专用语料库\n\nB.  平衡结构语料库与自然随机结构语料库\n\n36\n\n第②章 语料库\n\nC.  共时语料库与历时语料库\n\nD.  单媒体语料库与多媒体语料库\n\n(2)构建或研究语料库的时候", "metadata": {}}, {"content": "， 一般应考虑代表性、结构性、平衡性、(     )4\n\n个特性。\n\nA.  规模性      B. 便捷性      C.   安全性          D.   高效性\n\n(3)NLTK   的安装步骤为(    )。\n\nA.  安装 NLP 虚拟环境→安装 NLTK→检查是否存在NLTK→下载 NLTK 数据包 B.  安装 NLTK→安装 NLP虚拟环境→检查是否存在NLTK→下载 NLTK 数据包\n\nC.  安装 NLP 虚拟环境→安装 NLTK→下载 NLTK 数据包→检查是否存在NLTK\n\nD.  下载 NLTK数据包→安装 NLP 虚拟环境→安装 NLTK→检查是否存在NLTK\n\n(4)(     )函数用于搜索搭配词语。\n\nA.concordance                                              B.common                  contexts\n\nC.collocations                                               D.sorted\n\n(5)(      )函数用于获取语料库中的文件。\n\nA.categories           B.raw                                  C.open(fileid)                 D.fileids\n\n2. 操作题\n\n(1)安装NLTK,  并完成本章中所有代码操作。\n\n(2)访问在线古腾堡语料库，获取《伤寒杂病论》《孔雀东南飞》等网络数据资源。\n\n(3)构建一个本地语料库，并对语料进行分析。\n\n37\n\n 第③章 正则表达式\n\n在进行NLP 的过程中，经常需要从文本或字符串中抽取出想要的信息，用于进一步做 语义理解或其他处理。正则表达式是一种从文本中抽取信息的有效手段，它一般通过搜索 匹配特定模式的语句实现信息的抽取。正则表达式的应用范围非常广，如解析/替代字符串、 预处理数据和网页爬取。本章介绍正则表达式中的函数和元字符，并通过实例演示NLP中\n\n正则表达式的应用。\n\n学习目标\n\n(1)了解正则表达式的概念和正则表达式函数。\n\n(2)熟悉正则表达式的元字符。\n\n(3)掌握正则表达式的应用。\n\n3.1 正则表达式的概念\n\n正则表达式是一个可以用于模式匹配和替换的工具，它是一种小巧的、高度专用的编 程语言。使用正则表达式，可以对指定的文本实现匹配测试、内容查找、内容替换、字符\n\n串分割等功能。\n\nNLP 的文本信息通常有两种， 一种是文本格式的文档，另一种是来自网页端的信息。 文本格式的文档大多由人为或系统编写生成，包括结构化文本、半结构化文本、非结构化 文本。非结构化文本和半结构化文本难以挖掘出信息，而正则表达式可以将非结构化文本 转化为结构化文本。来自网页端的文本中有很多 HTML的标签，需要去掉。正则表达式能\n\n够在复杂的文本信息中提取出需要的关键信息，它是NLP中处理文本常用的基础手段之一。\n\n3.1.1 正则表达式函数\n\n正则表达式由一些普通字符和一些元字符组成。普通字符包括大小写字母、数字和 打印符号，而元字符是具有特殊含义的字符。正则表达式的设计思想是用一种描述性的 语言给字符串定义一个规则，凡是符合规则的字符串就认为它“匹配”了，否则就是匹\n\n配不成功。\n\nPython中的 re 库提供了一个正则表达式引擎接口，它将正则表达式编译成模式对象， 然后通过这些模式对象执行模式匹配搜索、字符串分割和字符串替换等操作。常用的正则\n\n表达式函数如下。\n\n第③章  正则表达式\n\n1.match 函数\n\nmatch  函数用于检测字符串开头部分是否匹配，若匹配成功则返回结果，否则返回 None。match  函数的格式为 re.match(pattern,string,flags), 第一个参数是正则表达式，如果 匹配成功，则返回一个 Match,  否则返回一个None;  第二个参数表示需要匹配的字符串； 第三个参数是标志位，用于控制正则表达式的匹配方式，如flags=0 表示不进行特殊指定， 不区分字母大小写。需要特别注意的是， match  函数并不是完全匹配，它要求必须从字 符串的开头进行匹配，如果字符串的开头不匹配，则整个匹配失败。使用 match 函数匹\n\n配文本，如代码3-1所示。\n\n代码3-1  使 用 match 函数匹配文本\n\n运行代码3-1后，输出的结果如下。\n\n如果要查找以“自然语言处理”开头的句子，那么可以先对文本 text1 进行切分，然后\n\n再进行匹配，如代码3-2所示。\n\n代码3-2 对文本 text1 进行切分后进行匹配\n\n运行代码3-2后，输出的结果如下。\n\n自然语言处理是研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法\n\n2.search 函数\n\nsearch  函数用于在整个字符串内查找符合对应模式的字符串并进行匹配，找到第一个 匹配对象后返回一个包含匹配信息的对象，如果字符串中没有能够匹配的对象，则返回 None.search函数的格式为re.search(pattern,string,flags) 。search函数与match函数不同，search 函数并不要求必须从字符串的开头进行匹配，也就是说，正则表达式可以是字符串的一部\n\n分。使用 search 函数进行匹配，如代码3-3所示。\n\n代码3-3 使用 search 函数进行匹配\n\nprint(re.search('通信',text1))#  返回一个包含匹配信息的对象\n\n运行代码3-3后，输出的结果如下。\n\n<re.Match object;span=(28,30),match='通信'>\n\n3.findall 函数\n\nfindall  函数返回的是正则表达式在字符串中所有匹配结果的列表。findall   函数的格式\n\n为 re.findall(pattern,string,flags)。如果匹配成功，那么将会返回字符串 string 中与 pattern 相匹\n\n39\n\nPython 中文自然语言处理基础与实战\n\n配的全部字符串，且返回形式是一个列表；如果匹配失败，那么将会返回一个空列表。使\n\n用 findall  函数进行匹配，如代码3-4所示。\n\n代码3-4  使用 findall 函数进行匹配\n\nprint(re.finda         ll(    '计算机',text1))#            返回一个列表\n\n运行代码3-4后，输出的结果如下。\n\n['计算机','计算机']\n\n4.sub  函数\n\nsub  函数为替换函数，能够找到所有匹配的字符串并将其替换成指定的字符串。sub 函 数的格式为 re.sub(pattern,repl,string) 。 如果字符串 string 包含了 pattern,  那么会将匹配到的\n\n字符串替换成 repl 。使 用 sub 函数替换指定文本，如代码3-5所示。\n\n代码3-5 使用 sub 函数替换指定文本\n\nprint(re.sub(         '自然语言处理','NLP',text1)\n\n运行代码3-5后，输出的结果如下。\n\n3.1.2  正则表达式的元字符\n\n元字符由特殊符号组成，元字符的应用是正则表达式强大的原因。元字符定义了字符 集合、子组匹配、模式重复次数。元字符使得正则表达式不仅可以匹配一个字符串，还可\n\n以匹配字符串集合。\n\n1. 字符匹配\n\n(1)英文句号“”表示匹配除换行符“\\n”  之外的任意一个字符。使用英文句号“. ”\n\n进行匹配，如代码3-6所示。\n\n代码3-6  使用英文句号“”进行匹配\n\nprint(re.findall(             '自.语言处理’,text1))#           匹配未知字符“然”\n\n运行代码3-6后，输出的结果如下。\n\n['自然语言处理',‘自然语言处理\n\n(2)中括号“[]”表示匹配多个字符，中括号内部的所有字符都会被匹配。使用中括\n\n号“[]”进行匹配，如代码3-7所示。\n\n代码3-7 使用中括号“[]”进行匹配\n\nprint(re.findall(’ [科数]学',text1))#   匹配中括号内的任意一个字符\n\n运行代码3-7后，输出的结果如下。\n\n['科学',‘数学',‘科学!]\n\n(3)竖线“”用于对两个正则表达式进行“或”操作。如果A 和 B 是正则表达式，则\n\nA|B 会匹配A 或 B 中出现的任何字符。使用竖线“”进行匹配，如代码3-8所示。\n\n代码3-8 使用竖线“|”进行匹配\n\nprint(re.findall(  '方法|计算机',text1))\n\n运行代码3-8后", "metadata": {}}, {"content": "，输出的结果如下。\n\n['科学',‘数学',‘科学!]\n\n(3)竖线“”用于对两个正则表达式进行“或”操作。如果A 和 B 是正则表达式，则\n\nA|B 会匹配A 或 B 中出现的任何字符。使用竖线“”进行匹配，如代码3-8所示。\n\n代码3-8 使用竖线“|”进行匹配\n\nprint(re.findall(  '方法|计算机',text1))\n\n运行代码3-8后，输出的结果如下。\n\n['计算机','方法',’计算机']\n\n40\n\n第③章\n\n输出含有“方法”或“计算机”的句子，如代码3-9所示。\n\n正则表达式\n\n41\n\n代码3-9 输出含有“方法”或“计算机”的句子\n\n运行代码3-9后，输出的结果如下。\n\n(4)乘方符号“^”表示匹配字符串起始位置的内容，如“^自”表示匹配所有以“自”\n\n开头的字符串，如代码3-10所示。\n\n代码3-10  匹配所有以“自”开头的字符串\n\n运行代码3-10后，输出的结果如下。\n\n自然语言处理是研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法\n\n(5)美元货币符号“$”表示匹配字符串的结束位置的内容，如“学$”表示匹配所有\n\n以“学”为结尾的字符串，如代码3-11所示。\n\n代码3-11 匹配所有以“学”为结尾的字符串\n\n运行代码3-11后，输出的结果如下。\n\n(6)量化符有“?”“*”“+”“{n}”“{n,}”“{n,m}”。在正则表达式中，可以通过量\n\n化符匹配需要的字符数。量化符的说明如表3-1所示。\n\n表 3 - 1 量化符的说明\n\n量化符 说明 ? 前面的元素可选，并且最多匹配1次 * 前面的元素会被匹配0次或多次 + 前面的元素会被匹配1次或多次 {n} 前面的元素会被匹配n次 {n,} 前面的元素至少会被匹配n次 {n,m} 前面的元素至少匹配n次，至多匹配m次\n\nPython 中文自然语言处理基础与实战\n\n量化符的常见具体用法示例，如代码3-12所示。\n\n代码3-12  量化符的常见具体用法示例\n\n#唐初著名诗人刘希夷的诗《代悲白头翁》其中两句\n\ntext2       ='今年花落颜色改，明年花开复谁在?年年岁岁花相似，岁岁年年人不同。’ re.findall(        '年?',text2)#           “年”最多重复1次\n\nprint(re.findall(             '年*',text2))#            “年”可以重复0或多次\n\n42\n\nre.finda      ll( re.findall( re.findall( re.findal1( '年+',text2)# '年{1}',text2)# '年{2}',text2)# '年(0,1}',text2)#\n\nre.findall(           '年 .+', text2)\n\nre.findall( '年+.',text2) re.findall('年.?',text2)\n\nre.f    indall(      '年 . *',text2)#\n\nre.findall('年.+?',text2)\n\n“年”可以重复1次或多次\n\n“年”重复1次\n\n“年”重复2次\n\n“年”至少重复0次，至多重复1次\n\n#  以“年”开始，后面可以跟任意多个字符\n\n#“年”可以重复1次或多次，后面跟任意字符\n\n#“年”后面至多可以跟1个任意字符\n\n“年”后面可以跟任意多个字符\n\n#“年”后面可以跟1个任意字符，并且这两个字符最多重复1次\n\nre.findall(              ' 年 . * ? ' , text2)            #  “年”后面允许不带其他字符的内容\n\nre.findall( '年?花', text2)   #“花”前面的“年”最多重复1次\n\nre.findall('年*花',text2)   #  “花”前面的“年”可以重复0或多次\n\nre.findall('年+花',text2)   # “花”前面的“年”可以重复1次或多次\n\nre.findall('年{1}花',text2)   #“花”前面的“年”重复1次\n\nre.findall('年{2}花',text2)   #“花”前面的“年”重复2次\n\nre.findall(       '年{0,1}花',text2)         #“花”前面的“年”至少重复0次，至多重复1次\n\nre.findall(  '年 . +花',text2)#      “年”开头、“花”结尾且中间的任意字符可以任意多个\n\nre.findall(1年.?花',text2)   #   “年”开头、“花”结尾且中间的任意字符至多一个\n\nre.findall(           '年 .*花',text2)#             “年”开头、“花”结尾且中间的任意字符可以任意多个   re.findall( '年.+?花',text2)    #“年”开头、“花”结尾且中间至少带有一个字符的内容\n\nre.findall( '年.*?花',text2)        “年”开头、“花”结尾且中间允许不带其他字符的内容\n\n运行代码3-12后，输出的结果如下。\n\n['',‘年','','’,'',’,'‘,    ” , 年 ， , , , ” , ” , 心 ， “ 年 年 ‘ , ”\n\n',  '','',    '',','’,         ,  1 年 年 ' , ’ , , ‘ ’ , , ‘ “ ]\n\n['年花',1年花!,1年年岁岁花']\n\n在代码3-12中，出现了“+?”和“*?”的复合用法，这种复合用法在字符匹配中是 一种比较常见的使用方法。例如， “A.+?B” 表示匹配 “A”  开头、“B”  结尾且中间至少带 有一个字符的内容，“A.*?B”  表示匹配 “A”  开头、“B”   结尾且中间允许不带其他字符的\n\n内容。\n\n2. 转义字符“\\”\n\n字符串中可以包含任何字符，如果待匹配的字符串中出现“$”“.”“[]”等特殊字符，  那么这将会与正则表达式的特殊字符发生冲突。遇到这种情况时，在 Python 中可以使用“\\” 对字符串内的特殊符号进行转义，即告诉 Python  把这个字符当作普通字符处理。“”是用   于转义的，如果字符串包含“”,那么需要使用“\\”将“\\”转义。 Python  中的一些预定义\n\n字符如表3-2所示。\n\n第③章 正则表达式\n\n表3 - 2 Python  中的一些预定义字符\n\n预定义字符 说明 \\w 字、字母、数字 \\W 与\\w反义，非字、非字母、非数字 \\s 空白字符 \\S 非空白字符 \\d 数字 \\D 非数字 \\b 单词的边界 \\B 非单词的边界\n\n在正则表达式中，对于一个反斜杠“”,需要用两个反斜杠 “I”  表示。例如，对于数 字“\\d”,  需要用“\\ld”  表示。这样操作比较烦琐，Python  中自带的原生字符 “r”  可以简 化操作。对于文本中的“\\”,只需要用 “rN” 表示即可，如 “ld”   可以写成 “r\\d'” 。 在原\n\n生字符的帮助下，书写正则表达式变得非常方便。\n\n转义字符“\\”的使用示例如代码3-13所示。\n\n代码3-13 转义字符“\\”的使用示例\n\n运行代码3-13后，输出的结果如下。\n\n['Hello','everyone','co']\n\n3.2  任务：正则表达式的应用\n\n本节演示如何使用正则表达式在实际应用中处理文本内容，包括过滤字符、提取特定\n\n的文本等。\n\n3.2.1 《西游记》字符过滤\n\n中文文本中经常有很多特殊的字符，如中文符号、英文符号、数字等。例如，《西游记》\n\n文本中就含有大量的特殊字符，查看《西游记》的部分文本内容", "metadata": {}}, {"content": "，输出的结果如下。\n\n['Hello','everyone','co']\n\n3.2  任务：正则表达式的应用\n\n本节演示如何使用正则表达式在实际应用中处理文本内容，包括过滤字符、提取特定\n\n的文本等。\n\n3.2.1 《西游记》字符过滤\n\n中文文本中经常有很多特殊的字符，如中文符号、英文符号、数字等。例如，《西游记》\n\n文本中就含有大量的特殊字符，查看《西游记》的部分文本内容，如代码3-14所示。\n\n代码3-14 查看《西游记》的部分文本内容\n\nfrom urllib.request import urlopen\n\nhtml1  =urlopen(urll).read()\n\nhtml1 =html1.decode('utf-8')\n\n43\n\nPython 中文自然语言处理基础与实战\n\nt ext4                =htmll[2269:2450]#\n\nprint(text4)\n\n运行代码3-14后，输出的结果如下。\n\n在进行中文分词前，要求数据格式全部是中文，因此需要过滤掉特殊符号、标点符号， 英文、数字等。读者也可以根据自己的需求过滤自定义字符。过滤中文文本特殊符号的常\n\n用正则表达式示例如代码3-15所示。\n\n代码3-15 过滤中文文本特殊符号的常用正则表达式示例\n\n运行代码3-15后，输出的结果如下。\n\n3.2.2  自动提取人名与电话号码\n\n例如，有文本为“J.Done:234-555-1234 J.Smith:(888)555-1234A.Lee:(810)555-1234M\n\nJones:666.555.9999”, 需要提取该文本中的姓名和电话号码。\n\n观察文本发现，文本中的人名都有一定规则(大写字母+.+空格+大小写字母),而电话 号码也有一定规则，都是数字并且带有“()”、“-”或“”这些字符。提取文本中的姓名时 首先要匹配第一个大写字母的位置“[A-Z]”,   待匹配的名字中第二个字符开始都只包含英 文句号、空格或字母，就可以统一采用“[\\a-zA-Z]”  表示，然后用“+”表示允许匹配多个。\n\n使用正则表达式提取姓名，如代码3-16所示。\n\n代码3-16 使用正则表达式提取姓名\n\n44\n\n第 ③ 章 正则表达式\n\n运行代码3-16后，输出的结果如下。\n\n电话号码是以“(”或数字开头，表示为“[0-90”;第二个字符开始则是接着数字、“-”、 “)”或英文句号，表示为“[0-9-].]”,最后用“+”表示允许匹配多个。使用正则表达式提\n\n取电话号码，如代码3-17所示。\n\ntel     =re.findall('[0( 9-使)用]+正',)式提取电话号码\n\nfor       i       in       tel:\n\nprint(i)#              输出电话号码\n\n运行代码3-17后，输出的结果如下。\n\n将文本中的姓名和电话号码一一对应，如代码3-18所示。\n\n代码3-18 将文本中的姓名和电话号码——对应\n\nzip(name,tel):#zip函数用于将元素打包成元组\n\n# 输出人名和电话号码\n\n运行代码3-18后，输出的结果如下。\n\n3.2.3  提取网页标签信息\n\n例如，有一段网页标签的内容为“<a  href=\"http://www.baidu.com\">百度</a><a    href= \"http://www.weibo.com\"> 微博</a>”,    需要提取网页标签中的网址和文本。通过观察，网 址信息保存在 href 属性中，文本内容则在特殊字符中间。提取网址和文本，如代码3-19\n\n所示。\n\n代码3-19 提取网址和文本\n\n运行代码3-19后，输出的结果如下。\n\n百度。http://www.baidu.com\n\n微博。http://www.weibo.com\n\n45\n\nPython 中文自然语言处理基础与实战\n\n小结\n\n本章主要介绍正则表达式的基本知识和使用。首先介绍常用的正则表达式的函数和正\n\n则表达式的元字符与元字符的含义，然后通过实例对正则表达式的使用方法进行演示。\n\n实训\n\n实 训 1 过滤《三国志》中的字符\n\n1.  训练要点\n\n掌握在中文文本中过滤特殊符号、标点符号、英文、数字的方法。\n\n2. 需求说明\n\n获取的《三国志》的文本中含有很多特殊字符，需要过滤掉特殊符号、标点符号、英\n\n文、数字等。\n\n3. 实现思路与步骤\n\n使用 sub 函数替换指定文本。\n\n实 训 2  提取地名与邮编\n\n1.   训练要点\n\n掌握在文本中提取中文与数字的方法。\n\n2. 需求说明\n\n现有一段文本“广州：510000深圳：518000佛山：528000珠海：519000东莞：523000”,\n\n需要提取其中的地名和对应的邮编。\n\n3. 实现思路与步骤\n\n(1)使用 findall 函数返回正则表达式在文本中所有匹配结果的列表。\n\n(2)使用 zip 函数将地名与邮编一一对应。\n\n实 训 3  提取网页标签中的文本\n\n1.  训练要点\n\n掌握在网页标签中提取指定文本的方法。\n\n2. 需求说明\n\n现有一段网页标签“<meta    name=\"description\"content=”京东 JD.COM-专业的综合网上 购物商城，销售家电、数码通信、电脑、家居百货、服装服饰、母婴、图书、食品等领域 数万个品牌优质商品。便捷、诚信的服务为您提供愉悦的网上购物体验!\"/>\",需要提取\n\n其中的文本内容。\n\n3. 实现思路与步骤\n\n(1)根据网页标签内容提取所需文本。\n\n( 2 ) 使 用findall  函数返回正则表达式在文本中所有匹配结果的列表。\n\n46\n\n第 ③ 章  正则表达式\n\n课后习题\n\n1. 选择题\n\n(1)不属于常用的正则表达式函数的是(    )。\n\nA.match   函数  B.search   函数   C.findall   函数      D.matplotlib   函数\n\n(2)“re.sub(   '自然语言处理，\"NLP\",text1)”   表示的含义为(    )。\n\nA.  将 test1 中的“自然语言处理”替换为“NLP ”\n\nB.   将test1 中的 “NLP” 替换为“自然语言处理”\n\nC.  找出 test1 中的“自然语言处理”\n\nD.  找出 test1 中的 “NLP”\n\n(3)竖线“”用于对两个正则表达式进行“或”操作。如果A 和B 是正则表达式，那 么A|B表示为(    )。\n\nA.  匹配A 和 B一起出现的字符   B.  匹配A或 B 中出现的任何字符\n\nC.  匹配A 中出现的任何字符     D.  匹配B 中出现的任何字符\n\n(4)美元货币符号“$”表示匹配字符串的(    )位置。\n\nA.  结束                       B.  开始\n\nC.  中间                        D.  表示货币的字符串\n\n(5)下列 Python 中的预定义字符描述正确的是(    )。\n\nA.\\w:     与\\W反义，非数字、非字母和非字\n\nB.\\s   : 空白字符\n\nC.\\D:      数字\n\nD.\\d:     非数字\n\n2. 操作题\n\n(1)完成本章中所有代码的操作。\n\n(2)使用正则表达式提取文本“(888)555-1234”中的“(888)”。\n\n(3)使用正则表达式提取文本“111111@qq.comabcdefg@126.comabc123@163.com”  中\n\n所有的电子邮箱地址。\n\n47\n\n第 章 中文分词技术\n\n词是中文语言理解中最小的能独立运用的语言单位。中文的词与词之间没有明确分隔 标志，因此在分词技术领域里，中文分词的实现要比英文困难。本章主要介绍基于规则分 词和基于统计分词的基本理论和方法，以及中文分词工具jieba 的使用方法", "metadata": {}}, {"content": "，因此在分词技术领域里，中文分词的实现要比英文困难。本章主要介绍基于规则分 词和基于统计分词的基本理论和方法，以及中文分词工具jieba 的使用方法，并通过实例演 示基于隐马尔可夫模型 (Hidden    Markov    Model,HMM) 分词和基于jieba 分词实现中文分\n\n词的应用。\n\n学习目标\n\n(1)了解中文分词的基本概念。\n\n(2)熟悉基于规则分词的基本概念和常用方法。\n\n(3)熟悉基于统计分词的基本概念、 n 元语法模型和隐马尔可夫模型的基本原理。\n\n(4)掌握中文分词工具jieba的使用方法。\n\n(5)掌握基于隐马尔可夫模型分词和基于jieba 分词的实现方法。\n\n4.1)  中文分词简介\n\n中文分词是指将汉字序列按照一定规则逐个切分为词序列的过程。在英文中，单词之 间以空格为自然分隔符，分词时自然以空格为单位进行切分，而中文分词则需要依靠一定\n\n技术和方法寻找类似英文中空格作用的分隔符。\n\n基于规则分词是中文分词最先使用的方法，常见的方法有正向最大匹配法、逆向最大 匹配法等。随着统计方法的发展， 一些基于统计的分词模型被提了出来，常见的分词模型\n\n有n 元语法模型、隐马尔可夫模型和条件随机场模型。\n\n4.2 基于规则分词\n\n基于规则的分词方法是一种较为机械的分词方法，其基本思想是将待分词语句中的字 符串和词典逐个匹配，找到匹配的字符串则切分，不匹配则减去边缘的某些字符，从头再\n\n次匹配，直至匹配完毕或者没有匹配到词典中的字符串而结束。\n\n基于规则分词主要有正向最大匹配法(Forward Maximum Matching Method,FMM法)、 逆向最大匹配法 (Reverse  Maximum  Matching  Method,RMM 法)和双向最大匹配法\n\n(Bi-direction Maximum Matching Method,BMM法)这3种方法。\n\n第 4 章  中文分词技术\n\n4.2.1 正向最大匹配法\n\n假设有一个待分词中文文本和一个分词词典，词典中最长字符串的长度为1。从左至 右切分待分词文本的前1个字符，然后在词典中查找是否有一样的字符串。若匹配失败，  则删去该字符串的最后一个字符，仅留下前1-1个字符，继续匹配这个字符串，以此类推。 如果匹配成功，那么被切分下来的第二个文本成为新的待分词文本，重复以上操作直至匹\n\n配完毕。如果一个字符串全部匹配失败，那么逐次删去第一个字符，并重复上述操作。\n\n例如，假设待分词文本为“北京市民办高中”,词典为“{\"北京市\",\"北京市民\",”民办\n\n高中\",\"天安门广场\",\"高中“}”。由词典得到最长字符串的长度为5,具体分词步骤如下。\n\n(1)切分待分词文本“北京市民办高中”前5个字符，得到“北京市民办”,在词典中\n\n找不到与之匹配的字符串，匹配不成功。\n\n(2)删去“北京市民办”的最后一个字符得到“北京市民”,再与词典进行匹配。在词\n\n典中找到与之匹配的字符串，匹配成功。此时，将文本划分为“北京市民”“办高中”。\n\n(3)将分词后的第二个文本“办高中”作为待分词文本。此时词典中找不到与之匹配\n\n的字符串，匹配不成功。\n\n(4)删去“办高中”的最后一个字符，匹配失败，直至删去所有字符都没有匹配成功， 因此删去“办高中”的第一个字符。匹配“高中”一词成功，将第二个文本划分为“办”\n\n“高中”。\n\n综上所述，用正向最大匹配法分词得到的结果是“北京市民”“办”“高中”。\n\n4.2.2  逆向最大匹配法\n\n逆向最大匹配法与正向最大匹配法原理相反。从右至左匹配待分词文本的后1个字符 串，在词典中查找是否有一样的字符串。若匹配失败，仅留下待分词文本的后1-1个词， 继续匹配这个字符串，以此类推。如果匹配成功，则被切分下来的第一个文本序列成为新 的待分词文本，重复以上操作直至匹配完毕。如果一个词序列全部匹配失败，则逐次删去\n\n最后一个字符，并重复上述操作。\n\n同样以待分词文本“北京市民办高中”,词典“{\"北京市\",\"北京市民\",\"民办高中\",”天\n\n安门广场\",\"高中“}”为例说明逆向最大匹配法，具体分词步骤如下。\n\n(1)切分待分词文本“北京市民办高中”后5个字符，得到“市民办高中”,在词典中\n\n找不到与之匹配的字符串，匹配不成功。\n\n(2)删去“市民办高中”的第一个字符得到“民办高中”,再与词典进行匹配，匹配成\n\n功，将文本划分为“北京市”和“民办高中”。\n\n(3)将分词后的第一个文本“北京市”作为待分词文本，匹配成功。\n\n综上所述，用逆向最大匹配法分词得到的结果是“北京市”“民办高中”。\n\n实现逆向最大匹配法，如代码4-1 所示，其中 RMM 是自定义的逆向最大匹配法函数 名。代码中包含读取词典、获取词典最大长度和切分文本3个流程。 RMM  函数中的参数 text 为待分词文本， dic.utf8 文件为自定义词典，strip  函数用于去除字符串首尾空格， set  函\n\n数用于去除重复元素， reverse  函数用于将列表中的字符串反向排列。\n\n49\n\nPython 中文自然语言处理基础与实战\n\n代码4-1 实现逆向最大匹配法\n\n#逆向最大匹配法\n\ndef         RMM(text):\n\ny   =[]\n\ndic  path   ='../data/dic.utf8'\n\nfor         lin . rip()  open(dic_path,'r',encoding='utf-8-sig'):\n\nif         not          line:\n\ncontinue\n\ndictionary.append(line)\n\ndictionary  =list(set(dictionary))\n\n取x_词le度\n\nword   length =[]\n\nfor word in dictionary:\n\nmax_ _n(lh))\n\n#切分文本\n\n_st length=[]=len(text)\n\nwhile   text   length>0:\n\nj=0\n\nfor i in range(max   length,0,-1):\n\nif           text_length            -i<0:\n\ncontinue\n\nnew  word =text[text   length -i:text   length]\n\nif new  word in dictionary:\n\ncut_list.append(new_word)\n\ntext_length                 -=i\n\nj+=1\n\nif j==0: break\n\ntext   length   -=1\n\ncut   list.reverse()\n\nprint(cut   list)\n\nRMM('北京市民办高中!)\n\n运行代码4-1后，输出结果如下。\n\n['北京市','民办高中']\n\n4.2.3  双向最大匹配法\n\n双向最大匹配法的基本思想是将正向最大匹配法和逆向最大匹配法的结果进行对比， 选取两种方法中切分次数较少的结果作为切分结果。例如，用正向最大匹配法和逆向最大 匹配法对“北京市民办高中”进行分词，结果分别为“北京市民”“办”“高中”和“北京\n\n市”“民办高中”,因此选取切分次数较少的结果“北京市”“民办高中”。\n\n研究表明，使用正向最大匹配法和逆向最大匹配法，中文分词大约有90%的词句完全\n\n重合且正确，有9%左右的句子得到的结果不一样，但其中有一个是正确的。剩下不到1%\n\n50\n\n第 ④ 章 中文分词技术\n\n的句子使用两种方法进行切分得到的结果都是错误的。因此，双向最大匹配法在中文分词\n\n领域中有广泛运用。\n\n4.3   基于统计分词\n\n基于规则的中文分词常常会遇到歧义问题和未登录词问题。中文歧义问题主要包括交  集型切分歧义和组合型切分歧义两大类。交集型切分歧义是指一个字符串中间的某个字或  词不管切分到哪一边都能独立成词，如“打折扣”一词，“打折”和“折扣”是两个独立的  词语。组合型切分歧义是指一个字符串中每个字单独切分或者不切分都能成词，如“将来”\n\n一词，既可以整体成词，也可以切分为单个字。\n\n未登录词也称为生词，即词典中没有出现的词。未登录词可以分为四大类，第一类是 日常生活中出现的普通新词汇，尤其是网络热门词语，这类词语更新换代快", "metadata": {}}, {"content": "，如“打折扣”一词，“打折”和“折扣”是两个独立的  词语。组合型切分歧义是指一个字符串中每个字单独切分或者不切分都能成词，如“将来”\n\n一词，既可以整体成词，也可以切分为单个字。\n\n未登录词也称为生词，即词典中没有出现的词。未登录词可以分为四大类，第一类是 日常生活中出现的普通新词汇，尤其是网络热门词语，这类词语更新换代快，且不一定符 合现代汉语的语法规定；第二类是专有名词，主要指人名、地名和组织机构名，还包括时 间和数字表达等；第三类是研究领域的专业名词，如化学试剂的名称等；第四类是其他专 用名词，如近期新上映的电影、新出版的文学作品等。遇到未登录词时，分词技术往往束\n\n手无策。\n\n基于统计的分词方法有效解决了中文分词遇到的歧义问题和未登录词问题。随着统计机 器学习方法的研究和发展，基于统计的分词方法逐渐应用到实际当中。基于统计的分词方法 的基本思想是中文语句中相连的字出现的次数越多，那么作为词单独使用的次数就越多，语 句拆分的可靠性就越高，分词的准确率就越高。因此，基于统计的分词方法的基本原理是统 计词出现的次数，出现次数足够高的词作为单独的词被保留。基于统计的分词方法的优势在 于其能够较好地处理未登录词和歧义词，不需要人为地搭建和维护词典，其缺点在于需要依\n\n靠语料库进行分词，语料库的准确度不一定高，且计算量较大，分词速度一般。\n\n使用基于统计的分词方法时通常需要进行两个步骤：建立统计语言模型；运用模型划 分语句，计算被划分语句的概率，选取概率最大的划分方式进行分词。常见的基于统计的\n\n分词方法包括n 元语法模型和隐马尔可夫模型。\n\n4.3.1 n 元语法模型\n\n统计语言模型是描述自然语言内在规律的数学模型，其原理为判断一个句子在文本中 出现的概率，是用于计算一个语句序列概率的概率模型。统计语言模型通常基于一个语料\n\n库构建，广泛应用于各种 NLP 问题，如语音识别、机器翻译、分词、词性标注等。\n\n设s 为一连串特定顺序排列的词序列，即s=o,o₂,…,0,,s         的概率记为p(oy,o₂,…,o,)。\n\n利用条件概率公式得到式(4-1)所示的统计语言模型。\n\np(s)=p(o)p(o₂|o₁)…p(o,|o,o₂,…,o )\n\n(4-1)\n\n其中w-  表示词序列o,0₂,…,O1。\n\n51\n\nPython 中文自然语言处理基础与实战\n\n1.n-gram 模型\n\n在式(4-1)所示的统计语言模型中，当词序列的长度增加时，计算难度也将逐渐加大，\n\n因此需要借助近似方法计算p(s) 。 为解决该问题，于是出现了马尔可夫假设。\n\n设o,o₂,…,o,   为一连串特定顺序排列的词序列，马尔可夫假设0,出现的概率只与前面N-1  个词0-v+,…,0_2,0    相关。当N=n  时，该统计语言模型称为n 元语法模型 (n-gram模型)。\n\n当N=1 时，该统计语言模型称为一元语法模型(unigram模型),p(s) 的概率如式(4-2) 所示。\n\np(a),o₂,…,o)=p(o₂)p(o₂)…p(o,)                             (4-2)\n\n当N=2 时，0,出现的概率只与前面一个词o   相关，该统计语言模型称为二元语法模型 (bigram模型), p(s) 的概率如式(4-3)所示。\n\np(o,o₂,…,o,)=p(o)p(o₂|o)…p(o,|o_)                                    (4-3)\n\n当N=3 时，o, 出现的概率只与前面两个词o₂   和o_  相关，该统计语言模型称为三元\n\n语法模型 (trigram 模型), p(s) 的概率如式(4-4)所示。\n\np(o,o₂,…,0)=p(o)p(o₂|o)…p(o,|O-2O_)                                    (4-4)\n\n一般情况下，N 的取值很小，应用最多的是三元语法模型，主要原因是n-gram模型的 空间复杂度是n的指数函数，即O(v-),    其中|表示语料库中的总词数， 一般为几万到\n\n几十万个。此外，n-gram模型的时间复杂度也是一个指数函数O(v)。\n\n2.n-gram  模型的参数估计\n\nn-gram模型的参数就是条件概率p(o,|0,…,2,           0) 。 模型的参数估计也称为模型\n\n的训练，n-gram 模型的参数表达式如式(4-5)所示。\n\n               (4-5)\n\n一般采用最大似然估计 (Maximum   Likelihood   Estimation,MLE) 的方法对模型参数进\n\n行估计，使用MLE 对式(4-5)进行估计，如式(4-6)、式(4-7)所示。\n\n(4-6)\n\n(4-7)\n\n其中，c(0)      …,O,0)    和c(0,…,O-2,0)          分别表示词语0-,…,0_,0,和0-+1,…,O-2,\n\n0在语料库中出现的次数，次数越多，参数估计的结果越可靠。\n\n假设语句序列 s={  小孩，喜欢，在家，观看，动画片},估计这一语句的概率。以二元语法模 型为例，需要检索语料库中每一个词，以及该词和相邻词同时出现的概率。假设语料库的\n\n总词数为7542个，单词出现的次数如图4-1所示。\n\n351\n\n动画片\n\n2046\n\n图4-1 单词出现的次数(单位：次)\n\n52\n\n第  4  章  中 文 分 词 技 术\n\n语句s 在当前语料库中出现概率的计算过程如式(4-8)所示。\n\np(s)=p    (小孩，喜欢，在家，观看，动画片)\n\n=p(小孩)p(喜欢|小孩)p(在家|喜欢)p(观看|在家)p(动画片观看)  (4-8)\n\n因此，语句s 在当前语料库中出现的概率约为0.2122347。\n\n3. 数据平滑\n\n在对统计语言模型的参数进行估计时，常会出现式(4-7)中的分子或分母为零的情况， 这种情况称为数据稀疏。这时需要考虑在模型中结合相应的数据平滑方法解决问题，常用\n\n的数据平滑方法有加1平滑、古德-图灵平滑、线性插值平滑等。\n\n(1)加1平滑是最简单、最直观的一种数据平滑方法。加1平滑规定在训练时任何一 个词语在训练语料库中都需要至少出现一次(包括没有出现的词语),这样没有出现过的词\n\n语的概率不再是0。加1平滑的公式如式(4-9)所示。\n\n(4-9)\n\n使用加1平滑后，训练语料库中出现的 n-gram的概率不再为0,而是一个大于0的较 小的概率值。但在实际的应用中却会出现一些问题，由于总的统计概率一定是1,那么这 些后来增加的概率就必然会造成原来概率的减小，而且实验证明这种概率的减小幅度是巨\n\n大的，那么就可能会导致结果不准确。\n\n(2)1953年古德 (IJ.Good)     引用图灵 (Turing)    的方法提出古德-图灵(Good-Turing)    估计法，它是很多平滑方法的核心方法。其基本思想是对于没有看见的事件，不能认为它 发生的概率就是零，应该从概率的总量中分配一个很小的比例给这些没有看见的事件。此\n\n时看得见的事件的概率总和将会小于1,因此，需要将所有看见的事件的概率适当调小。\n\n假设语料库中出现r 次的词有N,个，特别地，未出现的词数量为N₀ 。设语料库的大\n\n小 为N,   如式(4 - 10)所示\n\n                              (4-10)\n\n出 现r 次的词在整个语料库中的相对频度为r/N,      如果不做任何处理，这个相对频度\n\n就是出现r 次的词语的概率估计。\n\n现在假设当r 比较小时，它的估计可能不可靠", "metadata": {}}, {"content": "，特别地，未出现的词数量为N₀ 。设语料库的大\n\n小 为N,   如式(4 - 10)所示\n\n                              (4-10)\n\n出 现r 次的词在整个语料库中的相对频度为r/N,      如果不做任何处理，这个相对频度\n\n就是出现r 次的词语的概率估计。\n\n现在假设当r 比较小时，它的估计可能不可靠，因此在计算出现r 次的词的概率时\n\n要使用一个更小的系数r*,   如式(4- 11)所示。\n\n53\n\n因此， p(0\n\n…,0z,0)     的古德-图灵估计如式(4-12)所示。\n\n(4-11)\n\n(4-12)\n\n为方便理解古德-图灵估计方法，下面通过一个简单的例子进行介绍。假设训练集合\n\nT={张三，喜欢，外出，旅行，李四，喜欢，外出，登山},验证集合 V={王五，喜欢，外出，登山，不喜欢，\n\nPython 中文自然语言处理基础与实战\n\n旅行}。训练集合长度为8,验证集合长度为6。在训练集合中，由式(4-11)和式(4-12) 可以得出 p(喜欢)=p(外出)=0.25,其他词语概率都为0.125。验证集合中没有在训练集合里 出现的词语概率都为0。如果不使用平滑方法处理，那么验证集合中将有词语出现概率为0\n\n的情况，如p(王五)=0。\n\n利用古德-图灵估计进行平滑需要进行以下3个步骤。\n\n①计算出现r 次的词语的数目。出现两次的有“喜欢”“外出”,没有出现的有“王五”\n\n“不喜欢”,其他各出现一次，即N₀=2,N₁=4,N₂=2,          出现次数大于2的情况次数为0。\n\n②利用古德-图灵法进行平滑，重新计算概率值。r=0 的事件概率为 \n\n0.125r=1的事件概率的事件概率为  9\n\n保持原值0.25。\n\n③进行归一化处理。3个概率之和为2×P₀+4×P₁+2×P₂=1.25 。出现0次的词语出现 概率为0.125/1.25=0.1,出现1次的词语出现概率为0.125/1.25=0.1,出现2次的词语出现\n\n概率为0.25/1.25=0.2,这样就避免了概率为0的问题。\n\n(3)线性插值平滑是为高阶 n-gram 模型提供帮助的一种平滑方法，它利用低阶的 n-gram模型对高阶的n-gram模型进行线性插值。例如，要寻找“西红柿红”一词，但该词 在语料库中没有出现过，这时用低阶的n-gram模型替代，即用“西红柿”出现的次数代替。 通常在计算概率时，低阶的 n-gram 模型往往能够更好地反映信息。\n\n在一般的简单线性插值中，不管高阶模型是否可行，默认低阶模型和高阶模型都有着 相同的权重λ,这样并不太合理。因此，线性插值平滑中将权重改为λ(0-,…,0),以\n\n递归的方式改写线性插值法的公式，表示权重λ是历史的函数，如式(4-13)所示。\n\nPmap(0)|00)-+,…,Q)-)=λ()-m+,…,Q))P(0)|0-+,…,Q)-)+                     (4-13)\n\n(1-a(0-m+,…,0 ))Pmep(0) |0)-n+2,…,Q)-)\n\n其中λ(o-,…,0_)     可凭借经验设定，也可以通过某些算法确定，但是需要满足条件\n\nZPmm(0-,…,0)=1。\n\n4. 中文分词与n-gram 模型\n\n设中文句子由字序列s=o,o₂,…,0,    组成，字以不同的方法组合到一起成为词。s 有多 种切分方式，假设w;记为 s 中的第j 种切分方式， m,为第j种切分方式w,中词的个数， W,=(w)₁,W),2,…,W;m)。中文分词模型就是求字序列s=a,o₂,…,o,所有切分方式中概率最\n\n大的一种切分方式w,    如式(4-14)所示。\n\n54\n\n其中，表示式子p(w|s)达到最大值时变量w,的取值。\n\n由贝叶斯公式可得式(4-15)。\n\n(4-14)\n\n(4-15)\n\n第 4 章 中文分词技术\n\n由于概率模型是基于语料库构建的，当给定字序列s=o,0 ₂,…,0,,     式(4-15)中p(s)\n\n的值是确定的。另一方面，如果w,确定了，则s 字序列唯一确定，因此条件概率p(s|w,)=1。\n\n由式(4-14)和式(4-15)可得式(4-16)。\n\n55\n\n由统计语言模型式(4-1)可得式(4-17)。\n\n结合式(4-16)和式(4-17)可得式(4-18)。\n\n(4-16)\n\n(4-17)\n\n(4-18)\n\n中文分词实际是对式(4-18)求最优解w  的过程。特别地，对于二元语法模型，式(4-18)\n\n可简化为式(4-19)。\n\n(4-19)\n\n4.3.2  隐马尔可夫模型相关概念\n\n隐马尔可夫模型是一种概率模型，用于解决序列预测问题，可以对序列数据中的上下 文信息进行建模。HMM  用于描述含有隐含未知参数的马尔可夫过程。在 HMM  中，有两 种类型的节点，分别为观测序列与状态序列。状态序列是不可见的，它们的值是需要通过 对观测序列进行推断而得到的。很多现实应用可以抽象为此类问题，如语音识别，NLP  中\n\n的分词、词性标注，计算机视觉中的动作识别等。HMM 在这些问题中得到了成功的应用。\n\n1. 马尔可夫模型\n\n马尔可夫模型描述的是一类典型的随机过程。所谓随机过程，就是系统随时间变化而 随机变化的过程。这种模型可以计算出系统每一时刻处于各种状态的概率和这些状态之间\n\n的转移概率。\n\n设一个系统有有限个状态S={s₁,s₂,…,sx}。 随着时间推移，该系统将从某一状态转移 到另一状态。从1 时刻开始，到t 时刻为止，系统所有时刻的状态值构成一个随机变量序\n\n列，如式(4-20)所示。\n\nQ=q₁,q₂,…,q                                                                    (4-20)\n\n系统在不同时刻可以处于同一种状态，但在任一时刻只能有一种状态。不同时刻的状 态之间是有关系的。时刻t的状态由它之前时刻的状态决定，即当前时刻t处于某状态的概\n\n率取决于其在时间1,2,…,t-1  时刻的状态，系统状态的条件概率如式(4-21)所示。\n\np(q,|q₁₃q₂,…,q,-)                                                               (4-21)\n\n式(4-21)中的条件概率要考虑之前所有时刻的状态，计算起来较为复杂，为此需要 进行简化。如果假设t 时刻的状态只与t-1   时刻的状态有关，与更早的时刻无关，则式(4-21)\n\n中的条件概率可简化为式(4-22)。\n\np(q,|q₁,q₂…,q₁-)=p(q,|q, )                                                      (4-22)\n\nPython 中文自然语言处理基础与实战\n\n式(4-22)称为一阶马尔可夫假设，满足这一假设的马尔可夫模型称为一阶马尔可夫\n\n模型。\n\n设t-1 时刻的状态为s;,t    时刻的状态为s,,   条件概率p(q,Iq₁_)构成一个N×N  的矩阵\n\nA,   称为状态转移概率矩阵", "metadata": {}}, {"content": "，满足这一假设的马尔可夫模型称为一阶马尔可夫\n\n模型。\n\n设t-1 时刻的状态为s;,t    时刻的状态为s,,   条件概率p(q,Iq₁_)构成一个N×N  的矩阵\n\nA,   称为状态转移概率矩阵，其元素如式(4-23)所示。\n\nay=p(q₁=s;|q₁-=s;)                                            (4-23)\n\n有N 个状态的一阶马尔可夫过程有N² 次状态转移。状态转移概率矩阵的元素满足式\n\n56\n\n(4- 24)所示的约束。\n\na\n\n(4-24)\n\n式(4-24)表示概率值必须在[0,1]内，无论t-1 时刻的状态值是什么,在下一个时刻t\n\n一定会转向N 个状态中的一个，因此它们的转移概率和必须为1。\n\n2. 隐马尔可夫模型\n\n马尔可夫模型中的状态是可见的，而 HMM 中的状态则部分可见。HMM 描述观测变 量和状态变量之间的概率关系。与马尔可夫模型相比， HMM-不仅对状态建模，还对观测\n\n值建模。不同时刻的状态值之间，以及同一时刻的状态值和观测值之间，都存在概率关系。\n\n(1) 一 个 HMM 示例。下面通过掷骰子的例子说明 HMM 模型。假设暗室里有3个不 同形状的骰子，第一个骰子含有6个面(1～6,记为D6), 第二个骰子含有4个面(1～4, 记为 D4),  第三个骰子含有8个面(1～8,记为D8) 。假设暗室里的实验员根据某一概率 分布随机挑选一个骰子，然后掷骰子，并向室外报告投掷的数字结果，重复上述过程。对 于暗室外边的人来说，只能得到投掷的数字结果的序列，而骰子的序列是观察不到的。例 如，连续掷骰子8次，得到数字序列为{3,1,2,4,8,7,3,1}。这一串数字是可见的，称为可见\n\n状态链。除了可见状态链，还有一串隐含状态链，也就是实验员挑选骰子的序列。\n\nHMM 中的马尔可夫链通常是指隐含状态链。隐含状态链中有不同骰子之间的转换概 率，即不同隐含状态之间有一个转移概率。这里已知在3个骰子中任意一个骰子被选中的 概率是1/3,如 D4 转换成 D6 时，概率为1/3,D6 转换为D8 时，概率也为1/3。当然，也 可以人为设定转移概率，使抽取到不同骰子的概率不相同，从而形成一个新的 HMM。  另 一方面，在每一个骰子掷出数字时也有一个概率，即不可见状态和可见状态之间有一个发 射概率，如 D4 掷出数字为1～4的概率都是1/4,D8 掷出数字为1～8的概率都是1/8。同\n\n样地，输出概率也可以人为设定，如设定某个骰子掷出特定数字时的概率更大或更小。\n\n掷骰子过程的示意图如图4-2所示。\n\n图4-2 掷骰子过程的示意图\n\n第 4 章  中文分词技术\n\n(2)HMM  的结构。假设观测序列为O=o,O₂,…,o,O     是直接能观察到的序列(上例 中投掷骰子的点数序列)。任一时刻的观测值来自有限的观测集(上例中所有骰子点数的集合), 记为V={v,v₂,…,vy}。隐含状态序列(上例中选择投掷骰子的序列)为Q=q,9₂,…,qr,    这 条序列是不可观测的。任一时刻的状态值来自有限的状态集S={s₁S₂,…,Sx}。\n\nHMM  的结构如图4-3 所示。\n\n57\n\n9₁\n\n9₂\n\n.\n\n9,\n\n\\P(qlg-)\n\n9,    状态序列Q\n\nP(o₇lq)\n\nO₁                                                 O₂                                                …                                         Or-i                                                  Or                 观测序列O\n\n图4-3 HMM 的结构\n\n一 个HMM 由3个部分组成。\n\n①设模型中状态的数目为N  (上例中骰子的数目),每个状态可能输出的观测值的数\n\n目为M (上例中骰子的不同数字的数目)。状态转移概率矩阵如式(4-25)所示。\n\nA=[a,]vw,l≤i,j≤N                                              (4-25)\n\n其中a,=p(q₁=s,|q₁-₁=s), 所示的条件。\n\n表示从一个状态s;转向另一个状态s, 的概率，满足式(4-26)\n\n                         (4-26)\n\n②对于任一时刻t,   从状态s, 观察到v  的概率分布矩阵如式(4-27)所示。\n\nB=[b,(v₄)]vxm,1≤k≤M,l≤j≤N                                      (4-27)  其中b,(v)=p(o,=v₄|q₁=s;)     (如掷骰子的例子中实验员选取第j 个骰子投掷出数字k的概\n\n率为b,(v₄)),   满足式(4-28)所示的条件。\n\n观测值的概率b,(v₄)又称为符号发射概率。\n\n③初始时刻的状态概率分布如式(4-29)所示。\n\nπ₁=(πj,…,πy),l≤i≤N   其中， π,=(πj,…,πy)满足式(4-30)所示的条件。\n\nπ;=P(q₁=s;)\n\nπ,≥0\n\n一般地， 一个HMM 记为一个五元组λ=(S,V,A,B,π)。\n\n(4-28)\n\n(4-29)\n\n(4-30)\n\n为了简单起见，有时也将其记\n\n为三元组λ=(π,A,B) 。隐藏状态和观测值的数量是根据实际问题人为设定的，状态转移矩\n\nPython 中文自然语言处理基础与实战\n\n阵和发射概率矩阵通过样本学习得到。\n\n(3)HMM    的两个基本假设。\n\n①齐次马尔可夫性假设。\n\n隐马尔可夫链在任意时刻t 的状态只依赖于其前一时刻t-1   的状态，与其他时刻的状态\n\n和观测无关，如式(4-31)所示。\n\nP(q,|q₁,0₁,…,q₁-,0₁_)=P(q,|q₁-)                                             (4-31)\n\n②观测独立性假设。\n\n任意时刻t 的观测值o,只依赖于该时刻的马尔可夫链q,的状态，与其他观测和状态无\n\n关，如式(4-32)所示。\n\np(o,|g₁,o₁,…,q,,o,)=p(o,|q₁)                                            (4-32)\n\n(4)HMM    中的3个基本问题。\n\n①概率计算问题。给定一个三元组λ=(π,A,B)和观测序列O=o,0₂,…,0r, 计算在给\n\n定模型λ=(π,A,B) 条件下观测序列O的概率p(O|λ)。\n\n②参数估计问题。已知观测序列O=o,o₂,…,0r,估计模型λ=(π,A,B)参数，使得在\n\n该模型下观测序列概率p(O|λ)最大。\n\n③预测问题，也称解码问题。给定一个三元组λ=(π,A,B) 和观测序列O,   计算最有\n\n可能的状态序列Q=q₁,q₂,…,qr    c\n\n3. 中文分词与 HMM\n\n中文分词可以看作中文的标注问题。标注问题是指给定观测序列预测其对应的标记序 列。假设标注问题的数据是由HMM 生成的，利用HMM 的学习与预测算法进行标注。下\n\n面以中文分词问题为例，介绍 HMM 如何用于中文标注。\n\n对于句子“我是一位程序员”,在这里观测序列O 为“我是一位程序员”,每个字为每 个时刻的观测值。状态序列为标注的结果", "metadata": {}}, {"content": "，利用HMM 的学习与预测算法进行标注。下\n\n面以中文分词问题为例，介绍 HMM 如何用于中文标注。\n\n对于句子“我是一位程序员”,在这里观测序列O 为“我是一位程序员”,每个字为每 个时刻的观测值。状态序列为标注的结果，每个时刻的状态值有4种情况{B,M,E,S},  其中\n\nB 代表该字是起始位置的字， M 代表中间位置的字， E 代表末尾位置的字， S 代表能够单\n\n独成字的字。对待分词语句进行序列标注，如果得到状态序列Q 为{SS BEBME},     则有\n\n“我/S 是/S 一/B 位/E 程/B 序/M  员/E”。得到这个标注结果后，即可得到分词结果。遇到  S,  则为一个单字词；遇到B,  则为一个词的开始，直到遇到下一个E,  则为一个词的结尾。\n\n这样句子“我是一位程序员”的分词结果为“我/是/一位/程序员”。\n\n设O=0,0₂,…,0₇    为中文字观测序列， S={s,S₂,…,Sy}    为待标注的状态，中文分词问题\n\n可描述为求条件概率p(Q|O) 最大的状态序列Q=q₁,q₂,…,qr, 如式(4-33)所示。\n\n                       (4-33)\n\n根据贝叶斯公式可得式(4-34)。\n\n                  (4-34)\n\n由于已知待标注的字观测序列O=o,o₂,…,0γ,因此 p(O)为一确定的数，只需计算最\n\n大化情况下的 p(O|Q)p(Q)。 这样中文分词问题可描述为式(4-35)。\n\n(4-35)\n\n58\n\n第 4 章  中文分词技术\n\n由 HMM 的两个基本假设可知，每一时刻的观测值只与对应时刻的状态值有关，每一\n\n时刻的状态值只与上一时刻的状态值有关，如式(4-36)和式(4-37)所示。\n\np(O|Q)=p(o₁|q)p(o₂|q₂)…p(or|qr)                                                 (4-36)\n\np(Q)=p(q)p(q₂|q₁)…p(qr|qr-)                                                   (4-37)\n\n其中， p(q₁|q)     表示状态转移概率， p(o₁|q₁)  表示发射概率。由式(4-36)和式(4-37)\n\n59\n\n可得式(4-38)。\n\np(O|Q)p(Q)=p(q₁)p(o₁|q₁)×p(q₂|q₁)p(o₂|q₂)× … ×\n\np(qr|qr-)p(or|qr)\n\n因此，式(4-35)可表示为式(4-39)。\n\n(4-38)\n\n(4-39)\n\n除了 HMM, 条件随机场也是基于马尔可夫思想的统计分词算法。HMM 假设每个时刻 的状态只与前一时刻的状态有关，而条件随机场假设每一时刻的状态还与后一时刻的状态\n\n有关。条件随机场将在第5章介绍。\n\n4. 维特比算法\n\n维特比算法 (Viterbi   Algorithm) 是机器学习中应用非常广泛的动态规划算法，在求解 HMM模型预测问题时会用到该算法。实际上，维特比算法不仅是很多NLP 的解码算法， 还是现代数字通信中使用得最频繁的算法。中文分词问题可以利用维特比算法求解，得到\n\n标注的状态序列值。\n\n(1)动态规划。如果一类活动过程可以分为若干个互相联系的阶段，每一个阶段都需 做出决策， 一个阶段的决策确定以后，常常会影响下一个阶段的决策，从而能够完全确定\n\n一个过程的活动路线，则称此类活动为多阶段决策问题。\n\n各个阶段的决策构成一个决策序列，称为一个策略。每一个阶段都有若干个决策可供 选择，不同策略的效果也不同。多阶段决策问题，就是要在可以选择的策略中选取一个最\n\n优策略，使其在预定的标准下达到最好的效果。\n\n在多阶段决策问题中，各个阶段采取的决策一般来说是与时间有关的，决策依赖于当 前状态，又随即引起状态的转移。 一个决策序列就是在变化的状态中产生出来的，故有“动\n\n态”的含义，这种解决多阶段决策最优化问题的方法称为动态规划方法。\n\n动态规划是运筹学的一个分支，是求解决决策过程最优化的数学方法。20 世纪50 年 代初，美国数学家理查德·贝尔曼 (Richard     Bellman) 等人在研究多阶段决策过程的优化 问题时，提出了著名的最优化原理，将多阶段过程转化为一系列单阶段问题，利用各阶段\n\n之间的关系逐个求解，创立了解决这类过程优化问题的动态规划方法。\n\n动态规划原理：假设最优路径为i,…,i,,     如果最优路径在时刻t 通过节点i,    那么从 节点i 到终点i,的子路径i,…,i,必定是最优的。因为假如不是这样，那么从i 到，就有另 一条更好的路径存在，如果将它和从i, 到终点，的路径连接起来，就会形成一条比原来的\n\n路径i,…,i,  更优的路径，这是矛盾的。\n\n(2)维特比算法。由式(4-39)可知，求状态序列Q^=q,q₂,…,q是对1到T个时刻\n\nPython  中文自然语言处理基础与实战\n\n的状态进行决策，使得式(4-39)的值达到最大。这是一个多阶段决策最优化问题，可用 动态规划法求解。实际上，维特比算法就是用动态规划法来求解 HMM 的预测问题的，即 用动态规划法求概率最大路径。\n\n假设隐马尔可夫链生成的状态随机序列为Q=q,q₂,   …,qr,它的所有可能的状态集合为 S={s;,s₂,…,Sy}。为简化表示，将状态集合{s₁,s₂,…,Sx}用整数编号{1,2,…,N} 表示。 一个 隐马尔可夫链可以构造网络结构图，如图4-4所示。\n\n图4-4  隐马尔可夫链构造的网络结构图\n\n维特比算法就是根据隐马尔可夫链构造的网络结构图，用动态规划法求从时刻1到时 刻T的概率最大路径。假设δ(i) 为t-1  时刻，HMM 沿着某一路径到达状态s,,   并输出观\n\n测序列o,o₂,…,o   的最大概率，由式(4-39)可得式(4-40)。\n\n60\n\n由式(4-40)可得变量δ的递推公式，如式(4-41)所示。\n\n(4-40)\n\n(4-41)\n\n其中i=1,2,…,N,t=2,…,T-1。\n\n为了记录在时刻t,HMM   通过哪一条概率最大的路径达到状态s,,  维特比算法设置了 另外一个变量ψ用于记录该路径上状态的前一个时刻的状态。定义在时刻t 状态为s,的概 率最大路径中前一个时刻的状态如式(4-42)所示。\n\n(4-42)\n\n(3)维特比算法步骤如下。\n\n①初始化。(i)=π,b(o₁),l≤i≤N,ψ₁(i)=0。\n\n②递推计算。                        ,2≤t≤T,l≤i≤N\n\n③记忆回退路径。                  ),2≤t≤T,l≤i≤N。\n\n第④章  中文分词技术\n\n61\n\n④终止。\n\n,\n\nO\n\n⑤路径(状态序列)回溯。i=ψ(),t=T-1,T-2,…,1。\n\n下面通过一个例子说明维特比算法的过程。假定某地区的天气只分雨天和晴天，初始 状态下雨天的概率为0.6,晴天的概率为0.4,两种天气状态之间转移概率和不同天气状态 下产生的温度感受对应发射概率如图4-5 所示，已知该地区3天的温度感受分别是热、温\n\n和、凉爽，请判断这3天的实际天气状况(雨天，晴天)。\n\n图4-5 天气状态转移概率和发射概率\n\n由已知条件可知", "metadata": {}}, {"content": "，初始 状态下雨天的概率为0.6,晴天的概率为0.4,两种天气状态之间转移概率和不同天气状态 下产生的温度感受对应发射概率如图4-5 所示，已知该地区3天的温度感受分别是热、温\n\n和、凉爽，请判断这3天的实际天气状况(雨天，晴天)。\n\n图4-5 天气状态转移概率和发射概率\n\n由已知条件可知，状态集合为S={s;,S₂}={1,2}, 其中1表示雨天、2表示晴天；观测\n\n序列O=o,O₂,O₃,0 、o₂ 、o₃ 分别表示热、温和、凉爽。由图4-5可以得到状态转移概率为\n\n,发射概率矩阵为          初始概率分布为π=(0.6,0.4)。\n\n利用维特比算法，分4个步骤解答。\n\n①初始化。在t=1 时，对每一个状态s;,i=1,2,       求观测o₁为热的概率，记此概率为\n\nδ(i),    如式(4-43)所示。\n\nδ(i)=π,b(o),i=1,2                                                          (4-43)\n\n由式(4-43)代入实际数据可得δ(1)=0.6×0.1=0.06,δ(2)=0.4×0.4=0.16。记ψ₁(i)=0,\n\ni=1,2。\n\n②递推计算，记录回退路径。在t=2 时，对每一个状态s,,i=1,2, 求在t=1时状态 s, 观测为热，并在t=2 时状态s, 观测o₂ 为温和的最大概率，记此概率为δ₂(i),  如式(4-44)\n\n所示。\n\n                        (4-44)\n\n同时，对每一个状态s;,i=1,2, 记录概率最大路径的前一个状态s,, 如式(4-45)\n\n所示。\n\n(4-45)\n\n计算过程如式(4-46)所示。\n\nPython 中文自然语言处理基础与实战\n\n62\n\n=0.032\n\n由式(4-45)可知，ψ₂ (1)=2。\n\n同理可以得到式(4-47)。\n\n=0.0384\n\n则ψ₂ (2)=2。\n\n(4-46)\n\n(4-47)\n\n在t=3 时，对每一个状态s;,i=1,2, 求在t=2 时状态s,观测为温和，并在t=3 时状\n\n态s,观测o₃ 为凉爽的最大概率，记此概率为δ₃(i),   则可以得到式(4-48)～式(4-51)。\n\n=0.009\n\n=0.0046\n\n此时y₃(1)=1,ψ₃(2)=2。\n\n③终止。以p'表示最优路径的概率，则\n\n④路径(状态序列)回溯。最优路径的终点是 ，\n\n(4-48)\n\n(4-49)\n\n(4-50)\n\n(4-51)\n\n由最优路径的终点，逆向找到之和。在t=2 时 ，i=ψ₃(i)=ψ₃(1)=1;   在t=1时， i=ψ₂ (G)=ψ₂ (1)=2。于是求得最优路径，即最优状态序列I=(i,t,t)=(2,1,1)。因此， 这3天的天气情况最有可能是晴天、雨天、雨天。\n\n4.4  中文分词工具 jieba\n\n基于规则或词典、n-gram 模型、HMM、条件随机场模型等分词方法在实际应用中效果 差异并不大。分词技术更多以一种分词方法为主，其余分词方法为辅达到较高的分词准确 率。最常用的方法是以基于词典的分词方法为主，以统计分词方法为辅进行中文分词。这\n\n种方法既能较好地处理未登录词和歧义词，又能避免词典准确率带来的问题。中文分词工\n\n第 4 章  中文分词技术\n\n具jieba 就采用了这种方法进行中文分词。\n\n随着近年来 NLP 技术的快速发展， NLP  中实现中文分词的工具逐渐增多，其中包括 Ansj 、HanLP 和盘古分词等分词工具。在实际开发与研究过程中，使用jieba 进行中文分词\n\n的人员占大多数。\n\n相比其他分词工具，jieba不仅包含分词这一功能，而且还提供了许多分词以外的算法。 jieba 使用简单，并且支持 Python、R、C++等多种编程语言的实现，对新手而言是一个较 好的入门分词工具。在GitHub 社区，jieba 长期有着较高的讨论度，社区中也有不少jieba\n\n分词实例，遇到问题时可以在社区反馈。\n\n4.4.1 基本步骤\n\njieba分词结合了基于规则和基于统计的分词方法，分词过程包含3个步骤。\n\n(1)基于前缀词典快速扫描词图，搭建可能的分词结果的有向无环图，构成多条分词 路径。例如，待分词语句为“天津市河西区有点远”,词典中出现了“天”字，以“天”开 头的词语都会出现，如“天津”,继而出现“天津市”“天津市河西区”,前缀词典是按照包 含前缀词顺序进行的。有向无环图是指所有的分词路径都按照正向的顺序，如果将每一个 词语看成一个节点，那么分词路径对应从第一个字到最后一个字的分词方式，词语之间不\n\n能构成一个回路。\n\n(2)采用动态规划的方法寻找最大概率路径，从右往左反向计算最大概率，以此类推，\n\n得到概率最大的分词路径，作为最终的分词结果。\n\n(3)采用HMM处理未登录词，借助模型中语句构成的4个状态B 、M 、E 、S推导，\n\n最后利用维特比算法求解最优分词路径。\n\n4.4.2  分词模式\n\njieba分词支持精确模式、全模式和搜索引擎模式。\n\n(1)精确模式采用最精确的方式将语句切分，适用于文本分析。\n\n(2)全模式可以快速地扫描语句中所有可以成词的部分，但无法解决歧义问题。\n\n(3)搜索引擎模式在精确模式的基础上再切分长词，适用于搜索引擎的分词。\n\n下面对一句话采用3种分词模式进行分词，以介绍jieba 的分词模式。\n\n进入NLP 虚拟环境，输入 “conda install jieba” 或 “pip install jieba” 命令安装jieba,\n\n安装成功后检查安装列表中是否出现jieba,  如果有则表示安装成功。\n\n打开 Spyder, 使用3种分词模式进行分词，如代码4-2所示。\n\n代码4-2 使用3种分词模式进行分词\n\n63\n\nPython 中文自然语言处理基础与实战\n\n运行代码4-2后，3种分词模式的结果如下。\n\n全模式和搜索引擎模式会输出所有可能的分词结果，精确模式仅输出一种分词，除了\n\n一些适合使用全模式和搜索引擎模式的场合， 一般情况下会使用精确模式。\n\n这3种分词模式的分词主要用jieba.cut 函数和jieba.cut_for_search 函数。jieba.cut 函数 可输入3个参数，分别是待分词字符串、 cut  all 参数(用于选择是否采用全模式，默认为 精确模式)、HMM 参数(用于控制是否使用 HMM)。jieba.cut_for_search 函数可输入两个\n\n参数，分别是待分词字符串、HMM 参数。\n\n4.5 任务：中文分词的应用\n\n使用HMM 进行中文分词，将新闻文本分词后提取其中的高频词。\n\n4.5.1 HMM  中文分词\n\n使用 Python代码实现 HMM分词的过程主要包括训练HMM、定义 viterbi 函数、分词\n\n3个步骤。\n\n1. 训练 HMM\n\n训练HMM 过程定义了train 函数，用于在给定语料下，统计并计算各个位置状态的初 始概率、转移概率和发射概率。train 函数定义了3个用于存放初始概率、转移概率和发射\n\n概率的字典，并将结果存至JSON 文件当中。训练 HMM的过程包含4个步骤。\n\n(1)加载需要的库", "metadata": {}}, {"content": "，用于在给定语料下，统计并计算各个位置状态的初 始概率、转移概率和发射概率。train 函数定义了3个用于存放初始概率、转移概率和发射\n\n概率的字典，并将结果存至JSON 文件当中。训练 HMM的过程包含4个步骤。\n\n(1)加载需要的库，输入待分词文本。\n\n(2)读取语料。语料包含国内2012年6月和7月搜狐新闻中国际、体育、社会、娱乐 等18个频道的新闻内容，对其进行预处理后存放于trainCorpus.txt文件中。语料中每句话 中的每个词都以空格隔开，读取每一行中的词语并标注其位置状态信息，共有 B 、E 、M、\n\nS4 种位置状态。\n\n(3)计算概率参数。统计每个出现在词头的位置状态的次数，得到初始状态概率；统 计每种位置状态转移至另一种状态的次数，得到转移概率；统计每个位置状态下对应的字\n\n及其出现次数，计算时采用加法平滑，得到发射概率。\n\n(4)存储概率参数。将初始概率、转移概率和发射概率写入 JSON 文件中， dumps函\n\n数用于将字典转化为字符串格式， enumerate 函数用于将对象组合为一个索引序列。\n\n训练HMM,  如代码4-3所示。\n\n代码4-3 训练 HMM\n\n64\n\n第 ④ 章 中文分词技术\n\ndef train():\n\n#初始化参数\n\ntrans_prob             ={}    #   转移概率\n\nemit_pr     ob        ={) # 发射概率\n\ninit_prob=()               # 位置状态出现次数\n\nstate_list                                     =['B','M','E','S']\n\nCount_dict={}\n\nfor state in state   list:\n\ntrans ={}\n\nfor s in state   list:\n\ntrans_tate]=trans\n\npr bte][=s{t}ate]=0\n\nCount_dict[state]=0\n\ncount               =-1\n\n#读取并处理单词，计算概率矩阵\n\nar line in open(pa'.h./daa'/Crpus.txt\n\nionuent=+le.strip()\n\nif not line:\n\ncontinue\n\n#读取每一行的单词\n\nword_list      =[]\n\nfor i in line:\n\nif         i          !='':\n\nword_list.append(i)\n\n65\n\n#标注每个单词的位置标签\n\nword_label           =[]\n\nfor         del []   line.split():\n\nif                          len(word)==1:\n\nlabel.append('S')\n\nlabel\n\nword_label.extend(label)\n\n+=['B']+['M']*(len(word)-2)+['E']\n\n#统计各个位置状态的出现次数，用于计算概率\n\nfor          index,value           in           enumerate(word_label):\n\nif ind nxit=p=rb:[value]+=1\n\nelse:\n\ntrans prob[word   label[index-1]][value]+=1\n\nemit prob[word   label[index]][word   list[index]]=( emit prob[word   label[index]].get(\n\nword_list[index],0)+1.0)\n\nPython 中文自然语言处理基础与实战\n\n2. 定义 viterbi 函数\n\nviterbi  函数用于实现维特比算法。将待分词文本输入其中，可以得到最大概率时每个 字的位置状态序列。viterbi 函数包含5个参数，分别是待分词文本、4个位置状态、初始概\n\n率、转移概率和发射概率。viterbi 函数需要实现以下3个步骤\n\n(1)对待分词文本的第一个字，计算4个位置状态下的初始概率。在当前语料下，寻\n\n找每个字在上述发射概率字典中对应的概率值，计算其发射概率。\n\n(2)求解在4个位置状态下，待分词文本中每个字的最大概率的位置状态，求得最大\n\n概率位置状态序列。\n\n(3)根据待分词文本末尾字的位置状态，从状态序列中选取其中概率最大的，函数将\n\n返回最大的概率值和对应的位置状态序列。\n\n实现 viterbi 算法，如代码4-4所示。\n\n66\n\n第④章  中文分词技术\n\n#计算待分词文本的位置状态概率值，得到最大概率位置状态序列\n\nfor           t           in           range(1,len(text)):\n\nV.append({})\n\nnewpath         ={}\n\nfor state in state   list:\n\nif  text[t]in   key   list:\n\nemit_count             =emit_prob[state].get(text[t],0)\n\nelse:\n\nemit   count =1\n\n(prob,a)=max(\n\n[(V[t-1][s]*trans  prob[s].get(state,0)*emit   count,s)\n\nfor s in state   list ifV[t-1][s]>0])\n\nV[t][state]=prob\n\nnewpath[state]=path[a]+[state]\n\npath            =newpath\n\n#根据末尾字的位置状态判断最大概率位置状态序列\n\nif            emit  prob['M'].get(text[-1],0)>emit  prob['s'].get(text[-1],0):\n\n(prob,a)=max([(V[len(text)-1][s],s)for                                 s                                in('E','M')])\n\nelse:\n\n(prob,a)=max([(V[len(text)-1][s],s)for                     s                      in                     state_list])\n\nreturn(prob,path[a])\n\n3. 分词\n\n分词通过cut 函数实现。 cut 函数的参数text 为待分词文本。cut  函数利用 JSON 库中的 loads 函数调用已保存的JSON 文件，再调用viterbi 算法求得概率最大的位置状态序列，最\n\n后判断待分词文本每个字的位置状态，对文本进行分词并输出结果。\n\n需要注意的是，每次程序运行结束后，如果需要再次运行，需要先删除已生成的 JSON\n\n文件，否则会继续对原文件写入内容，出现解析错误。\n\n对搜狐新闻文本进行HMM 中文分词，如代码4-5所示。\n\n67\n\nPython 中文自然语言处理基础与实战\n\n68\n\nbegin,follow\n\n=0,0\n\nfor        st , os_lit[ind erate(text):\n\nif             state             =='B':\n\nelif stb  ==' ex\n\nyield text[begin:index+1]\n\nfollow               =index               +1\n\nelif                 state                 =='S':\n\nyield char\n\nfollow               =index               +1\n\nif                 tex\n\ni，.datetime.now()\n\ntrain()\n\nendtime =datetime.datetime.now()\n\nprint((endtime-starttime).seconds)\n\ncut(text)\n\n()list(cut(text))))\n\n运行代码4-5后，得到的结果如下。\n\n4.5.2  提取新闻文本中的高频词\n\n如果一个词语在一篇文档中频繁出现并且有意义，说明该词语能很好地代表这篇文档 的主要特征，这样的词语称为高频词。这种词语在单篇文档中是关键词，在类似于新闻的 文章中是热词。字词的重要性随着它在文档中出现次数的增加而上升，随着它在语料库中\n\n出现频率的升高而下降。\n\n提取高频词时常常会遇到两个问题。 一是分词前需要删除语句之间的标点符号；二是 需要删除类似“是”“在”“的”等常用的停用词。利用jieba 提取高频词包含3个步骤，具\n\n体如下。    \n\n(1)读取 news.txt 文件。这是一个存放新闻文本的文件，新闻内容来自搜狐新闻。\n\n(2)加载停用词文件 stopword.txt,   对新闻内容进行jieba 分词。\n\n(3)提取出现频率最高的前10个词语，依次输出文档内容、分词后的文档和出现频率\n\n最高的10个词语。\n\n提取新闻文本中的高频词", "metadata": {}}, {"content": "，具\n\n体如下。    \n\n(1)读取 news.txt 文件。这是一个存放新闻文本的文件，新闻内容来自搜狐新闻。\n\n(2)加载停用词文件 stopword.txt,   对新闻内容进行jieba 分词。\n\n(3)提取出现频率最高的前10个词语，依次输出文档内容、分词后的文档和出现频率\n\n最高的10个词语。\n\n提取新闻文本中的高频词，如代码4-6所示。\n\n代码4-6  提取新闻文本中的高频词\n\nimport jieba\n\ndef             word_extract():\n\n#读取文件\n\ncorpus             =[]\n\n第 4 章  中文分词技术\n\npath                                                     ='../data/news.txt\n\ncontent                    =1t\n\nfor          line l line.stri()        open(path,'r',encoding='gbk',errors='ignore'):\n\ncontent  +=line\n\ncorpus.append (content)\n\np载停_ws =[]\n\npath                           ='../data/stopword.txt'\n\nfor         lil =li.stri(path,encoding='utf8'):\n\nstop_words.append(line)\n\n#jieba     分词\n\n69\n\nsplit_words\n\nword_list\n\n=[]\n\n=jieba.cut(corpus[0])\n\nfor          word          in          word          list:\n\nif     word     not      in      stop_words:\n\nsplit_words.append(word)\n\n提ic取=前{ 0个高频词\n\nword_num   =10\n\nfor       word       in       split_words:\n\nfreq_dwic +tems(),key =lambda x:x[1],\n\nreverse=True)[:word_num]\n\nprint(     ' 样 本 ： ' + corpus[0]\n\nprint(      '样本分词效果：‘+'/' .join(split_words))\n\nprint( '样本前10个高频词： '+ str(freq_word))\n\nword_extract()\n\n运行代码4-6后，输出结果如下。\n\n小结\n\n本章主要介绍了基于规则的分词方法、基于统计的分词方法和jieba 分词。首先介绍了 基于规则的正向最大匹配法、逆向最大匹配法和双向最大匹配法3种中文分词方法的基本 原理，并使用 Python 实现逆向最大匹配法分词。接着对基于统计的n 元语法模型和 HMM 分词方法的原理进行了讲解，使用Python 实现基于HMM 的分词。最后介绍了中文分词工\n\n具jieba 的分词模式，并使用jieba 完成高频词的提取。\n\nPython 中文自然语言处理基础与实战\n\n实训\n\n实 训 1  使 用 HMM 进行中文分词\n\n1.  训练要点\n\n(1)掌握训练HMM 的过程。\n\n(2)掌握使用viterbi 函数实现维特比算法。\n\n(3)掌握对文本进行分词。\n\n2. 需求说明\n\n对新闻语句“深航客机攀枝花机场遇险：机腹轮胎均疑受损，跑道灯部分损坏”使用\n\nHMM 进行中文分词。\n\n3. 实现思路与步骤\n\n(1)定义 train 函数，用于将初始概率、转移概率和发射概率写入JSON 文件中。\n\n( 2 ) 定 义viterbi  函数，用于实现维特比算法。\n\n(3)定义 cut 函数实现分词。\n\n实 训 2  提取文本中的高频词\n\n1.  训练要点\n\n掌握利用jieba 提取新闻文本中高频词的方法。\n\n2. 需求说明\n\n读取新闻文本 (flightnews.txt)    语料并提取文本中出现频率最高的前10个词语。\n\n3. 实现思路与步骤\n\n(1)读取 flightnews.txt 文件。\n\n(2)加载停用词文件 stopword.txt, 对新闻内容进行jieba分词。\n\n(3)提取出现频率最高的前10个词语。\n\n课后习题\n\n1. 选择题\n\n(1)不属于基于规则的分词方法的是(    )。\n\nA. 正向最大匹配法              B.  逆向最大匹配法\n\nC.  反向最大匹配法              D.  双向最大匹配法\n\n(2)不属于未登录词的是(    )。\n\nA.   网络热门词语                B.  人名、地名和组织机构名\n\nC.   化学试剂的名称               D.  经典文学作品\n\n(3)假设有语句序列{小孩，喜欢，在家，观看，动画片},估计这一语句的概率为(    ),\n\n设语料库中总词数为6000,单词出现的次数如图4-6所示。\n\n70\n\n第 ④ 章  中文分词技术\n\n351             873             792             170\n\n动画片\n\n1000\n\n图4-6 单词出现的次数\n\nA.0.004584            B.0.002223         C.0.004558                  D.0.006587\n\n(4)适合高阶 n-gram 模型的平滑方法为(    )。\n\nA.  加 1 平 滑                     B.  古德-图灵平滑\n\nC.  线性插值平滑                D.  均值平滑\n\n(5)不属于jieba 分词步骤的是(    )。\n\nA.  基于前缀词典快速扫描词图，搭建可能的分词结果的有向无环图，构成多条\n\n分词路径。\n\nB.  统计每个出现在词头的位置状态的次数，得到初始概率；统计每种位置状态\n\n转移至另一种状态的次数，得到转移概率。\n\nC.  采用动态规划法寻找最大概率路径，从右往左反向计算最大概率，依此类推，\n\n得到概率最大的分词路径，作为最终的分词结果。\n\nD.  采用 HMM处理未登录词，借助模型中语句构成的4个状态 B 、M 、E 、S推\n\n导，最后利用维特比算法求解最优分词路径。\n\n2. 操作题\n\n(1)完成本章中的所有代码操作。\n\n(2)仿照逆向最大匹配法的程序，编写正向最大匹配法的 Python 程序。\n\n(3)编写双向最大匹配法的Python 程序。利用 train 函数、viterbi  函数和 cut 函数对文\n\n本 news.txt 进行高频词提取。\n\n71\n\n第 5 章 词性标注与命名实体识别\n\n词性标注与命名实体识别是 NLP 中的关键性基础任务。词性标注是很多 NLP 任务中 的预处理步骤，经过词性标注后的文本会给信息提取带来很大的便利。命名实体识别是信 息提取、信息检索、机器翻译、问答系统等NLP 技术中的重要组成部分。本章主要介绍词 性标注的基本概念、规范和基本方法，以及命名实体识别的基本概念和条件随机场模型，\n\n并通过实例演示命名实体识别的实现流程。\n\n学习目标\n\n(1)了解词性标注与命名实体识别的基本概念。\n\n(2)熟悉jieba 词性标注的流程。\n\n(3)熟悉条件随机场模型的基本原理。\n\n(4)掌握命名实体识别的实现流程。\n\n5.1 词性标注\n\n词性标注是指为分词结果中的每个词标注一个词性的过程，也就是确定每个词是名词、\n\n动词、形容词或其他词性的过程。\n\n5.1.1 词性标注简介\n\n中文词性标注与英文词性标注相比有一定的难度，因为中文不像英文可以通过词的形 态变化判断词的词性。此外， 一个中文词可能有多种词性，在不同的句子中表达的意思也 不相同。例如，“学习能使我进步”这句话中的“学习”是名词，而“我要好好学习”这句\n\n话中的“学习”是动词。\n\n词性标注主要有基于规则和基于统计两种标注方法。基于规则的标注方法是较早的一 种词性标注方法，这种方法需要获取能表达一定的上下文关系及其相关语境的规则库。获 取一个好的规则库是比较困难的，主要的获取方式是人工编制包含繁杂的语法或语义信息\n\n的词典和规则系统，比较费时费力，并且难以保证规则的准确性。\n\n20世纪70年代末到20世纪80年代初，基于统计的标注方法开始得到应用。其中具 有代表性的是基于统计模型(n 元语法模型和马尔可夫转移矩阵)的词性标注系统", "metadata": {}}, {"content": "，这种方法需要获取能表达一定的上下文关系及其相关语境的规则库。获 取一个好的规则库是比较困难的，主要的获取方式是人工编制包含繁杂的语法或语义信息\n\n的词典和规则系统，比较费时费力，并且难以保证规则的准确性。\n\n20世纪70年代末到20世纪80年代初，基于统计的标注方法开始得到应用。其中具 有代表性的是基于统计模型(n 元语法模型和马尔可夫转移矩阵)的词性标注系统，它们 通过概率统计的方法进行自动词性标注。基于统计的标注方法主要包括基于最大熵的词性\n\n标注、基于统计最大概率输出的词性标注和基于 HMM 的词性标注。基于统计的标注方法\n\n第6章词性标注与命名实体识别\n\n能够抑制小概率事件的发生，但会受到长距离搭配上下文的限制，有时基于规则的方法更\n\n容易实现。\n\n基于规则的标注方法和基于统计的标注方法在使用的过程中各有所长，但也存在一些 缺陷。因此，就有了将基于规则与基于统计相结合的词性标注方法——jieba  词性标注，此\n\n方法具有效率高、处理能力强等特点。\n\n5.1.2 词性标注规范\n\n现代汉语中的词性可分为实词和虚词，共有12种。实词有名词、动词、形容词、代 词、数词、量词；虚词有副词、介词、连词、助词、拟声词、叹词。名词是表示人和事 物的名称的实词，动词表示人或事物的动作、行为、发展、变化，形容词表示事物的形 状、性质、状态等。通常用一些简单字母编码对中文词性进行标注，如动词、名词、形 容词分别用 “v”“n”“a”       表示。事实上，中文的词性标注至今还没有统一的标注标准， 使用较为广泛的有宾州树库和北大词性标注规范。本书采用北大词性标注规范，如表5-1\n\n所示。\n\n表5 - 1 北大词性标注规范\n\n编码 词性名称 注解 Ag 形语素 形容词性语素。形容词代码为a,语素代码g前面置以A a 形容词 取英语形容词(adjective)的第一个字母 ad 副形词 直接作状语的形容词。形容词代码a和副词代码d并在一起 an 名形词 具有名词功能的形容词。形容词代码a和名词代码n并在一起 b 区别词 取汉字“别”的声母 C 连词 取英语连词(conjunction)的第一个字母 Dg 副语素 副词性语素。副词代码为d,语素代码g前面置以D d 副词 取英语副词(adverb)的第二个字母，因其第一个字母已用于形容词 e 叹词 取英语叹词(exclamation)的第一个字母 f 方位词 取汉字“方”的声母 g 语素 绝大多数语素都能作为合成词的“词根”,取汉字“根”的声母 h 前接成分 取英语单词“head”的第一个字母 i 成语 取英语成语(idiom)的第一个字母 简称略语 取汉字“简”的声母 k 后接成分 当后接成分前面为较长的短语或句子时，单独标注为k 习用语 习用语尚未成为成语，有点“临时性”,取“临”的声母 m 数词 取英语数字(numeral)的第三个字母，n和u已有他用 Ng 名语素 名词性语素。名词代码为n,语素代码g前面置以N n 名词 取英语名词(noun)的第一个字母 nr 人名 名词代码n和汉字“人”的声母并在一起\n\n73\n\nPython 中文自然语言处理基础与实战\n\n续表\n\n编码 词性名称 注解 ns 地名 名词代码n和处所词代码s并在一起 nt 机构团体 “团”的声母为t,名词代码n和t并在一起 nz 其他专名 “专”的声母的第一个字母为z,名词代码n和z并在一起 0 拟声词 取英语拟声词(onomatopoeia)的第一个字母 P 介词 取英语介词(prepositional)的第一个字母 q 量词 取英语量词(quantity)的第一个字母 r 代词 取英语代词(pronoun)的第二个字母，因p已用于介词 S 处所词 取英语处所(space)的第一个字母 t 时间词 取英语time的第一个字母 Tg 时语素 时间词性语素。时间词代码为t,在语素的代码g前面置以T u 助词 取英语助词(auxiliary)的第二个字母，因a已用于形容词 Vg 动语素 动词性语素。动词代码为v,在语素的代码g前面置以V V 动词 取英语动词(verb)的第一个字母 vd 副动词 直接作状语的动词。动词代码v和副词代码d并在一起 vn 名动词 指具有名词功能的动词。动词代码v和名词代码n并在一起 W 标点符号 所有的标点符号 X 非语素字 非语素字只是一个符号，字母x通常用于代表未知数、符号 y 语气词 取汉字“语”的声母 Z 状态词 取汉字“状”的声母的第一个字母\n\n表5-1所示为部分北大词性标注规范的编码及其注解，根据这个标准可以对一些句子段 落进行词性标注。例如，句子“元旦来临，安徽省合肥市长江路悬挂起3300 盏大红灯笼，\n\n为节日营造出‘千盏灯笼凌空舞，十里长街别样红’的欢乐祥和气氛。”的标注结果如下。\n\n5.1.3 jieba 词性标注\n\njieba 词性标注是基于规则与基于统计相结合的词性标注方法。jieba 词性标注与其分词 的过程类似，利用词典匹配与 HMM 共同合作完成词性标注。其词性标注流程可概括为以\n\n下两种情况。\n\n(1)如果是汉字，那么将基于前缀词典构建有向无环图，对有向无环图计算最大概率 路径，同时在前缀字典中查找所分词的词性。如果没有找到，那么将其标注为 “x”  (表示 词性未知);如果在标注过程中标志为未知，并且该词为未登录词，就调用HMM 进行词性\n\n标注。\n\n74\n\n第6章词性标注与命名实体识别\n\n(2)如果不是汉字，就使用正则表达式判断词的类型，并赋予对应的词性，其中 “x”\n\n表示未知词性， “m”  表示数词， “eng”   表示英文词。\n\njieba 词性标注的流程如图5-1所示。\n\n开始\n\n75\n\n正则表达式判断汉字\n\n是\n\n构建有向无环图  计算最大概率路径\n\n未知且为未登录词\n\n是\n\n调用HMM标注词性\n\n否\n\n否\n\n词典包含该词\n\n是\n\n标注词典中的词性\n\n标注为其他\n\n类型(数词、\n\n英文词或未\n\n知词性)\n\n否\n\n标注为 “x”\n\n输出词性标注结果\n\n图5-1 jieba 词性标注的流程\n\n下面以“去森林公园爬山”为例，介绍jieba 词性标注的流程。\n\n(1)加载离线统计词典。离线统计词典在 “chapte\\data” 文件夹中。词典的每一行有3\n\n列，第一列是词，第二列是词频，第三列是词性，其示例如表5-2所示。\n\n表 5 - 2 离线统计词典示例\n\n词 词频 词性 1号店 3 n 1號店 3 n 4 S 店 3 n A型 3 n B B 机 3 n\n\n(2)构建前缀词典。利用离线统计词典构建前缀词典。例如，统计词典中的词“森林 公园”的前缀分别是“森”“森林”“森林公”;“公园”的前缀是“公”;“爬山”的前缀是\n\n“爬”。统计词典中所有的词形成的前缀词典如表5-3所示。\n\nPython 中文自然语言处理基础与实战\n\n表 5 - 3 前缀词典\n\n前缀 词频 词性 去 123402 V 森 742 n 林 8581 ng 公 5628 n 园 1914 Zg 爬 4046 V 山 23539 n 森林 5024 n 森林公园 3 n 爬山 117 n\n\n(3)构建有向无环图。首先基于前缀词典对输入文本“去森林公园爬山”进行切分。  对于“去”,没有前缀，没有其他匹配词", "metadata": {}}, {"content": "，没有其他匹配词，划分为单个词；对于“森”,则有“森”“森林” “森林公园”3 种方式；对于“林”,只有一种划分方式；对于“公”,则有“公”“公园”\n\n两种划分方式，依次类推，可以得到每个字开始的前缀词的切分方式。\n\n然后构建字的映射列表。在jieba 分词中，每个字都是通过在文本中的位置进行标记 的，因此可以构建一个以每个字开始位置与相应切分的末尾位置构成的映射列表，如表\n\n5-4所示。\n\n表5 - 4  映射列表\n\n位置 字 映射 注解 0 去 0:[0] 表示0→0映射，即开始位置为0、末尾位置为0对应的词，就是“去” 1 森 1:[1,2,4] 表示1→1、1→2、1→4的映射，即开始位置为1、末尾位置为分别为1、 2、4的词，则有“森”“森林”“森林公园”这3个词 2 林 2:[2] 2→2 3 公 3:[3,4] 3 →3、3 →4,“公”“公园” 4 园 4:[4] 4 → 4、“园” 5 爬 5:[5,6] 5 →5、5 →6,“爬”“爬山” 6 山 6:[6] 6 → 6,“山’\n\n最后，根据映射列表构建有向无环图。对于每一种切分，将相应的首尾位置相连。例\n\n如，对于位置1,映射为1:[1,2,4],将位置1与位置1、位置2、位置4相连接，最终构成\n\n一个有向无环图，如图5-2所示。\n\n76\n\n第5章词性标注与命名实体识别\n\n公\n\n3\n\n图5-2 有向无环图\n\n(4)计算最大概率路径。得到有向无环图后，每条有向边的权重(用概率值表示)  可以通过前缀词典的词频获得。图5-2所示的从起点到终点存在多条路径，每一条路径代 表一种分词结果。例如“去/森/林/公/园/爬/山”“去/森林/公园/爬山”“去/森林公园/爬山”。 在所有的路径中，需要计算一条概率最大的路径，也就是在所有切分结果中选取概率最 大的一种切分。jieba  利用动态规划法计算概率最大的路径，通过从句子的最后一个字开 始倒序遍历句子的每个字的方式，计算不同分词结果的概率对数得分。得到概率最大的 路径后，就可获得分词的结果，同时在前缀字典中查找所分词的词性，得到分词后的词\n\n性标注。\n\njieba 词性标注在实际应用中使用psg.cut 函数命令实现，不需要另外编写命令。对“去\n\n森林公园爬山”进行词性标注，如代码5-1所示。\n\n代码5- 1 对“去森林公园爬山”进行词性标注\n\n输出结果如下。\n\n在词性标注结果中，“。”被标注为 “x” 。 因为句号是标点符号的一种，所以被标注 为未知词性。在实际应用， 一些字典中不存在的词会被标注为未知词性 “x”,    这对词性 统计等处理结果会有一定的影响。因此在使用 jieba   自定义字典时，要尽可能完善词典\n\n信息。\n\n5.2 命名实体识别\n\n命名实体识别 (Named      Entity      Recognition,NER) 中的“命名实体”一般是指文本中\n\n具有特别意义或指代性非常强的实体。\n\n5.2.1  命名实体识别简介\n\n命名实体可分为实体类、时间类和数字类3大类，以及人名、机构名、地名、时间、 日期、货币和百分比7小类。命名实体识别在NLP- 中占有重要地位，它是信息提取、机器\n\n翻译和问答系统等应用领域里的基础工具。\n\n命名实体识别的任务就是识别出文本中的命名实体。命名实体识别通常分为实体边界\n\n77\n\nPython 中文自然语言处理基础与实战\n\n识别和实体类别识别两个过程。中文文本中没有类似英文文本中空格之类的显式标示词的 边界标示符，也没有英文中较为明显的首字母大写标志的词，这使中文的实体边界识别变\n\n得很有挑战性。中文实体识别的挑战性主要表现在以下3个方面。\n\n(1)中文词灵活多变。有些词语在不同语境下可能是不同的实体类型，如中国辽宁省 有个市叫“沈阳”,中国也有一些人名叫“沈阳”,同一个词“沈阳”在不同语境下可以是 地名或人名。有些词语在脱离上下文语境的情况下无法判断是否为命名实体，特别是一些\n\n带有特殊意义的名称，如“柠檬”在某些情况下会被认为是一个现代流行的形容词。\n\n(2)中文词的嵌套情况复杂。 一些中文的命名实体中常常嵌套另外一个命名实体。 例如，“北京大学附属中学”这一组织机构名中还嵌套着同样可以作为组织机构名的“北 京大学”,以及地名“北京”。命名实体的互相嵌套情况，为命名实体的识别带来一定的\n\n困难。\n\n(3)中文词存在简化表达现象。有时会对一些较长的命名实体词进行简化表达，如“北 京大学”通常简化为“北大”,“北京大学附属中学”通常简化为“北大附中”,这无疑也为\n\n命名实体的识别带来了一定负担。\n\n命名实体识别实际上是序列标注问题。命名实体识别领域常用的3种标注符号B 、I 、 O  分别代表实体首部、实体内部、其他。在字一级的识别任务中，对人名、地名、机构名 的3种命名实体 PER、LOC 、ORG,定义了7种标注的集合L={B-PER,I-PER,B-LOC,I-LOC,   B-ORGI-ORGO},   分别代表的是人名首部、人名内部、地名首部、地名内部、机构名首部、 机构名内部和其他。例如，“尼克松是出身于加利福尼亚的政治家”,标注序列为{B-PER,\n\nI-PER,I-PER,O,O,O,O,B-LOC,I-LOC,I-LOC,I-LOC,I-LOC,O,0,0,0                  }。\n\n假设x=x ;,x₂,…, x, 为待标注的字观测序列， S={s;,s₂,…,sy}    为待标注的状态集合。命 名实体识别问题可描述为求概率p(y|x) 最大的状态序列y=yy,y₂,…,y₂,      其中y,∈S,   如式\n\n(5-1)所示。\n\n                                (5-1)\n\n早期的命名实体识别方法主要是基于规则的方法，后来，基于大规模语料库的统计方 法逐渐成为主流。HMM、最大熵马尔可夫模型(Maximum  Entropy  Markov  Model,MEMM) 和条件随机场 (Conditional    Random     Field,CRF) 是命名实体识别中最常用、最基本的3\n\n个统计模型。首先出现的是 HMM,  其次是MEMM,   最后是CRF。\n\n5.2.2  CRF 模型\n\nCRF 模型最早由拉弗蒂等人于2001年提出，其模型思想主要来源于最大熵模型。CRF 模型是一种基于统计方法的模型，可以被认为是一个无向图模型或一个马尔可夫随机场。 它是一种用于标记和切分序列化数据的统计框架模型。相对于 HMM  和 MEMM,CRF     模\n\n型没有 HMM 那样严格的独立性假设，同时克服了MEMM 标记偏置的缺点。\n\nCRF 理论在命名实体识别、语句分词、词性标注等语言处理领域有着十分广泛且深入 的应用。与 HMM 不同， CRF 中当前的状态不是只由这一时刻的观测条件给出，而是与整 个序列的状态相关，即 CRF 模型输出序列依赖于观测条件下的所有观测数值。CRF 在解决\n\n英语浅层分析、英文命名实体识别等任务时取得了良好的效果。 CRF 的特性和研究成果表\n\n78\n\n第5章词性标注与命名实体识别\n\n明，它也适用于中文命名实体识别的研究任务。\n\n1. 线性链条件随机场\n\n如果一个随机变量序列X=X₁,X₂,…,X,中各个节点之间的关系是呈线性的，则称序列\n\nX 是一个线性链。\n\n设 X=X₁ ,X₂,…,X,   和 Y=Yγ,Y₂,…,Y,均为线性链表示的随机变量序列。若在给定观测 序 列X 的条件下，随机变量序列γ的条件概率分布p(Y|X)   满足马尔可夫性", "metadata": {}}, {"content": "，它也适用于中文命名实体识别的研究任务。\n\n1. 线性链条件随机场\n\n如果一个随机变量序列X=X₁,X₂,…,X,中各个节点之间的关系是呈线性的，则称序列\n\nX 是一个线性链。\n\n设 X=X₁ ,X₂,…,X,   和 Y=Yγ,Y₂,…,Y,均为线性链表示的随机变量序列。若在给定观测 序 列X 的条件下，随机变量序列γ的条件概率分布p(Y|X)   满足马尔可夫性，如式(5-2)\n\n所示。\n\np(Y₁|X,Y,Y₂,…,Y₁)=p(Y₁|X,Y_,Y),i=1,2,…,n                                             (5-2)\n\n则称 p(Y|X)   为线性链条件随机场 (linear-CRF) 。linear-CRF      的结构如图5-3 所示。\n\n图5-3 linear-CRF 的结构\n\n2.linear-CRF 参数化形式\n\nlinear-CRF 可以用于文本标注问题。在条件概率p(Y|X)   中 ，X 表示输入观测序列， Y  表示对应的输出标记序列或状态序列。模型在学习时，利用训练数据通过极大似然估计或 正则化的极大似然估计得到条件概率模型p(Y|X);      预测时，对给定的输入序列X,     求 出\n\n条件概率 p(Y|X) 最大的输出序列。\n\n如果将Y的每一个标注序列当作一个类别，那么可以将序列标注问题看作多分类问题， 即输入观测序列X,    求出条件概率p(Y|X)    最大的类别。在多分类问题中，多项逻辑斯谛\n\n回归是最经典的一类模型，如式(5-3)所示。\n\n(5-3)\n\n其中， x 是特征向量，0为特征向量的权重。\n\n因为输入序列X 一般是一段文本序列，所以不能直接使用机器学习算法，需要将它们 转化成机器学习算法可以识别的数值特征，然后再交给机器学习算法进行操作。对于序列  标注问题，需要先对输入序列X 进行特征提取。由式(5-2)可知， p(Y₁|X) 条件概率分布  与节点X 、Y₂ 、Y,和Y  有关。因此，构建的特征函数应该将这些节点信息特征提取出来。\n\n选取适当的特征是 CRF 的一项重要任务，特征的选择会直接影响最终标注的质量。\n\n设随机变量X 取值为x,Y   取值为Y。序列X  的特征提取是通过构建特征函数完成的。 特征函数包含4个参数，分别为文本x 、参数i  (表示文本x 中第i 个词)、y, ( 第i 个词的\n\n标注值)和y  ( 第i-1个词的标注值)。因为 linear-CRF 满足马尔可夫性，所以只有上下\n\n79\n\nPython 中文自然语言处理基础与实战\n\n文相关的局部特征函数，没有不相邻节点之间的特征函数。\n\n特征函数分为两类，第一类是定义在Y上的转移特征函数t,  这类特征函数只与当前节\n\n点y, 和上一个节点y   有关，如图5-4所示。\n\n图5-4  特征函数\n\n特征函数如式(5-4)所示。\n\nt₄(V-,Y,x,i),k=1,2,…,K₁                                                 (5-4)  其中， i 表示当前节点在序列中的位置， y, 表示序列x 第i 个字的标注， K₁是定义在该节点\n\n的局部特征函数的总数。\n\n第二类是定义在 Y节点y;上的状态特征函数 s,  这类特征函数只和当前节点有关，如\n\n图5-4所示。特征函数如式(5-5)所示。\n\ns,(vj,x,i),1=1,2,…,K₂                                                (5-5)\n\n其中， K, 是定义在该节点的局部特征函数的总数。\n\n需要注意的是，两类特征函数是人为定义的。特征函数的输出值是0或1,0表示要标 注序列不符合这个特征，1 表示要标注序列符合这个特征。假设x= “我去广州旅游”,标 注的状态集合为S={B,I,O},    分别代表命名实体首部、实体内部、其他。状态特征函数s 和\n\n转移特征函数t 可以定义为如下形式，如式(5-6)～式(5-11)所示。\n\n(5-6) (5-7)\n\n(5-8)\n\n(5-9)\n\n(5-10)\n\n80\n\n第 5 章 词性标注与命名实体识别\n\n(5-11)\n\n特征函数不限于上述的定义，还可以定义很多个。直观上，上述特征函数t 比t₃在语 料库中出现的可能性更大，理应拥有更大的权重。同理，可以为每个特征函数赋予一个权\n\n值，用于表达这个特征函数的重要性。\n\n构建完特征函数，即可仿照多项逻辑斯谛回归模型定义 linear-CRF  的条件概率分布\n\np(Y|X)。\n\n设p(Y|X) 为 linear-CRF 的条件概率分布，随机变量X 取值为x, 则随机变量Y 取值\n\n为y的条件概率分布的定义如式(5-12)所示。\n\n81\n\n在式(5- 12)中， Z(x)  的定义如式(5-13)所示。\n\n(5-12)\n\n(5-13)\n\n在式(5-13)中， t₄(V;_,y,,x,i)和s,(v,,x,i)称为转移特征函数和状态特征函数，特征函 数t 和s,取值为1或0。当满足特征条件时取值为1,否则为0。λ和μ,分别是转移函数和\n\n状态函数的权重。\n\n3. 特征函数简化形式\n\n为了便于描述，将两类特征函数表示为式(5-14)。\n\n对转移与状态特征在各个位置i 求和，如式(5-15)所示。\n\n,k=1,2…,K₇+K₂\n\n用o₂ 表示特征f(x,y)   的权值，如式(5-16)所示。\n\n这样式(5-12)、式(5-13)可分别表示为式(5-17)、式(5-18)。\n\n(5-14)\n\n(5-15)\n\n(5-16)\n\n(5-17)\n\n(5-18)\n\n其中， K=K₁+K₂。\n\n进一步，若用F(x,y)=(f;(x,y),f₂(x,y),…,f(x,y))     表示全局特征向量，w=(oy,o₂,…,ox)   表示权值向量，则式(5-17)和式(5-18)可以表示为内积形式，如式(5-19)和式(5-20)\n\n所示。\n\nPython 中文自然语言处理基础与实战\n\n(5-19)\n\n(5-20)\n\n4. 条件随机场的参数估计问题\n\n在式(5- 19)和式(5-20)中，参数o 是未知的， linear-CRF 模型参数需要通过训练数\n\n据进行估计。\n\n假定对于训练数据有一组样本集D={x,,y;},  其中j=1,2,…,N,  样本之间是相互独立 的", "metadata": {}}, {"content": "，参数o 是未知的， linear-CRF 模型参数需要通过训练数\n\n据进行估计。\n\n假定对于训练数据有一组样本集D={x,,y;},  其中j=1,2,…,N,  样本之间是相互独立 的，p(x,y)为训练样本中(x,y)的经验概率。对于条件概率模型p。(y|x),  训练数据样本集 D\n\n的对数似然函数如式(5-21)所示。\n\n82\n\n由式(5-17)、式(5-18)可得式(5-22)。\n\n(5-21)\n\n(5-22)\n\n其中 = (oy,o₂,…,Ox) 是需要估计的权重参数。\n\n已知训练数据集D,    由此可知经验概率分布p(x,y) 。 可以通过极大化训练数据的对数 似然函数求模型参数。具体的优化算法有改进的迭代尺度法、梯度下降法和拟牛顿法。有\n\n兴趣的读者可以查阅相关资料学习。\n\n5. 条件随机场的预测问题\n\n预测问题就是给定条件随机场 p(Y|X)  和输入序列x  (观测序列),求条件概率最大的\n\n输出序列y^ (标记序列),如式(5-23)所示。\n\n(5-23)\n\n可以利用动态优化的算法求解式(5-23),常用的求解方法有 Viterbi 动态优化法。\n\n5.3    任务：中文命名实体识别\n\n在中文命名实体识别中，比较常见的是对文本中的时间、人名、地名和组织机构名 进行识别。本节将使用 sklearn-crfsuite   库对时间、人名、地名和组织机构名进行命名实\n\n体识别。\n\n第 6 章 词性标注与命名实体识别\n\n5.3.1 sklearn-crfsuite 库简介\n\nCRFsuite 基于C/C++实现了条件随机场模型，可用于快速训练和序列标注。sklearn-  crfsuite 库是基于 CRFsuite 库的一款轻量级的 CRF 库，提供了条件随机场的训练、预测 和评测方法。该库可兼容 sklearn 算法，因此可以结合 sklearn 库的算法设计命名实体识\n\n别系统。\n\n启动Anaconda Prompt,在命令行输入并执行“pip install sklearn-crfsuite”命令安装 sklearn-\n\ncrfsuite 库。\n\n5.3.2  命名实体识别流程\n\n使用 sklearn-crfsuite 库进行中文命名实体识别的步骤包括文本预处理(语料预处理、 语料初始化、训练数据),以及模型训练与预测(模型训练、模型预测)。其中将实现文 本预处理步骤的代码定义为CorpusProcess   类，实现模型训练与预测步骤的代码定义为\n\nCRF  NER 类，\n\nCorpusProcess 类主要实现的内容包括语料读取与写入、语料预处理(全角转半角、连 接姓和名、合并中大粒度分词与时间)、语料初始化(初始化字序列、词性序列、标记序列)、\n\n训练数据(窗口切分、特征提取)。CorpusProcess 类的框架如代码5-2所示。\n\n代码5-2 CorpusProcess  类的框架\n\nclass   CorpusProcess(object):\n\n#初始化\n\ndef            _init     (self):\n\npass\n\n#读取语料\n\ndef   read   corpus   from   file(self,file  path):\n\npass\n\n#写入语料\n\ndef           write_corpus_to_file(self,data,file_path):\n\npass\n\n#全角转半角\n\ndef q_to_b(self,q_str):\n\npass\n\n#处理姓名，将姓和名连接在一起，如：张/nr  三/nr\n\ndef  process  nr(self,words)\n\npass\n\n#处理语料库中大粒度分词，如：[湖南/n    电视台/n]nt\n\ndef  process  k(self,words):\n\npass\n\n#处理分开的时间，如：1999年/t   12月 /t\n\ndef     process_t(self,words):\n\n83\n\nPython 中文自然语言处理基础与实战\n\npass\n\n#语料预处理和存储\n\ndef pre process(self):\n\npass\n\n#由词性提取标签\n\ndef pos  to  tag(self,p):\n\npass\n\n#标签使用BIO 模式\n\ndef                          tag_perform(self,tag,index):\n\npass\n\ndef initialize(self):\n\npass\n\n#初始化字序列、词性序列、标记序列\n\ndef           init_sequence(self,words_list):\n\npass\n\n口 ment_by_window(self,words_list=None,window=3):\n\npass\n\n#特征提取\n\ndef extract   feature(self,word   grams):\n\npass\n\n#训练数据\n\ndef   generator(self):\n\npass\n\n#标签使用BIO 模式\n\ndef           tag_perform(self,tag,index):\n\npass\n\n#语料初始化\n\ndef initialize(self):\n\npass\n\n#初始化字序列、词性序列、标记序列\n\ndef                           init_sequence(self,words_list):\n\npass\n\n#窗口切分\n\ndef     segment_by_window(self,words_list=None,window=3): pass\n\n84\n\n第 6 章 词性标注与命名实体识别\n\nCRF_NER 类主要实现的内容包括语料预处理执行、模型定义、模型训练与保存、模型\n\n预测。CRF_NER 类的框架如代码5-3所示。\n\n代码5-3 CRF_NER 类的框架\n\nclass           CRF_NER(object):\n\n#初始化\n\ndef            _init     (self):\n\npass\n\n#初始化CRF模型参数\n\ndef                     initialize_model(self):\n\npass\n\nf型t练in(self):\n\npass\n\n#模型测试\n\ndef\n\npass\n\n1. 语料预处理\n\n本小节使用的数据来源于1998年《人民日报》分词数据集，该数据集中的所有词已经\n\n标注了词性，其中部分数据集示例如下。\n\n由于数据集中存在字符格式不统一、姓氏与名字分为两个词语等问题，因此需要对语\n\n料进行统一格式化处理，主要包括以下4个内容。\n\n① 将语料全角字符统一转为半角字符。\n\n② 处理姓名，将姓和名连接在一起，如：张/nr  三/nr。\n\n③ 处理合并语料库中括号中的大粒度分词，如：[湖南/n    电视台/n]ns。\n\n④ 处理合并语料库中分开标注的时间，如：12月/t  31日/t.\n\n2. 语料初始化\n\n语料初始化包括初始化字序列和词性序列，主要是对语料中的句子、词性、实体分类\n\n标记进行区分。\n\n85\n\nPython 中文自然语言处理基础与实战\n\n① 由词性提取标签。将语料中的时间、人名、组织机构名和地名分别转化为T 、PER、\n\nORG 和 LOC。\n\n② 标签使用 BIO 模式。本次语料的标签采用 BIO 模式，即实体的第一个字为 B*,\n\n其余字为I*,     非实体字统一标记为O。\n\n此外，由于模型采用 tri-gram  形式", "metadata": {}}, {"content": "，主要是对语料中的句子、词性、实体分类\n\n标记进行区分。\n\n85\n\nPython 中文自然语言处理基础与实战\n\n① 由词性提取标签。将语料中的时间、人名、组织机构名和地名分别转化为T 、PER、\n\nORG 和 LOC。\n\n② 标签使用 BIO 模式。本次语料的标签采用 BIO 模式，即实体的第一个字为 B*,\n\n其余字为I*,     非实体字统一标记为O。\n\n此外，由于模型采用 tri-gram  形式，因此字符列中需要在句子前后加上占位符\n\n(BOS/EOS) 。  语料初始化的实现过程如代码5-4所示。\n\n86\n\n代码5-4\n\nimport joblib\n\nimport                 sklearn_crfsuite\n\nclass          CorpusProcess(object): #由词性提取标签\n\n语料初始化的实现过程\n\ndef pos  to  tag(self,p):\n\nt=self._maps.get(p,None)\n\nreturn     t     if    t     else     'O'\n\n#标签使用BIO模式\n\ndef                                 tag_perform(self,tag,index):\n\nif     index     ==0      and      tag      !='O':\n\nelif trrB''{:}'.format(tag)\n\nreturn 'I   {}'.format(tag)\n\n#全 to_b(self,q_str):\n\nb_str           =\"\n\nfor uchar  in  q   str:\n\ninside_code                    =ord(uchar)\n\nif     inside_code      ==12288:   #全角空格直接转换\n\ninside  code        =32\n\n_\n\nelif              65374>=inside_code\n\ninside  code -=65248\n\n>=65281:# 全角字符(除空格)根据关系转化\n\nb   str +=chr(nside   code)\n\nreturn b   str\n\n#语料df始iitialize(self):\n\nlines   =self.read   corpus   from   file(self.process   corpus  path)\n\nwords_list=[line.strip().split('')for         line         in         lines         if        line.strip()]\n\ndel                 lines\n\nself.init   sequence(words   list)\n\n#初始化字序列、词性序列\n\ndef\n\nwords_seq\n\ninit_sequence(self,words_list): =[[word.split('/')[0]for\n\nword       in       words]for       words       in\n\nwords_listos   seq=[[word.split('/')[1]for word inwords]for words in words   list]\n\ntag_seq=[[self.pos_to_tag(p)for p in pos]for pos in pos_seq]\n\nself.tag   seq=[[[self.tag  perform(tag   seq[index][i],w)\n\nfor w in range(len(words   seq[index][i]))] for  i  in  range(len(tag   seq[index]))]\n\n第 5 章 词性标注与命名实体识别\n\n3. 训练数据\n\n训练数据是指将经过数据处理后的语料切分为整齐序列并提取相应的特征。\n\n① 窗口切分。将每个特征按统一的大小切分成整齐序列。\n\n② 特征提取。提取文本中的字符组合或具有其他意义的标记组成特征，作为特征函数\n\n的参数；利用一组函数完成由特征向数值转换的过程，使特征和一个权值对应。\n\n整个训练数据的实现过程如代码5-5所示。\n\n代码5-5 训练数据的实现过程\n\n#窗口切分\n\ndef      segment_by_window(self,words_list=None,window=3):\n\nwords        =[]\n\nbegin,end            =0,window\n\nfor                 _in                 range(1,len(words_list)):\n\nif                       end>len(words_list):\n\nbreak\n\nwords.append(words_list[begin:end])\n\nbegin           =begin           +1\n\nend       =end       +1\n\nreturn words\n\n#特征提取\n\ndef extract   feature(self,word   grams):\n\nfeatures,feature_list                                  =[],[]\n\nfor         index         in         range(len(word_grams)):\n\nfor i in range(len(word   grams[index])):\n\nword_gram   =word_grams[index][i]\n\nfeature ={'w-l':word   gram[0],\n\n'w':word_gram[1],'w+1':word_gram[2], 'w-1:w':word_gram[0]+word_gram[1],  'w:w+1':word_gram[1]+word_gram[2],\n\n'bias':1.0}\n\nfeature   list.append(feature)\n\nfeatures.append(feature   list)\n\nfeature_list                 =[]\n\nreturn features~\n\n#  训练数据\n\ndef    generator(self):\n\nword_grams    =[self.segment_by_window(word_list)for     word_list     in\n\nself.word_sefqe]atures =self.extract   feature(word   grams)\n\nreturn   features,self.tag   seq\n\n4. 模型训练\n\n模型训练由初始化模型参数、数据预处理与模型定义、模型训练3个部分组成。\n\n87\n\nPython 中文自然语言处理基础与实战\n\n① 初始化模型参数。定义优化算法、迭代次数和基本模型参数。其中优化算法选用了 无约束优化算法 L-BFGS,   该算法是解决无约束非线性规划问题最常用的方法，具有收敛\n\n速度快、内存开销小等优点。\n\n② 数据预处理与模型定义。执行数据预处理模块并定义模型。\n\n③ 模型训练。将数据集中前500行数据作为训练集，其余数据作为测试集，接着进行\n\n模型训练，最后将模型结果保存。\n\n整个模型训练的实现过程如代码5-6所示。\n\n代码5-6 模型训练的实现过程\n\nclass CRF NER(object):\n\n# 初 C_Fn参( lf):\n\nself.algorithm                      ='lbfgs'\n\n.= .    ='0.1!\n\nself.max iterations =100 # 迭代次数\n\n88\n\nself.model\n\nself.corpus\n\nself.model\n\n#定义模型\n\npath='../data/model.pkl\n\n=CorpusProcess()    # 加载语料预处理模块\n\n=None\n\ndef                      initialize_model(self):\n\nself.corpus.pre_process()                      # 语料预处理\n\n(gorithm\n\nmax iterations =int(self.max iterations)\n\nself.model =sklearn crfsuite.CRF(algorithm=algorithm,c1=c1,c2=c2,\n\nmax              iterations=max              iterations, all_possible_transitions=True)\n\n#模型训练\n\ndef train(self):\n\nself.initialize model()\n\nx,y=self.corpus.generator()\n\nx  train,y  train      =x[500;],y[500:]\n\nx_test,y                    test                    =x[:500],y[:500]\n\n.filist(selftdel.clas ns)_)\n\n_elsp.r ')=self.model.predict(x_test)\n\nmetrics.flat_f1_score(y_test,y_predict,average='weighted',labels=\n\nsorted labels =sorted(labels,key=lambda name:(name[1:],name[0])) print(metrics.flat  classification  report(\n\ny  test,y  predict,labels=sorted    labels,digits=3))\n\n#保存模型\n\njoblib.dump(self.model,self.model path)\n\n第6章词性标注与命名实体识别\n\n5. 模型评价\n\n在模型训练的过程中，可以通过精确率 P(precision) 、 召回率 R(recall)    和 F1 值\n\n(fl-score)3    项指标测评中文命名实体识别的识别性能和效果", "metadata": {}}, {"content": "，可以通过精确率 P(precision) 、 召回率 R(recall)    和 F1 值\n\n(fl-score)3    项指标测评中文命名实体识别的识别性能和效果，计算公式分别如式(5-24)、\n\n89\n\n式(5-25)、式(5-26)所示。\n\n(5-24)\n\n(5-25)\n\n(5-26)\n\n其中， P 表示识别出的命名实体中出现在测试结果中的比例， R 表示标准结果中被正 确识别出的命名实体的比例，N。表示正确找出的命名实体数， N₄表示测试结果中的所有 命名实体数， N,表示测试集中实际命名实体数。\n\n模型训练过程中输出的测评指标如下。\n\n其中LOC、ORG、PER、T分别表示地名、组织机构名、人名、时间，precision、recall、\n\nfl-score 、support 则为对应的准确率、召回率、F1 值和参与训练样本数。\n\n从测评指标结果看，总训练样本5955个，平均预测准确率0.953,平均召回率0.904,\n\n平均 F1 值0.928,总体模型训练效果较好。\n\n6. 模型预测\n\n模型预测需要先对待预测语料进行数据预处理，然后加载训练完成的模型进行预测，\n\n并输出语料中存在的命名实体。模型预测的实现过程如代码5-7所示。\n\n代码5-7 模型预测的实现过程\n\nPython 中文自然语言处理基础与实战\n\n输入两个句子，使用训练完成的模型进行预测，如代码5-8所示。\n\n代码5-8 使用训练完成的模型进行预测\n\n运行代码5-8后，模型输出结果如下。\n\n从输出结果可以看出，模型能正确将第一条语料中的时间、地名识别并输出；第二条 语料中的时间、公司名称、人名等命名实体能被完全识别并输出，效果最佳；总体上看\n\n该模型的命名实体识别效果较好。\n\n第一条语料中只有部分命名实体被识别并输出，是因为该语料较为复杂，命名实体较 难被识别出。对于这种情况，可以通过丰富训练数据语料库、增加训练次数等方式优化模\n\n型，以达到提高模型识别准确率的目的。\n\n小结\n\n本章主要介绍了词性标注和基于条件随机场的命名实体识别。首先介绍了词性标注， 重点介绍了基于规则和基于统计相结合的方法——jieba 词性标注方法。然后介绍了命名实 体识别，着重介绍了条件随机场的基本概念和基于条件随机场的命名实体识别的过程。最\n\n后利用条件随机场对命名实体识别过程进行讲解。\n\n实训  中文命名实体识别\n\n1.   训练要点\n\n掌握中文命名实体识别流程。\n\n90\n\n第 5 章 词性标注与命名实体识别\n\n2. 需求说明\n\n使用 sklearn-crfsuite 库对语句“2020年9月23日，‘1+X’  证书制度试点第四批职业 教育培训评价组织和职业技能等级证书公示，其中广东泰迪智能科技股份有限公司申请的\n\n大数据应用开发 (Python)   位列其中。”中的命名体进行识别。\n\n3. 实现思路与步骤\n\n(1)将数据预处理定义为CorpusProcess 类。\n\n(2)将模型训练与预测定义为CRF_NER 类。\n\n(3)输出语料中的命名实体。\n\n课后习题\n\n1. 选择题\n\n(1)下列关于jieba 词性标注的流程错误的是(   ~)。\n\nA.  加载离线统计词典            B.  构建前缀词典\n\nC.  构建无向无环图             D.  计算最大概率路径\n\n(2)不属于中文的实体边界识别变得更加有挑战性原因的是(    )。\n\nA.  中文词数量繁多              B.   中文词灵活多变\n\nC.  中文词的嵌套情况复杂        D.   中文词存在简化表达现象\n\n(3)CRF    模型思想主要来源于(    )。\n\nA.  无向图模型  B.   最大熵模型  C.   马尔可夫随机场  D.   统计方法\n\n(4)多分类问题中最经典的模型是(    )。\n\nA.CRF   模型                    B.   聚类模型\n\nC.  多项逻辑斯谛回归            D.  神经网络模型\n\n(5)下列关于特征函数的输出值是0或1的叙述正确的是(    )。\n\nA.0    表示要标注序列不符合这个特征，1表示要标注序列符合这个特征 B.0   表示要标注序列符合这个特征，1表示要标注序列不符合这个特征\n\nC.0    和1都表示要标注序列符合这个特征\n\nD.0    和1都表示要标注序列不符合这个特征\n\n2. 操作题\n\n(1)完成本章中的所有代码操作。\n\n(2)找一篇文章进行词性标注练习，查看标注结果。\n\n(3)找一篇文章进行命名实体识别练习，查看识别结果。\n\n91\n\n第 6 章 关键词提取\n\n文本是海量信息中最大并且使用最广泛的一种数据类型。信息数据虽然能为人们的生 活提供便利，但人们在提取其中有价值的信息时仍面临着困难。在NLP 领域中，常常要从 海量的文档中提取关键词汇，这些词汇能在一定程度上体现文档的核心内容，从而帮助用 户寻找所需的内容。本章首先介绍关键词提取技术，然后介绍 TF-IDF 、TextRank、LSA和 LDA 关键词提取算法，最后通过实例介绍 TF-IDF 、TextRank 和 LSI 三种关键词提取算法\n\n的使用。\n\n学习目标\n\n(1)了解关键词提取的基本概念。\n\n(2)了解 TF-IDF 、TextRank、LSA 和 LDA 关键词提取算法的基本原理。\n\n(3)熟悉使用 TF-IDF 、TextRank 和 LSI 算法实现关键词提取的流程。\n\n6.1   关键词提取技术简介\n\n关键词是能够反映文本主题或内容的词语。关键词这个概念是随着信息检索学科的出 现而被提出的，中文关键词是西方信息检索科学移植到中文的直接成果。关键词提取是从 单个文本或一个语料库中，根据核心词语的统计和语义分析，选择适当的、能够完整表达\n\n主题内容的特征项的过程。\n\n关键词提取技术的应用非常广泛，主要应用对象可以分为人类用户和机器用户。在面 向读者的应用中，要求所提取的关键词具有很高的可读性、信息性和简约性。关键词提取 技术的主要应用领域有新闻阅读、广告推荐、历史文化研究、论文索引等。在NLP中，关 键词作为中间产物，应用也非常广泛，主要应用领域有文本聚类、文本分类、机器翻译、\n\n语音识别等。\n\n由于关键词具有非常广泛的用途，因此开发出一套实用的关键词提取系统非常重要。 这就要求关键词提取算法不仅要在理论上正确，而且还要在工程上具有很好的实践效果。\n\n关键词提取系统的实用性主要表现在以下4个方面。\n\n(1)可读性。 一方面，由于中文的字与字之间是没有空格隔开的，因此需要使用分词 工具对文本进行切分，但分词工具对专有名词的切分准确率很低。另一方面，词的表达能\n\n力也非常有限，如“市场/经济”,其中任何一个词“市场”或“经济”都无法表达这个短\n\n第 6 章  关键词提取\n\n语的含义。因此，系统所提取出的关键词的可读性对系统的实用性影响较大。\n\n(2)高速性。系统应该具有较快的处理速度，能够及时处理大量的文本。例如一个针 对各类新闻的关键词提取系统，当新闻产生后，其应该能在数秒内提取出该新闻的关键词，\n\n这样才能保证新闻的实时性。\n\n(3)学习性。实用的关键词提取系统应该能处理的文本领域非常广泛，而不是仅局限 于特定领域。随着社会的高速发展，各种未登录词、网络新词频频出现，系统应具有较强\n\n的学习能力。\n\n(4)健壮性。系统应该具有处理复杂文本的能力，如中、英文混杂的文本，文字、图\n\n表、公式混杂的文本。\n\n6.2  关键词提取算法\n\n关键词能概括文本的主题，从而帮助读者快速辨别出所选内容是不是感兴趣的内容。 常见的关键词提取算法有 TF-IDF 算法、TextRank 算法和主题模型算法，其中主题模型算\n\n法主要包括 LSA 和 LDA 两种算法。\n\n6.2.1  TF-IDF  算 法\n\n关键词通常是指对文本内容具有重要代表性的词，即描述性关键词。在文本分类任务 中，更关注那些对分类具有重要意义的词，这些词称为区分性关键词。相应地描述性关键 词称为非区分性关键词。要得到区分性关键词，需要构建一些基本统计量，然后基于训练 文本计算词表中每个词在该统计量上的值", "metadata": {}}, {"content": "，其中主题模型算\n\n法主要包括 LSA 和 LDA 两种算法。\n\n6.2.1  TF-IDF  算 法\n\n关键词通常是指对文本内容具有重要代表性的词，即描述性关键词。在文本分类任务 中，更关注那些对分类具有重要意义的词，这些词称为区分性关键词。相应地描述性关键 词称为非区分性关键词。要得到区分性关键词，需要构建一些基本统计量，然后基于训练 文本计算词表中每个词在该统计量上的值，最后依据这些统计量的大小选择所需要的关键 词。如果这些统计量具有区分性，那么选出的关键词为区分性关键词，反之为非区分性关\n\n键词。\n\n词频-逆文档频率 (Term Frequency-Inverse Document Frequency,TF-IDF) 算法是基于 统计的最传统、最经典的算法，拥有简单而又迅速的优点。TF-IDF 算法的主要思想是字词 的重要性随着它在文档中出现次数的增加而上升，并随着它在语料库中出现频率的升高而 下降。TF-IDF算法由词频(Term   Frequency,TF)、逆文档频率(Inverse Document Frequency, IDF)  两部分组成。\n\n1. 词 频\n\n词频是一个词在一篇文档中出现频次的统计量。 一个词在一篇文档中出现的频次越高，\n\n其对文档的表达能力越强。词频的计算公式如式(6-1)所示。\n\n(6-1)\n\n其中， n., 表示词t, 在文档j  中出现的频次， 表示文档j 的总词数。\n\n2. 逆文档频率\n\n逆文档频率是一个词出现在文档集中文档的频次的统计量。 一个词在文档集中越少\n\n出现在文档中，说明这个词对文档的区分能力越强。逆文档频率的计算公式如式(6-2)\n\n93\n\nPython 中文自然语言处理基础与实战\n\n所示。\n\n(6-2)\n\n其中， |D|表示文档集中的总文档数， {j:t,ed,}|表示文档集中文档d, 出现词t, 的文档个数，\n\n分母加1是为了避免文档集中没有出现词t,,   导致分母为零的情况。\n\n词频 TF  注重词在文档中的出现频次，没有考虑到词在其他文档下的出现频次，缺乏 对文档的区分能力。逆文档频率IDF 则更注重词对文档的区分能力，两种算法各有不足之\n\n处。假设有如下文档。\n\n文档中“孩子们”“快乐”“都是”“他们”“大山”几个词出现的次数都是2,文档总 词数是60。由式(6-2)可知，这几个词语的TF 值都为0.033,但实际上在这段文本中，“孩\n\n子们”“快乐”“大山”这3个词语更为重要。\n\n同样地，假设文档集共有2000篇文档，出现“孩子们”“快乐”“都是”“他们”“大山” 这几个词的文档数分别为60、30、250、200、20,那么每个词的IDF 值分别为1.516、1.810、  0.901、0.998、1.979。由此可知，“大山”“孩子们”“快乐”比较重要，而“都是”“他们”\n\n这类文档中常见的词语，其IDF 值较低。\n\n综合权衡词频、逆文档频率两个方面衡量词的重要程度， TF-IDF 算法的计算公式如式\n\n94\n\n(6-3)所示。\n\n(6-3)\n\n根据 TF-IDF 算法的计算公式，将上述每个词语的 TF 值和 IDF 值相乘，得到5个词语 的 TF-IDF 值分别为0.0455 、0.0543 、0.027 、0.0299 、0.0594 。 因此，选取 TF-IDF 值中相 对较大的前3个关键词，即“大山”“孩子们”“快乐”作为这篇文档的关键词。通常，会\n\n将关键词按照 TF-IDF 值降序排列，然后选出较大的前几个值的关键词。\n\nTF-IDF 算法倾向于过滤常用的词语，保留相对重要的词语，它实际上只考虑了词的出 现频次、出现文档的数量这两个方面，对文本内容的利用程度较低。因此，如果利用更多 的信息进行关键词提取，会对提升关键词提取的效果有很大帮助，如考虑每个词的词性、 词的位置和出现场合等。当考虑词的词性时，可以对名词赋予较高的权重，名词往往含有  更多的关键信息。当考虑词的位置时，可以对文本的起始和末尾位置的词赋予较高的权重， 始末位置的词往往更为重要。在实际应用中，可以结合应用情况，对算法进行适当的调整，\n\n以达到更好的提取效果。\n\n6.2.2       TextRank     算 法\n\nTextRank 算法是一种基于图的文本排序算法，它可以用于自动摘要和提取关键词。与 TF-IDF 算法相比，TextRank 算法不同的地方在于，它不需要依靠现有的文档集提取关键词， 只需利用局部词汇之间的关系对后续关键词进行排序，随后从文本中提取词或句子，从而\n\n实现提取关键词和自动摘要。TextRank 算法的基本思想来自PageRank 算法。\n\n第⑥章 关键词提取\n\n1.PageRank   算法\n\nPageRank 算法是1997年由拉里·佩奇和谢尔盖·布林构建早期的搜索引擎时提出的 链接分析算法。最早的搜索引擎是人工对网页进行分类，从而将不同质量的网站区别开。 随着互联网的发展，人工分类的方法逐渐无法处理与日俱增的网站。搜索引擎通过计算用 户查询关键词和网页内容的相关性的方法给出搜索结果，但这种方法有其局限性。因此， 拉里·佩奇和谢尔盖·布林开始研究网页排序问题，PageRank 算法由此诞生。该算法是标 识网页重要性的一种网页排名算法，也是衡量一个站点好坏的标准。使用PageRank 算法计\n\n算网页的重要性，可以使更重要的网页在搜索结果中靠前显示。\n\n将每一个网页视为一个节点，将网页之间的链接看作有向边，网页之间就构成一张有\n\n向图，即网页链接图，如图6-1所示。网页的得分越高，该网页的重要程度就越高。\n\n0.42\n\nA\n\n0.86\n\n0.42\n\n0.3\n\nB\n\n0.6\n\n0.3\n\n图6 - 1 网页链接图\n\n图6-1所示的网页A 自身网页得分为0.86,分别被网页C 和网页D 链接到，那么网页 A 会将其得分平均地贡献出去，网页C 和 D 各得分0.42。网页B 自身得分为0.6,同样将 自身得分平均地贡献给链接到它的网页，这时网页C 和网页D 各获得0.3分，将网页C 和 网页D 各自的总得分作为其网页得分。网页C 有两个输出链接，网页D 有3个输出链接，\n\n按照链接的个数再次平均地将得分分配给链接到它们的网页，以此类推。\n\n在获取和图6-1类似的网页链接图时，通常会遇到以下两个问题。\n\n(1)上一个网页的初始得分未知。在计算网页得分时需要得知上一个链接网页的得分， 这时就需要对该方法进行调整。首先，将所有网页的初始得分设置为1,然后以迭代的方\n\n式求得每个网页的分数，或控制最大迭代次数，最终得到网页的总得分。\n\n(2)孤立网页没有得分。在计算过程中会遇到一些孤立网页，它没有输入链接和输出 链接，网页得分将会被计算为0。这时需要对 PageRank 算法进行调整，加入阻尼系数，使\n\n得孤立网页也有网页得分。\n\nPageRank 算法的基本思想包括两方面内容。一方面， 一个网页被越多其他的网页链接， 说明这个网页越重要。另一方面， 一个网页被越高权值的网页链接，说明这个网页越重要。\n\n根据 PageRank算法的基本思想，计算一个网页的PageRank 值的公式如式(6-4)所示。\n\n(6-4)\n\n其中，S(V)为网页V;的得分， V;为链接到网页V的网页， In(V)为网页V;的入链集合，\n\nOut(V)为网页V,的出链集合， |Out(V))是出链网页的数量，d 为阻尼系数(表示某一网页\n\n95\n\nPython 中文自然语言处理基础与实战\n\n链接到其他任意网页的概率，取值范围为0～1)。\n\n在图6-1 中，假设V,是指网页C,  那么V;是指网页A 和网页B 。每一个网页将自身所 有入链得分加起来，就是网页自身的得分，再将自身的得分平均地分配给每一个链接它的\n\n网页。\n\n2.TextRank  算法详解\n\nTextRank 算法通过将文本切分成若干个词或句子建立图模型", "metadata": {}}, {"content": "，d 为阻尼系数(表示某一网页\n\n95\n\nPython 中文自然语言处理基础与实战\n\n链接到其他任意网页的概率，取值范围为0～1)。\n\n在图6-1 中，假设V,是指网页C,  那么V;是指网页A 和网页B 。每一个网页将自身所 有入链得分加起来，就是网页自身的得分，再将自身的得分平均地分配给每一个链接它的\n\n网页。\n\n2.TextRank  算法详解\n\nTextRank 算法通过将文本切分成若干个词或句子建立图模型，采取投票的方式对文档 中的重要成分进行排序，进而实现关键词提取。TextRank  算法与 PageRank 算法类似，不 同之处在于 TextRank算法在实现关键词提取时，需要考虑链接词的重要性和词之间的相似\n\n性，它将构成一个加权图，而 PageRank 算法则是构成有向无权图。\n\nTextRank 算法在计算每个词的链接词得分时，通过赋予不同权重的方式分配得分，不 再采取 PageRank 算法平均分配得分的方式，并且 TextRank 算法默认所有词之间都存在链 接关系。两个词之间的权重越大，则相似度越高。用0,表示权重，其他部分与 PageRank\n\n算法类似。TextRank 关键词提取算法公式如式(6-5)所示。\n\n                (6-5)\n\n其中， WS(V)为词V,的得分， V;为链接到V;的词， In(V)为词V,的入链集合， Out(V₁)为 词V,的出链集合， |Out(V,)|是出链的数量，d 为阻尼系数(表示某个词链接到其他任意词\n\n的概率，取值范围为0～1)。\n\nTextRank 关键词提取算法的实现步骤如下。\n\n①将给定的文本内容T 依照完整句子切分开，即T={S,S₂,…,S}。\n\n②对每个句子 S;(S;∈T),   进行分词和词性标注处理，并过滤掉停用词，只保留特\n\n定词性的词，如名词、动词、形容词。记S,={tt,₂,…,t,,},          其 中t,   是保留后的候选关\n\n键词。\n\n③构建候选关键词图 G=(V,E), 其中V为节点集，由上一步生成的候选关键词组成， 然后采用共现关系构造任意两节点之间的边，两个节点之间存在边当且仅当候选关键词在\n\n长度为k 的窗口中时共现 (k  表示窗口大小，即最多共现k 个词)。\n\n④根据式(6-5)初始化权重o,,     迭代计算各个节点的权重，直到收敛为止。\n\n⑤对节点的权重进行降序排序，得到最重要的前n 个候选关键词。\n\n⑥查看这n 个候选关键词是否有相邻出现的情况，相邻出现则组合成多词关键词，否\n\n则单独成关键词。\n\n6.2.3      LSA  与 LDA 算 法\n\nTF-IDF 算法和 TextRank 算法各有所长，但在某些场合中无法提取文本中的语义信 息。例如， 一篇文章讲各式各样的水果及其功效，当水果这一关键词没有直接出现在文 本中时，这两种算法都无法准确提取到水果这一关键词。主题模型算法可以有效地解决\n\n这个问题。\n\n96\n\n第⑥章  关键词提取\n\n1. 主题模型算法\n\n主题模型 (Topic    Model) 算法是在大量文档中发现潜在主题的一种统计模型。直观地 看，如果一篇文章有一个中心思想，那么一些特定词语会更频繁地出现。例如， 一篇文章 在讲汽车，那么“汽车”和“驾驶”等词出现的频率会高些。 一篇文章通常包含多个主题， 而且每个主题所占比例各不相同。因此，如果一篇文章10%的内容和“汽车”有关，90% 的内容和“驾驶”有关，那么和“驾驶”相关的关键词出现的次数大概会是和“汽车”相\n\n关的关键词出现次数的9倍。主题模型算法试图用数学框架体现文档的这种特点。\n\n主题模型算法认为文档是由主题组成的，而主题是词的一个概率分布，即每个词都是 通过“文档以一定的概率选择某个主题，再从这个主题中以一定的概率选择某个词”这样\n\n的过程得到的。\n\n主题模型算法能自动分析每个文档，统计文档内的词语，根据统计的信息判断当前文 档含有哪些主题，以及每个主题所占的比例各为多少。常见的主题模型算法主要有潜在语 义分析 (Latent    Semantic    Analysis,LSA)、概率潜在语义分析(Probabilistic   Latent   Semantic Analysis,PLSA) 、 潜在狄利克雷分布 (Latent    Dirichlet     Allocation,LDA), 以及基于深度学\n\n习的lda2vec 等。所有主题模型算法都基于以下两个相同的基本假设。\n\n(1)每个文档包含多个主题。\n\n(2)每个主题包含多个词。\n\nLSA 和潜在语义索引(Latent    Semantic    Index,LSI) 都是对文档的潜在语义进行分析， 但是 LSI 在分析后，还会利用分析的结果建立相关的索引。二者通常被认为是同一种算法，\n\n只是应用的场景不同。\n\n2.LSA   算法\n\nLSA 算法是主题建模算法的基础技术之一，其核心思想是将所拥有的词语-文档矩阵\n\n分解成相互独立的词语-主题矩阵和主题-文档矩阵。\n\n(1)词语-文档矩阵。LSA 算法使用向量表示词语和文档，词袋(Bag   of   Words,BOW) 模型是文本向量化的最简单的模型。BOW 模型就是将一个文本中的所有词语装进一个袋子 里，不考虑其词法和语序的问题，每个词语都是独立的，对每个词语都进行统计，同时计\n\n算每个词语出现的次数。\n\nBOW 模型首先对文本进行分词，然后统计每个词在文档中出现的次数。例如，有两个\n\n短文本，通过分词后得到如下结果。\n\n将这两个短文本中的所有词语装进一个袋子里，构成一个 BOW 。BOW   中 包 含 了 7 个 不重复的词{张三，喜欢，外出，旅行，李四，也，看电影}。 BOW  模型规定每个文本向量长度为词\n\n袋中词的个数，文本向量的分量对应词袋中词出现的次数。\n\n例如，在第一个短文本中，“喜欢”“外出”“旅行”3个词语出现了两次，“张三”“李 四”“也”3个词语出现了一次，“看电影”一词则没有出现。在BOW 中对应的位置标上词 的出现次数，得到第一个短文本对应的向量为[1,2,2,2,1,1,0]。在第二个短文本中，“张\n\n三”“喜欢”“看电影”3个词各出现了一次，其他词语出现次数都为0,得到第二个短文本\n\n97\n\nPython 中文自然语言处理基础与实战\n\n对应的向量为[1,1,0,0,0,0,1]。\n\n需要注意的是，因为 BOW  模型不考虑词语的顺序，所以得到的向量不会保存原始句\n\n子中词的顺序。\n\n每个文本都可以利用 BOW 模型得到一个向量，将所有文本的向量合在一起即可得到 一个词语-文档矩阵。例如，词语-文档矩阵示例如表6-1 所示，其中共有4个词和4个文\n\n档，词语-文档矩阵的每一列为文档(文本)的向量，每一行表示词在文档中出现的次数。\n\n表6 - 1 词语-文档矩阵示例\n\n文档1 文档2 文档3 文档4 词语1 0 1 1 2 词语2 1 1 0 0 词语3 1 2 0 2 词语4 0 1 1 1\n\n(2)矩阵奇异值分解。如果在语料库中给出m个词和n 个文档，则可以构造一个m×n   阶矩阵A,    其中每行代表一个词，每列代表一个文档。当拥有词语-文档矩阵A 后，即可 开始思考文本潜在的主题。但是词语-文档矩阵A 极有可能非常稀疏且噪声很大，在很多维 度上非常冗余。因此，为了找出能够捕捉词和文档关系的少数潜在主题，需要降低矩阵A 的 维度。线性代数中的一种奇异值分解 (Singular    Value     Decomposition,SVD) 技术可以分解\n\n矩阵奇异值从而达到降维的目的。\n\n设矩阵A 为一个m×n  阶的矩阵，其中的元素全部属于实数域或复数域，那么矩阵A 的\n\nSVD   如式(6-6)所示。\n\nA=UZVT                                                                  (6-6) 其中", "metadata": {}}, {"content": "，在很多维 度上非常冗余。因此，为了找出能够捕捉词和文档关系的少数潜在主题，需要降低矩阵A 的 维度。线性代数中的一种奇异值分解 (Singular    Value     Decomposition,SVD) 技术可以分解\n\n矩阵奇异值从而达到降维的目的。\n\n设矩阵A 为一个m×n  阶的矩阵，其中的元素全部属于实数域或复数域，那么矩阵A 的\n\nSVD   如式(6-6)所示。\n\nA=UZVT                                                                  (6-6) 其中， U 是 一 个m×m  阶正交矩阵(行向量和列向量皆为正交的单位向量),乙是m×n  阶 半正定对角矩阵(对角元素为非负的对角阵),乙主对角线上的每个元素称为奇异值，yT 是\n\nn×n  阶正交矩阵， U 的列向量称为左奇异向量， yT 的列向量称为右奇异向量，并且满足\n\nUTU=I  和VTy=I。\n\nSVD 有如下性质。\n\n① 一 个m×n 阶矩阵至多有P 个不同的奇异值， p=min(m,n)。\n\n② 奇异值包含着矩阵中的重要信息，值越大表示越重要。\n\n词语-文档矩阵A 经过SVD 后的U 和vT 的维度较大，需要进行降维，原因有以下几点。\n\n① 原始的词语-文档矩阵维度太大且过于稀疏，计算复杂度过高，无法准确地反映每\n\n个词是否出现在某些文档之中。\n\n② 矩阵降维可以对矩阵数据去噪，得到重要特征。\n\n③ 矩阵降维可以降低同义词和多义词的影响，减少数据冗余。\n\n截断奇异值分解，就是奇异值从大到小排序，取前r 个非零奇异值对应的奇异向量代\n\n表矩阵A 的主要特征，如式(6-7)所示。\n\nA≈Uy ExV                             (6-7)\n\n98\n\n第 6 章  关键词提取\n\n左奇异向量矩阵U 代表词的部分特征，矩阵中的每一列由m个词按照一定的权重组合 而来，它们相互独立并且各代表一个潜在语义，这r 个潜在语义共同构成一个语义空间。 中间的奇异值矩阵乙包含词和文档的重要程度的信息，数值越大，重要程度越高。右奇异\n\n向量矩阵yT 则代表文档的部分特征。\n\n利用 SVD 可以得到文档、词语与主题、语义之间的相关性。通过这些文档向量和词语向\n\n量，应用余弦相似度等度量评估不同文档的相似度、不同词语的相似度和词语与文档的相似度。\n\n(3)LSA    关键词提取算法的具体步骤如下。\n\n① 利用BOW 模型将文档集中的每个文档表示为向量。\n\n② 将文档集中的所有词语和文档向量构成一个m×n 阶的词语-文档矩阵。其中，文档\n\n集中的每一篇文档为矩阵的列，文档集中的所有词语为矩阵的行。\n\n③ 采用 SVD,  将该矩阵分解为3个矩阵，分别是mxr  阶左奇异矩阵、 rxr   阶奇异值\n\n对角矩阵、 r×n  阶右奇异矩阵。\n\n④ 根据 SVD 的结果，取前k 个非零奇异值对应的向量用于代表该矩阵的主要特征， 构建潜在语义空间，计算每个词语和每个文档之间的相似度，相似度最高的词语将作为该\n\n文档的关键词。\n\n下面举例说明 LSA关键词提取算法的实现过程。假设有一个词语-文档矩阵，如表6-2 所示，其中共有城市、密集型、必须、我国、行业、轨道交通、运营、里程8个词， d1~ d7 分别代表7个文档。矩阵中每一行代表词在对应文档中出现的次数，每一列表示每一个\n\n文档包含了哪些词。\n\n表6-2  词语-文档矩阵\n\n词语 文档 d1 d2 d3 d4 d5 d6 0/ 城市 0 0 1 1 0 0 1 密集型 0 0 1 1 0 0 0 必须 0 0 1 2 0 0 0 我国 1 0 0 0 0 0 1 行业 0 0 1 1 1 0 轨道交通 1 0 1 1 0 0 1 运营 0 1 0 0 1 0 0 里程 0 2 0 0 0 0 0\n\n对词语-文档矩阵进行 SVD,  选取奇异值最大的3项，得到分解后的3个矩阵，分别是\n\n左奇异矩阵(如表6-3所示)、奇异值矩阵(如表6-4所示)和右奇异矩阵(如表6-5所示)。\n\n表6- 3 左奇异矩阵\n\n城市 -0.42 -0.35 -0.55 密集型 0.05 0.01 0.02 必须 -0.18 0.17 0.3\n\n99\n\nPython 中文自然语言处理基础与实战\n\n续表\n\n我国 -0.11 -0.2 -0.41 行业 0.83 0.03 -0.44 轨道交通 -0.18 0.09 -0.27 运营 -0.22 0.68 -0.42 里程 -0.01 0.58 0.01\n\n表6 - 4  奇异值矩阵    \n\n3.75 0 0 0 2.3 0 0 0 1.94\n\n表6 - 5 右奇异矩阵\n\nd1 d2 d3 d4 d5 d6 d7 -0.15 -0.01 -0.59 -0.73 -0.12 -0.11 -0.27 0.06 -0.95 0.01 0.02 -0.29 -0.06 0.08 -0.59 -0.17 0.09 0.24 0.21 0.20 -0.69\n\n观察表6-3 和表6-5 可以发现，每个词语和每个文档都可以用一个三维向量表示，如 “城市”一词可用向量表示为(-0.42,-0.35,-0.55),文档d1 可用向量表示为(-0.15,0.06, \t-0.59)。这说明， SVD  将每个词和每个文档都映射到一个三维空间上。可以将这个三维空\n\n间看作一个三维语义空间，3个维度的潜在语义可以表示为如下形式。\n\n第一维度： -0.42×城市+0.05×密集型-0.18×必须-0.11×我国+0.83×行业-0.18×轨道交通-\n\n0.22×运营-0.01×里程。\n\n第二维度： -0.35×城市+0.01×密集型+0.17×必须-0.2×我国+0.03×行业+0.09×轨道交通+\n\n0.68×运营+0.58×里程。\n\n第三维度： -0.55×城市+0.02×密集型+0.3×必须-0.41×我国-0.44×行业-0.27×轨道交通-\n\n0.42×运营+0.01×里程。\n\n当词与文档在同一个空间上的时候，可以获得两个比较重要的信息。 一是当两个词或 两个文档在空间上的距离比较近的时候，可以认为两个词是近义词或两个文档有较高的相 似度；二是当一个词与某个文档在空间上的距离比较近的时候，可以认为这个词是文档的\n\n关键词。\n\nLSA 算法利用 SVD 将词语、文档映射到更低维度的空间。低维空间去除了部分噪声， 大大降低了计算代价，也更容易发现同义词和相似的主题，更好地剖析词语和文档中的潜\n\n在语义。然而， LSA 算法仍存在许多不足之处。\n\n① SVD 计算复杂度高，尤其是对于文本处理， SVD 用于高维度矩阵时效率较低。当\n\n新文档进入特征空间时，需要重新训练模型。\n\n② 没有解决多义词的问题，每一个词语仅对应映射空间中的一个点", "metadata": {}}, {"content": "， 大大降低了计算代价，也更容易发现同义词和相似的主题，更好地剖析词语和文档中的潜\n\n在语义。然而， LSA 算法仍存在许多不足之处。\n\n① SVD 计算复杂度高，尤其是对于文本处理， SVD 用于高维度矩阵时效率较低。当\n\n新文档进入特征空间时，需要重新训练模型。\n\n② 没有解决多义词的问题，每一个词语仅对应映射空间中的一个点，多个含义的词语\n\n在映射空间中没有区分开。\n\n100\n\n第 6 章 关键词提取\n\n③LSA   受 BOW 模型的影响，会忽略文档中句子的先后顺序。\n\n④LSA   缺乏严谨的统计基础，难以直观进行解释。\n\n3.LDA   算 法\n\n由于LSA 算法存在诸多不足，于是产生了PLSA 算法，它是一个概率模型算法，采用 更符合文本特性的多项式分布和最大期望(Expectation-Maximization,EM)  算法拟合概率 分布信息，使模型中变量的概率分布有更好的解释。但 PLSA 算法仍然不够完善，它只能 生成所在文档集的文档的模型。当使用 EM 算法进行反复迭代时，计算量会很大。当文档 和词语数量增多时，模型训练参数的值也会随之线性增加，这容易导致过度拟合。因此，\n\nLDA 算法在对 PLSA 算法修改的基础上被提出。\n\n(1)LDA    算法是应用比较广泛的一种主题模型算法，包含词语、主题和文档3层结构。 LDA 模型假定词语之间没有顺序，所有的词语都无序地放在一个袋子里，并且认为一个文\n\n档可以有多个主题，每个主题对应有不同的词语。\n\n假设语料库包含m 个 词 语 ，n 个文档，每个文档包含K 个主题，则 LDA  算法可以表\n\n101\n\n示为式(6-8)。\n\n(6-8)\n\n其中w; 、d,分别表示词语和文档，t₄ 代表第k个主题。\n\n假设P(w|d) 、P(w|t)        和 P(t|d)    分别称为词语-文档、词语-主题和主题-文档概率分布\n\n矩阵，则式(6-8)中的3个概率分布矩阵分别如式(6-9)、式(6-10)、式(6-11)所示。\n\n这样， LDA  算法的矩阵形式可以表示为式(6-12)。\n\nP(w|d)=P(w|t)P(t|d)\n\n(6-9)\n\n(6-10)\n\n(6-11)\n\n(6-12)\n\nLDA 算法对一篇文档进行关键词提取时，能够得到每个主题，生成每个词的概率，然\n\n后将每个主题中概率最大的前k 个词取出并作为该文档的关键词。\n\n(2)LDA      算法的概率分布可以描述为已知 P(w|d)   概率分布矩阵，求概率分布矩阵\n\nP(w|t)   和 P(t|d) 。  假设算法的主题的先验分布和主题中词语的先验分布如下。\n\nPython 中文自然语言处理基础与实战\n\n① 假设文档主题的先验分布为 Dirichlet分布，即对于任一文档i, 其主题分布0,如式 (6-13)所示。\n\nδ~Dir(a),i=1,2,…,n                                                    (6-13)\n\n其中，α为文档主题的超参数，是一个k 维向量。\n\n② 假设主题中词语的先验分布为 Dirichlet分布，即对于第k 个主题，其词语分布可如\n\n式(6-14)所示。\n\nφ~Dir(β),k=1,2,…,K                                                  (6-14)\n\n其中，卢为分布的超参数，是一个m 维向量。\n\n对于数据库中任一文档d,中的第 j个词语，由主题分布可以得到它的主题编号z₂, 的\n\n分布为多项式分布，如式(6-15)所示。\n\nzj~multi(θ)                                                              (6-15)\n\n对于主题编号z;, 可以得到词w,,的概率分布为多项式分布表，如式(6-16)所示。\n\nW,y~multi(φ)                                                              (6-16)\n\n(3)LDA   算法生成文档的步骤。LDA 算法是一种文档生成模型算法，它认为一篇文档\n\n有多个主题，每个主题对应着不同的词。 一篇文档的生成过程：首先以一定的概率选择某 个主题，然后在这个主题下以一定的概率选出某一个词，这样就生成了这篇文档的第一个 词；不断重复这个过程，就生成了整篇文章。这里假定词与词之间是没有顺序的，即所有 词无序地堆放在一个大袋子中(称为词袋),这种方式使算法相对简化一些。在 LDA 算法\n\n中， 一篇文档生成的步骤如下。\n\n① 按照先验概率 p(d,)选择一篇文档d,。\n\n② 人为设置超参数α,获得主题分布B。\n\n设置文档d,中对应主题每个词的个数，如文档d, 中有5个词对应主题1,有7个词对 应主题2,……,有4个词对应主题K,   得到ã=(5,7,…,4)。由0,～Dir(a),    采样生成文\n\n档d, 的主题分布O=(P,P₂,…,P)。\n\n③ 获取主题索引z₁j 。由z~multi(θ),         采样生成文档d  第j 个词的主题索引z;,。\n\n④ 人为设置超参数声，获取隐含参数φ。设置主题z;, 产生字典中各个词的数量β。\n\n由 φ ~Dir(β),    采样生成主题z,, 对应词的分布。\n\n⑤ 获取文档i 第j 个词w,, 的索引。由w,;~multi(q₂),      采样生成W;,。\n\n(4)Gibbs    采样算法。在 LDA 算法生成文档的步骤中，主题z;, 对应词的分布和文档i\n\n第j 个 词w,, 的索引都是通过采样生成的。LDA 算法使用的是Gibbs 采样算法。\n\n在 LDA 算法中，超参数α、β是已知的先验输入，算法的目标是通过Gibbs 采样算法\n\n得到各个z, 、W.,  对应的整体三、 w 的概率分布，即文档主题的分布和主题词的分布。\n\nGibbs 采样算法每次选取概率向量的一个维度，给定其他维度的变量值，采样当前维度\n\n的值。不断迭代，直到收敛输出待估计的参数。\n\nGibbs 采样算法在LDA 算法中的抽样过程如下。\n\n首先从文本集合中抽取一个词标记，在其他所有词标记和主题给定的条件下，将选\n\n102\n\n第 6 章 关键词提取\n\n定的词分配给一个主题的概率为p(z₁=j|wa,,z_y,α,β)。然后从中抽取一个主题z;取代当\n\n前词的主题，不断循环这个过程，最终收敛于一个不变点。具体的计算公式如式(6-17)\n\n103\n\n所示。\n\n(6-17)\n\n其中，n), 表示单词w被分配给主题j 而没有包含当前主题i的次数， 表示在文档d 中\n\n分配给主题j 的词而没有包含当前主题i 的次数。\n\n如果通过采样得到了所有词的主题，那么统计所有词的主题计数，即可得到各个主题\n\n的词分布。接着统计各个文档对应词的主题计数，即可得到各个文档的主题分布。\n\n6.3    任务：自动提取文本关键词\n\n本节根据算法原理自定义 TF-IDF 、TextRank 和 LSA 三种算法的函数，并通过实例完 成关键词自动提取。关键词提取流程主要包括数据预处理、算法实现和结果分析等步骤。\n\n在提取关键词之前，需要先输入准备好的文档，如代码6-1所示。\n\nimport\n\nimport\n\nimport\n\nimport\n\nimport\n\nimport\n\n1It\n\n代码6 - 1  输入准备好的文档\n\njieba\n\njieba.posseg\n\nnumpy     as     np\n\npandas     as     pd\n\nmath\n\noperator\n\n提供Python    内置的部分操作符函数，这里主要应用于序列操作\n\n用于对大型语料库进行主题建模，支持TF- IDF、LSA 和 LDA 等多种主题模型算法", "metadata": {}}, {"content": "，需要先输入准备好的文档，如代码6-1所示。\n\nimport\n\nimport\n\nimport\n\nimport\n\nimport\n\nimport\n\n1It\n\n代码6 - 1  输入准备好的文档\n\njieba\n\njieba.posseg\n\nnumpy     as     np\n\npandas     as     pd\n\nmath\n\noperator\n\n提供Python    内置的部分操作符函数，这里主要应用于序列操作\n\n用于对大型语料库进行主题建模，支持TF- IDF、LSA 和 LDA 等多种主题模型算法，提供了\n\n诸如相似度计算、信息检索等一些常用任务的API  接口\n\n1II\n\nfrom         gensim         import         corpora,models\n\ntext      ='广州地铁集团工会主席在开幕式上表示，我国城市轨道!\\\n\n交通事业蓬勃发展，城轨线路运营里程不断增长，目前，全国城市轨道交通线网总里程’\\ 接近5000公里，每天客运量超过5000万人次。城市轨道交通是高新技术密集型行业，!\\\n\n'几十个专业纷繁复杂，几十万台(套)设备必须安全可靠，线网调度必须联动周密，!\n\n列车运行必须精准分秒不差。城市轨道交通又是人员密集型行业，产业工人素质的好坏、!\\\n\n'高低，直接与人民生命安全息息相关。本届“国赛”选取的列车司机和行车值班员，!\\\n\n'正是行业安全运营的核心、关键工种。开展职业技能大赛的目的，就是弘扬!\\\n\n!“工匠精神”,在行业内形成“比、学、赶、帮、超”的良好氛围，在校园里掀起!\\\n\n“学本领、争上游”的学习热潮，共同为我国城市轨道交通的高质量发展’\\\n\n做出应有的贡献。\n\n1. 文本预处理\n\n输入文档后，需要加载停用词，并对当前文档进行分词和词性标注，过滤一些对提取 关键词帮助不大的词性。本节只将名词作为候选关键词，在过滤词性时只留下名词，并且\n\n删除长度小于或等于1的无意义词语。文本预处理的步骤如下。\n\nPython 中文自然语言处理基础与实战\n\n(1)加载停用词文件 stopword.txt并按行读取文件中的停用词，对文本中多余的换行符\n\n进行替换，最终获取停用词列表。其中，自定义的 Stop_words 函数用于获取停用词列表。\n\n(2)对当前文档去停用词。自定义的 Filter_word 函数用于对当前文档进行处理，输入 参数为当前文档内容。处理后的文档存放在 filter_word 变量中，它是一个包含多个字符串\n\n的列表。\n\n(3)对文档集 corpus.txt 去停用词。文档集选取国内2012年6月—2012年7月期间， 搜狐新闻中国际、体育、社会、娱乐等18个频道的新闻内容，其中包含多行文本内容，读\n\n取时以列表的形式追加，每个文档以字符串的形式存放在列表中。\n\n(4)自定义的 Filter_words 函数用于对文档集进行处理，输入参数是文档集路径。处理 后的文档集存放在 document  变量中，它是一个包含多个列表的列表，相当于将多个\n\nfilter_word 变量组合为一个列表。\n\n文本预处理的具体实现过程如代码6-2所示。其中， startswith  函数表示查看字符串第 一个字符是否是某个字符。在代码6-2 中借助 startswith 函数过滤词性，其中参数“n”表\n\n示词性被标注为名词。\n\n代码6-2  文本预处理的具体实现过程\n\n#加载停用词\n\ndef    Stop_words():\n\nstopword            =[]\n\ndata          =[]\n\nf                                              =open('../data/stopword.txt',encoding='utf8')\n\nfor  line  in  f.readlines():\n\nfor          i   d a.aapta nd  (line)\n\noutput                                      =i.replace('\\n','')\n\nstopword.append(output)\n\nreturn  stopword\n\n# 采 用jieba     进行词性标注，对当前文档过滤词性和停用词\n\ndef                  Filter_word(text):\n\nfilter_word             =[]\n\nstopword  =Stop_words()\n\ntext                           =jieba.posseg.cut(text)\n\nfor          word,flag          in          text:\n\nif                      flag.startswith('n')is                      False:\n\ncontinue\n\nif        not        word        in        stopword        and        len(word)>1:\n\nfilter_word.append(word)\n\nreturn filter word\n\n#加载文档集，对文档集过滤词性和停用词\n\ndef                     Filter_words(data_path                      ='../data/corpus.txt'):\n\n104\n\ndocument              =[]\n\nfor               line\n\nsegment\n\nfilter_words stopword\n\nin               open(data_path,'r',encoding='utf8'):\n\n=jieba.posseg.cut(line.strip())\n\n=[]\n\n=Stop_words()\n\n第 6 章  关键词提取\n\n2.TF-IDF  算法\n\n自定义的 TF-IDF算法函数名为tfidf,    其算法实现包括以下3个步骤。\n\n( 1 ) 对TF 值进行统计。调用自定义的 Filter_word 函数处理当前文档，统计当前文档\n\n中每个词的 TF 值。\n\n( 2 ) 对IDF 值进行统计。调用自定义的 Filter_words函数处理文档集，统计IDF 值 。\n\n( 3 ) 对 TF 值和 IDF 值进行统计，并将二者结果相乘，得到 TF-IDF值。\n\nTF-IDF 算法的具体实现过程如代码6-3所示。其中 set 函数用于去重，使集合中的每\n\n个元素都不重复； operatoritemgetter(1) 表示获取序列的第一个域的值，获取降序列表中的\n\n105\n\n关键词。\n\n#TF-IDF    算法\n\ndef              tf  idf():\n\n#统计TF 值\n\ntf  dict ={}\n\nfilter_word\n\nfor       word       in\n\nif     word\n\n代码6-3 TF-IDF  算法的具体实现过程\n\n=Filter_word(text)\n\nfilter_word:\n\nnot      in      tf  dict:\n\ntf  dict[word]=1\n\nelse:tf  dict[word]+=1\n\nfor     word     in     tf  dict:\n\ntf  dict[word]=tf  dict[word]/len(text)\n\n#统计IDF值\n\nidf  dict\n\ndocument   doc_total\n\n={}\n\n=Filter_words()\n\n=len(document)\n\nfor doc for in           document: word          in          set(doc):\n\nif      word      not      in      idf  dict:\n\nidf  dict[word]=1\n\nelse:\n\nidf  dict[word]+=1\n\nfor    word    in    idf  dict:\n\nidf dict[word]=math.1og(doc  total\n\n#计算T-IDF值\n\ntf  idf  dict               ={}\n\nfor       word       in       filter_word:\n\nif   word   not    in    idf  dict:\n\nidf dict[word]=0\n\n/(idf  dict[word]+1))\n\nPython 中文自然语言处理基础与实战\n\n3.TextRank  算法\n\n自定义的 TextRank  算法函数名为 TextRank,    其算法实现包括以下3个步骤。\n\n(1)构建每个节点对应的窗口集合，当不同窗口中出现相同的词语时，相互连接形\n\n成边。\n\n(2)构建以边相连的关系矩阵，对矩阵进行归一化。\n\n(3)根据 TextRank算法公式计算对应的 TextRank 值，提取关键词。\n\nTextRank  算法的具体实现过程如代码6-4所示。其中，窗口数 window 设 置 为 3 ,win   dict\n\n表示所有节点对应的窗口词汇，迭代次数iter num设置为700。\n\n代码6 - 4 TextRank 算法的具体实现过程\n\ndef                       TextRank():\n\nnow   dict  {}\n\nfilter_word                              =Filter_word(text)\n\nlength       =len(filter_word)\n\n#构建每个节点的窗口集合\n\nfor        word        in        filter_word:\n\nindex                            =filter_word.index(word)\n\n#设置窗口左、右边界", "metadata": {}}, {"content": "，迭代次数iter num设置为700。\n\n代码6 - 4 TextRank 算法的具体实现过程\n\ndef                       TextRank():\n\nnow   dict  {}\n\nfilter_word                              =Filter_word(text)\n\nlength       =len(filter_word)\n\n#构建每个节点的窗口集合\n\nfor        word        in        filter_word:\n\nindex                            =filter_word.index(word)\n\n#设置窗口左、右边界，控制边界范围\n\nif      word      not      in      win_dict:\n\nht     ex     dow   +1\n\nif leflte0\n\nwords                 =set()\n\nfor           i            in           range(left,right):\n\nif            i            ==index:\n\ncontinue\n\nwords.add(filter_word[i])\n\nwin_dict[word]=words\n\n#构建以边相连的关系矩阵\n\n    =len(set(fi rt(se_ord))\n\nmatrix                                            =pd.DataFrame(np.zeros([lengths,lengths]))\n\nfor         v ue _ :n  dict[word]:\n\nindex1   =word_dict.index(word)\n\nindex2     =word_dict.index(value)\n\nmatrix.iloc[index1,index2]=1\n\n106\n\n第 6 章  关键词提取\n\nmatrix.iloc[index2,index1]=1\n\n  atrix.shape[1]\n\nrows =matrix.shape[0]\n\n#归一化矩阵\n\nfor       j       in       range(cols):\n\nfor         i         in         range(rows):\n\nmatri=summ      +=matrix.iloc[i,j]\n\n#.根据公式计算TextRank  值\n\nd=0.85\n\niter_num          =700\n\nnk  =np.es([lengths,1])\n\nfor       i       in       range(iter_num):\n\ntextrank                                  =(1-d)+d*np.dot(matrix,textrank)\n\n#将词语和 TextRank    值一一对应\n\nfor  i  in  range(len(textrank)):\n\nd_ted]=textrank[i,0]\n\nprint(¹------------------------------')\n\ntk(' inrd_textrank.items(),key=operator.itemgetter(1),\n\nreverse=True)[:keyword]:\n\nprint(key   +'/',end='')\n\n4.LSI  算法\n\n由于 gensim 库中只定义了LSI 算法，而 LSA 算法和LSI 算法原理基本一致，因此 这里用LSI 算法替代LSA 算法。自定义的LSI 算法函数名为lsi,  其算法实现包括以下3\n\n个步骤。\n\n(1)构建基于文档集的词语空间。使用 BOW  模型对每篇文档进行向量化，得到每一 篇文档对应的稀疏向量。向量中包括向量的id 和词频，其中id 表示文档中每一个词语的索\n\n引，词频表示词语出现在文档中的次数，都以数字表示。\n\n( 2 ) 构 建 TF-IDF  模型，在此基础上加入向量化处理后的文档集语料 corpus,   结合 成为经过TF-IDF  加权的文档向量。基于SVD 建立主题模型，得到当前文档和主题之间\n\n的分布。\n\n(3)采用余弦相似度计算相似度，求得当前文档与文档中的词语的相似度，相似度最\n\n高的前10个词作为当前文档的关键词。\n\nLSI 算法的具体实现过程如代码6-5所示。为了减少数据冗余，除去相同的词语，extend\n\n函数会将文档集列表合并后去重，使文档集中所有的词语不重复。\n\n代码6-5 LSI 算法的具体实现过程\n\ndef                lsi():\n\n#主题-词语\n\ndocument\n\n107\n\nPython 中文自然语言处理基础与实战\n\n2. 需求说明\n\n实现基于TF-IDF算法的新闻文本关键词提取，利用TF-IDF算法对在实训1中经过文\n\n本预处理后的新闻文本提取关键词。\n\n3. 实现思路与步骤\n\n(1)调用Filter_word 函数统计每个词的TF 值。\n\n(2)调用Filter_words 函数统计 IDF值。\n\n(3)对 TF 值和IDF 值进行统计，得到 TF-IDF值。\n\n实训3 使用 TextRank 算法提取关键词\n\n1. 训练要点\n\n掌握 TextRank算法的具体实现过程。\n\n2. 需求说明\n\n实现基于 TextRank 算法的新闻文本关键词提取，利用 TextRank 算法对在实训1中经\n\n过文本预处理后的新闻文本提取关键词。\n\n3. 实现思路与步骤\n\n(1)构建每个节点对应的窗口集合，当不同窗口中出现相同的词语时，相互连接形成边。\n\n(2)构建以边相连的关系矩阵，对矩阵进行归一化。\n\n(3)根据 TextRank算法公式计算对应的 TextRank 值，提取关键词。\n\n实训4  使用 LSA 算法提取关键词\n\n1.  训练要点\n\n掌握 LSA算法的具体实现过程。\n\n2. 需求说明\n\n实现基于 LSA 算法的新闻文本关键词提取，利用LSA 算法对在实训1中经过文本预\n\n处理后的新闻文本提取关键词。\n\n3. 实现思路与步骤\n\n(1)构建基于文档集的词空间。\n\n(2)构建 TF-IDF 模型。\n\n(3)采用余弦相似度计算相似度，将相似度最高的前10个词作为当前文档的关键词。\n\n课后习题\n\n1. 选择题\n\n(1)要求关键词提取算法应具有的性质不包括(    )。\n\nA.  可读性      B.   高速性      C.   简洁性          D.  健壮性\n\n(2)不属于关键词提取算法的是(    )。\n\nA.TF-IDF   算法 B.TextRank    算法 C.   主题模型算法     D.  关联算法\n\n110\n\n第 6 章  关键词提取\n\n输出结果如下。\n\n基于当前文档，在3种模型中， TF-IDF  模型的结果较好，其次是 TextRank 算法，最 后是 LSI 模型。LSI  算法是较早的主题模型算法，存在诸多不足之处，因此用其提取关键 词时效果不太理想，相较之下 TF-IDF 模型和 TextRank 模型的结果会比较好。由于在提取 关键词时，有些词语在文档中出现的频次、词性等比较接近，因此出现的概率很有可能一\n\n样，多次运行程序，关键词的出现顺序会发生变化。\n\n小结\n\n本章主要介绍了关键词提取技术的3种算法。首先对关键词提取技术做了简单介绍。 其次对 TF-IDF 算法的基本原理进行了阐述，并简单举例说明。接着通过引入PageRank 算 法，给出了 TextRank 算法的基本原理。然后引入主题模型的概念，详细介绍了 LSA 算法 和 LDA 算法的流程。最后根据3种关键词提取算法原理，编写每种算法的自定义函数，并\n\n通过实例实现关键词提取。\n\n实训\n\n实 训 1 文本预处理\n\n1. 训练要点\n\n(1)掌握对文档集去停用词的方法。\n\n(2)掌握使用 Filter  words 函数对文档集进行处理的方法。\n\n2. 需求说明\n\n提取新闻文本中的关键词，用于给新闻贴标签和分类。文件 csgnews.txt  中包含一个新 闻文本，需要提取该新闻文本的关键词。在提取关键词之前，需要对 csgnews.txt 中的新闻\n\n文本进行文本预处理。\n\n3. 实现思路与步骤\n\n(1)加载停用词文件 stopword.txt。\n\n(2)对当前文档 text 去停用词。\n\n(3)对文档集 csgnews.txt 去停用词。\n\n( 4 ) 利 用 Filter_words    函数对文档集进行处理。\n\n实 训 2  使用TF-IDF  算法提取关键词\n\n1.   训练要点\n\n掌握 TF-IDF 算法的具体实现过程。\n\n109\n\n 第⑦章 文本向量化\n\n随着计算机计算能力的大幅提升，机器学习和深度学习都有了较大的发展， NLP 越来 越多地应用机器学习和深度学习工具解决问题。在这种背景下，文本向量化成为NLP 中非 常重要的内容，因为文本向量化可将文本空间映射到一个向量空间，从而使文本可计算。 本章介绍文本向量化的概念、文本离散表示的常用方法和文本分布式表示的方法", "metadata": {}}, {"content": "，机器学习和深度学习都有了较大的发展， NLP 越来 越多地应用机器学习和深度学习工具解决问题。在这种背景下，文本向量化成为NLP 中非 常重要的内容，因为文本向量化可将文本空间映射到一个向量空间，从而使文本可计算。 本章介绍文本向量化的概念、文本离散表示的常用方法和文本分布式表示的方法，并通过\n\n实例介绍文本相似度的计算方法。\n\n学习目标\n\n(1)了解文本向量化的基本概念。\n\n(2)了解文本离散表示的常用方法。\n\n(3)熟悉文本向量化模型 Word2Vec 和 Doc2Vec 的基本原理。\n\n(4)掌握 Word2Vec 和 Doc2Vec 模型的训练流程和文本相似度的计算方法。\n\n7.1   文本向量化简介\n\n文本向量化就是将文本表示成一系列能够表达文本语义的机读向量，它是文本表示的 一种重要方式。在 NLP中，文本向量化是一个重要环节，其产出向量的质量直接影响后续 模型的表现，例如，在一个文本相似度比较的任务中，可以取文本向量的余弦值作为文本 相似度，也可以将文本向量输入神经网络进行计算得到文本相似度，但是无论后续模型是\n\n怎样的，前期的文本向量化都会影响整个文本相似度比较的准确性。\n\n就像图像领域天然有着高维度和局部相关性的特性， NLP 领域也有着其自身的特性。 一是计算机任何计算的前提都是向量化，而文本难以直接被向量化；二是文本的向量化应 当尽可能地包含语言本身的信息，但是文本中存在多种语法规则和其他种类的特性，这导 致向量化比较困难；三是自然语言本身体现了人类社会的一种深层次的关系(如讽刺等语\n\n义),这种关系给向量化带来了挑战。\n\n文本向量化按照向量化的粒度可以分为以字为单位、以词为单位和以句子为单位向量 表达，可以根据不同的情景选择不同的向量表达方法和处理方式。目前，文本向量化的大 部分研究是通过以词为单位的向量化进行的。随着深度学习技术的广泛应用，基于神经网 络的文本向量化已经成为 NLP  领域的研究热点，尤其是以词为单位的向量化研究。\n\nWord2Vec 是目前的最典型的以词为单位生成词向量的工具，其特点是将所有的词向量化，\n\n第 6 章  关键词提取\n\n(3)TF-IDF     算法的主要思想是(    )。\n\nA.   字词的重要性随着它在文档中出现次数的增加而上升，随着它在语料库中出\n\n现频率的升高而下降\n\nB.   字词的重要性随着它在文档中出现次数的增加而下降，随着它在语料库中出\n\n现频率的升高而下降\n\nC.   字词的重要性随着它在文档中出现次数的增加而下降，随着它在语料库中出\n\n现频率的升高而上升\n\nD.    字词的重要性随着它在文档中出现次数的增加而上升，随着它在语料库中出\n\n现频率的升高而上升\n\n(4)关于逆文档频率说法错误的是(     )。\n\nA.   逆文档频率是一个词出现在文档集中文档频次的统计量\n\nB.   一个词在文档集中越少的文档中出现，说明这个词对文档的区分能力越强\n\nC.   一个词在文档集中越少的文档中出现，说明这个词对文档的区分能力越弱\n\n(5)一篇文章在讲各式各样的水果及其功效，当“水果”这一关键词没有直接出现在\n\n文本中时，应该使用(     )。\n\nA.TF-IDF  算法                   B.TextRank    算法\n\nC.   主题模型算法                 D.PageRank  算法\n\n2. 操作题\n\n(1)完成本章中的所有代码操作。\n\n(2)更换本章实战中的文本，利用3种算法的自定义函数实现关键词提取。\n\n( 3 ) 利 用 gensim 库中的函数提取关键词，通过实例实现关键词提取。\n\n111\n\n 第⑦章 文本向量化\n\n随着计算机计算能力的大幅提升，机器学习和深度学习都有了较大的发展，NLP 越来 越多地应用机器学习和深度学习工具解决问题。在这种背景下，文本向量化成为NLP 中非 常重要的内容，因为文本向量化可将文本空间映射到一个向量空间，从而使文本可计算。 本章介绍文本向量化的概念、文本离散表示的常用方法和文本分布式表示的方法，并通过\n\n实例介绍文本相似度的计算方法。\n\n学习目标\n\n(1)了解文本向量化的基本概念。\n\n(2)了解文本离散表示的常用方法。\n\n(3)熟悉文本向量化模型 Word2Vec 和 Doc2Vec 的基本原理。\n\n(4)掌握 Word2Vec 和 Doc2Vec 模型的训练流程和文本相似度的计算方法。\n\n7.1   文本向量化简介\n\n文本向量化就是将文本表示成一系列能够表达文本语义的机读向量，它是文本表示的 一种重要方式。在NLP 中，文本向量化是一个重要环节，其产出向量的质量直接影响后续 模型的表现，例如，在一个文本相似度比较的任务中，可以取文本向量的余弦值作为文本 相似度，也可以将文本向量输入神经网络进行计算得到文本相似度，但是无论后续模型是\n\n怎样的，前期的文本向量化都会影响整个文本相似度比较的准确性。\n\n就像图像领域天然有着高维度和局部相关性的特性， NLP 领域也有着其自身的特性。 一是计算机任何计算的前提都是向量化，而文本难以直接被向量化；二是文本的向量化应 当尽可能地包含语言本身的信息，但是文本中存在多种语法规则和其他种类的特性，这导 致向量化比较困难；三是自然语言本身体现了人类社会的一种深层次的关系(如讽刺等语\n\n义),这种关系给向量化带来了挑战。\n\n文本向量化按照向量化的粒度可以分为以字为单位、以词为单位和以句子为单位向量 表达，可以根据不同的情景选择不同的向量表达方法和处理方式。目前，文本向量化的大 部分研究是通过以词为单位的向量化进行的。随着深度学习技术的广泛应用，基于神经网 络的文本向量化已经成为 NLP  领域的研究热点，尤其是以词为单位的向量化研究。\n\nWord2Vec 是目前的最典型的以词为单位生成词向量的工具，其特点是将所有的词向量化，\n\n第⑦章  文本向量化\n\n这样词与词之间的关系即可度量，并且它们之间的联系也可挖掘。也有一部分研究将句子\n\n作为文本处理的基本单元，于是就产生了 Doc2Vec 和 Str2Vec 等技术。\n\n7.2   文本离散表示\n\n文本向量化主要有离散表示和分布式表示两种。离散表示是一种基于规则和统计的向 量化方式，常用的方法有词集模型和词袋(BOW)   模型。两类模型都以词之间保持独立性、 没有关联为前提，将所有文本中的词形成一个字典，然后根据字典统计词的出现频数。这 两类模型也存在不同之处。例如，词集模型采用独热表示(one-hot    representation),  只要单 个文本中的单词出现在字典中，就将其置为1,不管出现多少次；而在BOW 模型中只要文 本中的一个词出现在字典中，就将其向量值加1,出现多少次就加多少次。文本离散表示 的特点是忽略文本信息中的语序信息和语境信息，仅将其反映为若干维度的独立概念。由 于这类模型本身无法解决某些问题，如主语和宾语的顺序问题，因此会无法理解诸如“我\n\n为你鼓掌”和“你为我鼓掌”两个语句之间的区别。\n\n7.2.1 独热表示\n\n独热表示用一个长的向量表示一个词，向量长度为字典的大小，每个向量只有一个维\n\n度的数值为1,其余维度的数值全部为0,为1的维度表示该词语在字典中的位置。\n\n例如，有两句话“小张喜欢看电影，小王也喜欢。”和“小张也喜欢看足球比赛。”首   先对这两句话分词后构造一个字典，字典的键是词语，值是ID,   即{\"小张\":1,”喜欢”:2,” 也\":3,\"看\":4,\"电影\":5,\"足球\":6,\"比赛\":7,\"小王\":8}。然后根据 ID 值对每个词语进行向   量化，用0和1代表这个词是否出现，如“小张”和“小王”的独热表示分别为[1,0,0,0,0,\n\n0,0,0],[0,0,0,0,0,0,0,1]。\n\n独热表示词向量构造简单，但通常不是好的选择，它有明显的缺点，具体如下。\n\n(1)维数过高。上例只有两句话，每个词是一个8维向量，随着语料的增加，维数会\n\n越来越大", "metadata": {}}, {"content": "，用0和1代表这个词是否出现，如“小张”和“小王”的独热表示分别为[1,0,0,0,0,\n\n0,0,0],[0,0,0,0,0,0,0,1]。\n\n独热表示词向量构造简单，但通常不是好的选择，它有明显的缺点，具体如下。\n\n(1)维数过高。上例只有两句话，每个词是一个8维向量，随着语料的增加，维数会\n\n越来越大，最终导致维数灾难。\n\n(2)矩阵稀疏。利用独热表示的每一个词向量只有一个维度的数值是1,其他维度上\n\n的数值都为0。\n\n(3)不能保留语义。独热表示的结果不能保留词语在句子中的位置信息。\n\n7.2.2  BOW 模型\n\nBOW 模型用一个向量表示一句话或一个文档。BOW 模型忽略文档的词语顺序、语法、\n\n句法等要素，将文档看作若干个词汇的集合，文档中每个词都是独立的。\n\nBOW 模型每个维度上的数值代表ID 对应的词在句子中出现的频次。上例中两句话的\n\nBOW 模型向量化表示分别为[1,2,1,1,1,0,0,1],[0,1,1,1,0,1,1,1]。\n\nBOW 模型也存在自己的缺点，具体如下。\n\n(1)不能保留语义。不能保留词语在句子中的位置信息，如“我为你鼓掌”和“你为 我鼓掌”的向量化结果没有区别。“我喜欢北京”和“我不喜欢北京”这两个文本语义相反，\n\n但 BOW 模型却认为它们是相似的文本。\n\n(2)维数高和稀疏性。当语料增加时，维数也会增大， 一个文本里不出现的词就会增\n\n多，导致矩阵稀疏。\n\n113\n\nPython 中文自然语言处理基础与实战\n\n7.2.3 TF-IDF 表示\n\nTF-IDF 表示是用一个向量表示一句话或一个文档，它是在BOW 模型的基础上对词 出现的频次赋予 TF-IDF 权值，对 BOW 模型进行修正，进而表示该词在文档集合中的\n\n重要程度。\n\n7.3   文本分布式表示\n\n文本分布式表示是将每个词根据上下文从高维空间映射到一个低维度、稠密的向量上。 文本分布式表示的思想是词的语义根据上下文信息确定，即相同语境中的词语义相近。分 布式表示的优点是考虑到了词之间存在的相似关系，降低了词向量的维度。常用的方法有 基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示，如 LSA 矩阵分解\n\n模型、PLSA 潜在语义分析概率模型、LDA 文档生成模型和 Word2Vec 模型。\n\n分布式表示与独热表示相比，在形式上，独热表示的词向量是一种稀疏词向量，其长 度就是字典长度，而分布式表示是一种固定长度的稠密词向量；在功能上，分布式表示最\n\n大的特点是相关或相似的词在距离上比独热表示更接近。\n\n7.3.1  Word2Vec    模型\n\nWord2Vec 模型其实就是简单化的神经网络模型。随着深度学习技术的广泛应用，基于 神经网络的文本向量化成为 NLP 领域的研究热点。2013 年， 一款用于词向量建模的工具 Word2Vec 出现了，它引起了工业界和学术界的广泛关注。首先，Word2Vec 可以在百万数 量级的字典和上亿的数据集上进行高效的训练；其次，利用该工具得到的训练结果可以很\n\n好地度量词与词之间的相似性。\n\nWord2Vec 模型的输入是独热向量，根据输入和输出模式不同， Word2Vec 模型分为连 续词袋 (Continuous      Bag-of-Words,CBOW) 模型和跳字 (Skip-Gram)    模型。CBOW 模型\n\n的训练输入是某一个特定词的上下文对应的独热向量，而输\n\n出是这个特定词的概率分布。Skip-Gram 模型和 CBOW 模\n\nx₁                                                           型的思路相反，输入是一个特定词的独热词向量，而输出是\n\n这个特定词的上下文的概率分布。CBOW  模型对小型语料\n\n较适用，而 Skip-Gram 模型在大型语料中的表现更好。\n\nWord2Vec   模型特点是：当模型训练好后，并不会使用\n\n训练好的模型处理新的任务，真正需要的是模型通过训练数\n\nX₂\n\n据所得的参数，如隐藏层的权重矩阵。\n\n1.CBOW   模型\n\nCBOW  模型根据上下文的词语预测目标词出现的概\n\n率，其结构如图7-1 所示。CBOW  模型的神经网络包含输 入层、隐藏层和输出层。输入层输入的是某一个特定词上下 文的独热向量，输出层的输出是在给定上下文的条件下特定\n\n词的概率分布。\n\n114\n\n第⑦章 文本向量化\n\n(1)CBOW    模型的网络结构。假设某个特定词的上下文含C 个词，词汇表中词汇量的 大小为 V, 每个词都用独热向量表示，神经网络相邻层的神经元是全连接的。CBOW 模型\n\n的网络结构如下。\n\n①输入层含有C 个单元，每个单元含有 V 个神经元，用于输入 V 维独热向量。\n\n②隐藏层的神经元个数为N,   在输入层中，每个单元到隐藏层连接权重值共享一个\n\nV×N 维的权重矩阵W,   如式(7-1)所示。\n\n(7-1)\n\n③输出层含有 V个神经元，隐藏层到输出层连接权重为N×V 维的权重矩阵W',   如\n\n式(7-2)所示。\n\n(7-2)\n\n④输出层神经元的输出值表示词汇表中每个词的概率分布，可以利用softmax 函数计\n\n算每个词出现的概率。\n\n(2)CBOW    模型的数学形式如下。\n\n①CBOW   模型的简单形式。\n\n假定预测目标词的上下文只有一个词，也就是说模型是在只有一个上下文的情况下预\n\n测一个目标词，此时 CBOW模型的结构如图7-2所示。\n\nN-dim                                                   V-dim\n\n图7-2 单个上下文 CBOW 模型的结构\n\n模型的输入为一个V维独热词向量，输出也是一个V维向量，它包含 V个词的概率，\n\n每个概率代表输入一个词的条件下输出词的概率。\n\n假设给定了一个输入x  (独热向量),则隐藏层的输出如式(7-3)所示。\n\nh=x¹W                                                                                 (7-3)\n\n输出层每个神经元的输入如式(7-4)所示。\n\nu,=w/¹·h                                          (7-4)\n\n115\n\nPython 中文自然语言处理基础与实战\n\n其中，w, 是矩阵W'的第j 列向量。\n\n词汇表中的词用w 表 示 ，w, 表示实际输入的词， w。表示目标词，词汇表中第 j 个词 用w, 表示，条件概率p(w 。=w,w)    表示输入词为w, 、 目标词为w, 的概率。输出层每个神\n\n经元的输出值用 softmax 函数计算，当输入词为w,,   词汇表中每个词为目标词的概率如式\n\n116\n\n(7-5)所示。\n\n(7-5)\n\n②CBOW   模型的一般形式。\n\n在实际问题中， 一个特定词的上下文往往有许多词，需要在给定多个词的条件下，预 测特定词出现的概率。这种情况下，利用上述简单形式的 CBOW 模型的思路，先将多个输\n\n入做平均，然后将平均值看作隐藏层的输入，如式(7-6)所示。\n\n                         (7-6)\n\n因为每个输入x,(i=1,2,…,C)是独热词向量，所以式(7-6)可以简化为式(7-7)。\n\n其中， v,=x,'W。\n\n输出层每个神经元的输入如式(7-8)所示。\n\n(7-7)\n\n(7-8)\n\n在输入为wy,w₂,…,Wc的条件下，词汇表中第j 个词w,的条件概率记为p(w。=w,|wy,w₂,\n\n…,wc) 。 一般形式CBOW 模型的输出值与式(7-5)形式相同", "metadata": {}}, {"content": "， v,=x,'W。\n\n输出层每个神经元的输入如式(7-8)所示。\n\n(7-7)\n\n(7-8)\n\n在输入为wy,w₂,…,Wc的条件下，词汇表中第j 个词w,的条件概率记为p(w。=w,|wy,w₂,\n\n…,wc) 。 一般形式CBOW 模型的输出值与式(7-5)形式相同，如式(7-9)所示。\n\n(7-9)\n\n(3)CBOW 模型的损失函数。CBOW模型输出的真实目标词的概率记为p(wo|w₂,w₂,\n\n…,W),  训练的目的就是使这个概率最大，如式(7-10)所示。\n\nmax p(wo|w₁,w₁,…,W)=maxlogp(wo|w₁,w₁,,…,w₁)\n\n(7-10)\n\n其中， j  表示真实目标词在词汇表中的下标。\n\n在实际求解过程中，习惯求解目标函数的最小值，因此，定义损失函数如式(7-11)\n\n所示。\n\nE=-logp(w 。|w₁,w₁,,…,W₁)\n\n(7-11)\n\n(4)CBOW    模型学习步骤。假设词向量空间维度为V,   上下文词的个数为C,    词汇表\n\n第⑦章 文本向量化\n\n中的所有词都转化为独热向量， CBOW 模型的学习步骤如下。\n\n① 初始化权重矩阵W(V×N   矩阵， N 为人为设定的隐藏层单元的数量),输入层的\n\n所有独热向量分别乘以共享的权重矩阵W,    得到隐藏层的输入向量。\n\n② 对隐藏层的输入向量求平均，将结果作为隐藏层的输出。\n\n③ 隐藏层的输出向量乘以权重矩阵W’(N×V  矩阵),得到输出层的输入向量。\n\n④ 输入向量通过激活函数处理得到输出层的概率分布。\n\n⑤ 计算损失函数。\n\n⑥ 更新权重矩阵。\n\nCBOW  模型由权重矩阵W 和W'确定，学习的过程就是确定权重矩阵W 和 W'的过程， 权重矩阵可以通过随机梯度下降法确定。具体过程是先给权重赋一个随机值进行初始化，然后\n\n按顺序训练样本，计算损失函数及其梯度，再在梯度方向更新权重矩阵。\n\n2.Skip-Gram 模型\n\nSkip-Gram 模型与 CBOW 模型相反，是根据目标词预测上下文。Skip-Gram  模型的结\n\n构如图7-3所示。\n\n图7-3 Skip-Gram 模型的结构\n\n(1)Skip-Gram   模型的结构组成。假设词汇表中词汇量的大小为V,   隐藏层的大小为N,\n\n相邻层的神经元是全连接的， Skip-Gram 模型的结构组成如下。\n\n① 输入层含有V 个神经元，输入是一个V 维独热向量。\n\n② 输入层到隐藏层连接权重是一个V×N  维的权重矩阵W,    如式(7-1)所示。\n\n③ 输出层含有C 个单元，每个单元含有V 个神经元，隐藏层到输出层每个单元连接权\n\n重共享一个N×V  维的权重矩阵W',    如式(7-2)所示。\n\n④ 输出层每个单元使用 softmax  函数计算得到上下文的概率分布。\n\n(2)Skip-Gram     模型的数学形式。假设给定一个输入x,     则隐藏层的输出为一个N 维\n\n117\n\nPython 中文自然语言处理基础与实战\n\n向量，如式(7-12)所示。\n\nh=x¹w                                                             (7-12)\n\n输出层有C×V 个输出神经元，每个神经元节点的净输入如式(7-13)所示。\n\n                           (7-13)\n\n其中， u,  为输出层第c 个单元的第j 个神经元的净输入， v,  是矩阵W’的第j 列向量。\n\n由于每个输出单元共享相同的W',    所以每个单元的第j 个神经元的净输入相同，即\n\nu,=u, 。  净输入经过 softmax 函数计算后，输出层第c 个单元的第j 个神经元的输出如式\n\n118\n\n(7-14)所示。\n\n(7-14)\n\n其中， w。,表示输出层第c 个单元的第j 个神经元对应词汇表中的词； w 。表示实际的第\n\nc 个单元的词； w, 表示输入的特定词； y.,  表示输出层第c 个单元的第j 个神经元的输 出 ；p(we,=w 。w,)     表示输入特定词时，输出的第c 个单元上的词就是上下文第c 个词\n\n的概率。\n\n(3)Skip-Gram  模型的损失函数。模型训练的目标是：给定一个特定词，使输出的C 个\n\n单元为实际的C 个上下文的概率最大，即最大化条件概率，如式(7-15)所示。\n\np(w₀j,W₀₂,…,W₀|w))=p(wo₁|w))p(w。₂|w,)…p(w。w,)          (7-15)\n\nSkip-Gram 模型的损失函数定义如式(7-16)所示。\n\nE=-logp(wo₁,W。₂,…,W₀。w,)\n\n=-logp(w。|w,)p(wo₂|w,)…p(w。。|w,)\n\n                               (7-16)\n\n其 中 ，j   是第c 个单元上的词在词汇表中的索引。\n\n7.3.2  Doc2Vec  模 型\n\n利用 Word2Vec 模型获取一段文本的向量时， 一般做法是先对文本分词，提取文本的 关键词，用 Word2Vec 模型获取这些关键词的词向量，然后计算这些关键词向量的平均值， 或将这些词向量拼接起来，得到一个新的向量，将其看作这个文本的向量。然而，这种方 法只保留了句子或文本中词的信息，丢失了文本中的主题信息。为此，有研究者在 Word2Vec\n\n模型的基础上提出了文本向量化 Doc2Vec模型。\n\nDoc2Vec模型与 Word2Vec模型类似，只是在 Word2Vec 模型输入层增添了一个与词向\n\n量同维度的段落向量，可以将这个段落向量看作另一个词向量。\n\nDoc2Vec 技术存在两种模型，它们分别是分布式记忆 (Distributed   Memory,DM)模\n\n第 ⑦ 章 文本向量化\n\n型和分布式词袋 (Distributed   Bag   of   Words,DBOW) 模型，分别对应 Word2Vec 模型里的\n\nCBOW 模型和 Skip-Gram 模型。\n\n1.DM  模型\n\nDM  模型与 CBOW  模型类似，在给定上下文的前提下，试图预测目标词出现的概率\n\n只不过 DM 模型的输入不仅包括上下文，还包括相应的段落。\n\n假设词汇表中词汇量的大小为V,    每个词都用独热向量表示，神经网络相邻层的神经\n\n元是全连接的，则 DM 模型的网络结构如下。\n\n① 输入层含一个段落单元、C 个上下文单元，每个单元有V 个神经元，用于输入V 维\n\n独热向量。\n\n② 隐藏层的神经元个数为N, 段落单元到隐藏层连接权重为V×N 维矩阵D, 每个上\n\n下文单元到隐藏层连接权重是一个V×N  维的权重矩阵W,    如式(7- 1)所示。\n\n③ 输出层含有V 个神经元，隐藏层到输出层连接权重为N×V 维的权重矩阵W', 如\n\n式(7-2)所示。\n\n④ 利用 softmax  函数计算输出层的神经元输出值。\n\nDM 模型增加了一个与词向量长度相等的段落向量，即 Paragraph  ID, 从输入到输出的\n\n计算过程如下，其结构如图7-4所示。\n\n① Paragraph ID 通过矩阵D 映射为段落向量。段落向量和词向量的维数虽然一样，但 是分别代表两个不同的向量空间。每个段落或句子被映射到向量空间中时，都可以用矩阵D\n\n的一列表示。\n\n② 上下文通过矩阵W 映射到向量空间", "metadata": {}}, {"content": "，即 Paragraph  ID, 从输入到输出的\n\n计算过程如下，其结构如图7-4所示。\n\n① Paragraph ID 通过矩阵D 映射为段落向量。段落向量和词向量的维数虽然一样，但 是分别代表两个不同的向量空间。每个段落或句子被映射到向量空间中时，都可以用矩阵D\n\n的一列表示。\n\n② 上下文通过矩阵W 映射到向量空间，用矩阵W 的一列表示。\n\n③ 对段落向量和词向量求平均值或按顺序拼接后输入softmax 层。\n\n段落ID\n\n图7-4  DM 模型的结构\n\n在句子或文档的训练过程中，Paragraph ID 始终保持不变，共享同一个段落向量，相当 于每次在预测单词的概率时，都利用了整个句子的语义。这个段落向量也可以认为是一个\n\n词，它的作用相当于上下文的记忆单元或这个段落的主题。\n\n在预测阶段，预测的句子新分配一个 Paragraph   ID, 词向量和输出层的参数保持不变，\n\n119\n\nPython 中文自然语言处理基础与实战\n\n重新利用随机梯度下降法训练预测的句子，待误差收敛后即可得到预测句子的段落向量。\n\n2.DBOW    模 型\n\n与 Skip-Gram  模型只给定一个词语预测上下文概率分布类似， DBOW  模型的输入只\n\n有段落向量，其结构如图7-5 所示。DBOW   模型通过一个段落向量预测段落中随机词的\n\n概率分布。\n\n图7-5  DBOW   模型的结构\n\nDBOW 模型的训练方法为忽略输入的上下文，让模型去预测段落中随机的一个词，在 每次迭代的时候从文本中采样得到一个窗口，再从这个窗口中随机采样一个词作为预测任\n\n务，并让模型去预测，输入就是段落向量。\n\nDoc2Vec 模型主要包括以下两个步骤。\n\n(1)  训练模型。在已知的训练数据中得到词向量W、 各参数项和段落向量或句子向\n\n量 D。\n\n(2)推断过程。对于新的段落，需要得到它的向量表达。具体做法是：在矩阵D 中添 加更多的列，并且在固定参数的情况下利用上述方法进行训练，使用随机梯度下降法得到\n\n新的D,    从而得到新段落的向量表达。\n\nDoc2Vec 技术从 Word2Vec 技术扩展而来，DM 模型与 CBOW 模型相对应，所以 DM  模型可以根据上下文词向量和段落向量预测目标词的概率分布；DBOW  模型与 Skip-Gram  模型对应，所以只输入段落向量，DBOW 模型可预测从段落中随机抽取的词组的概率分布。 总而言之， Doc2Vec 是 Word2Vec 的升级， Doc2Vec 不仅提取文本的语义信息，还提取文本\n\n的语序信息。\n\n7.4   任务：文本相似度计算\n\n在实际应用中，虽然文本向量化应用的场景常有不同，如文本分类、文本聚类、情感 分析等，但是文本向量化的训练和使用方式却类似。本节将对两篇论文进行向量化，计算\n\n两篇论文之间的相似度，并为读者介绍 Word2Vec 模型和 Doc2Vec 模型具体的训练和应用\n\n120\n\n第⑦章  文本向量化\n\n流程。本节使用 gensim 库进行具体的代码实现，内容主要包括词向量的训练、段落向量的\n\n训练和文本相似度计算。\n\n7.4.1 Word2Vec 词向量的训练\n\nNLP 的任务离不开语料数据的支持，词向量的训练也不例外。词向量的训练分两个步\n\n骤完成，先对中文语料进行预处理，然后利用 gensim 库训练词向量。\n\n1. 中文语料预处理\n\n训练词向量需要有一个包含大量数据的语料库，本书第2章介绍过国内外一些著名的\n\n语料库，本小节使用某网站的中文网页数据作为训练语料库。\n\n中文语料预处理主要包含以下3个步骤。\n\n( 1 ) 将XML 格式的语料文件读入后存储为 TXT 格式。\n\n(2)将繁体字转换为简体字。\n\n(3)利用jieba库对语料库中的句子进行分词。\n\n中文语料预处理的具体过程如代码7-1所示。\n\n代码7-1 中文语料预处理的具体过程\n\nfrom  gensim.corpora  import  WikiCorpus\n\nimport  os\n\nimport   jieba\n\nimport      gensim      # 加载gensim    库自动提取文档语义主题\n\nfrom  langconv  import  Converter  # 加载 Converter 库转化繁体字\n\nfrom  gensim.models   import  Word2Vec  # 加载Word2Vec  模块训练词向量\n\n# 加 载LineSentence          库将类别加入向量的训练中\n\nfrom gensim.models.word2vec import LineSentence\n\ndef reduce   zh():\n\nspace=!!\n\ni=0\n\n1=[]\n\nzh_name='../data/zh-latest-pages-articles.xml.bz2'\n\nf=open('../data/reduce_zh.txt','w',encoding='utf-8')#\n\nwkc    =WikiCorpus(zh_name,lemmatize=False,\n\ndictionary={})#                从 XML文件中读出的训练语料\n\nfor        text        in        wkc.get_texts():\n\nfor temp_sentence in text:\n\ntemp_sentence  =Converter('zh-hans').convert(temp_sentence) #将繁体字转换成简体字\n\nseg_list=list(\n\njieba.cut(temp_sentence))#      利用jieba 库对语料库中的句子进行分词\n\nfor   temp_term   in   seg_list:\n\n1.append(temp_term)\n\nf.write(space.join(1)+'\\n')#                                将处理完的语料存入TXT文档\n\n1  =[]\n\ni=i+1\n\nif(i          t('S d      '+str(i)+'articles')\n\nf.close()\n\n121\n\nPython 中文自然语言处理基础与实战\n\n代码7-1 中 的 reduce_zh   函数的功能包括格式、字体的转换和分词的操作。其中， WikiCorpus  函数处理从 XML  文件中读出的训练语料， Converter   函数用于将繁体字转换 为简体字，jieba.cut  函数用于对语料库中的句子进行分词，最后将处理完的语料存入TXT\n\n文档。\n\n2. 词向量训练\n\n当中文语料预处理完成后，使用 gensim 库训练词向量，如代码7-2所示。\n\n代码7-2 使用 gensim 库训练词向量\n\ndef train():\n\n=open('../data/reduce_zh.txt','r',encoding='utf-8')\n\n=Word2Vec(LineSentence(wk_news),sg=0,size=192,window=5,\n\nworkers=9)      #  使用Word2Vec 函数训练词向量\n\nmodel.save('../tmp/zhwk_news.word2vec')\n\nif    name      =='main_':\n\nif                                        os.path.exists('../tmp/zhwk_news.word2vec')==False:\n\nprint(     '开始训练模型)\n\ntrain()\n\nprint    ( '模型训练完毕')\n\n代码7-2中的 train 函数使用 gensim.models  中的 Word2Vec 函数训练词向量。Word2Vec 函数的第一个参数表示预处理后的训练语料库； sg=0 表示使用 CBOW   模型训练词向量， sg=1 则表示使用 Skip-Gram 模型训练词向量；参数 size 表示词向量的维度；参数 window 表示目标词与预测词的最大距离，参数值越大所需枚举的预测词就越多，计算时间也就越 长；参数 min_count 表示词语出现的最小次数，如果一个词语出现的次数小于min_count,\n\n则直接忽略该词语；参数 workers  表示训练词向量时使用的线程数。\n\n由于语料较多，所以进行数据预处理和训练需要等待较长的时间。因此，代码7-2 的\n\n最后设置了判断语句，如果检测到训练好的模型已存在则不需要再次训练网络。\n\n训练完成后得到的词向量模型即可用于做一些应用，如计算“番茄”与“西红柿”,以\n\n及“卡车”与“货车”的相似度", "metadata": {}}, {"content": "，如果一个词语出现的次数小于min_count,\n\n则直接忽略该词语；参数 workers  表示训练词向量时使用的线程数。\n\n由于语料较多，所以进行数据预处理和训练需要等待较长的时间。因此，代码7-2 的\n\n最后设置了判断语句，如果检测到训练好的模型已存在则不需要再次训练网络。\n\n训练完成后得到的词向量模型即可用于做一些应用，如计算“番茄”与“西红柿”,以\n\n及“卡车”与“货车”的相似度，如代码7-3所示。\n\n代码7-3 计算“番茄”与“西红柿”,以及“卡车”与“货车”的相似度\n\n7.4.2  Doc2Vec   段落向量的训练\n\n与训练词向量类似，Doc2Vec 段落向量的训练同样分为数据预处理和利用gensim 库训\n\n练段落向量两个步骤。\n\n1. 数据预处理\n\n定义 LabeledLineSentence  类对数据进行预处理，预处理过程与 Word2Vec 基本类似，\n\n如代码7-4所示。\n\n122\n\n第⑦章 文本向量化\n\n代码7-4 定义 LabeledLineSentence 类对数据进行预处理\n\n代码7-4中定义的 LabeledLineSentence 类包含繁体字与简体字的转换操作和利用jieba 库进行分词的操作。此外，gensim 库里的 Doc2Vec 提供的 LabeledSentence 函数还可以将文\n\n档标签(如类别)加入文档向量的训练中，因此 Doc2Vec 在训练时能够采用标签 (tags)\n\n信息进行更好的辅助训练。相对于 Word2Vec 模型，这里的输入文档多了一个 tags 属性，\n\n如代码7-4的最后一行所示。\n\n2. 段落向量训练\n\n当数据预处理完成后，使用 gensim 库训练段落向量，如代码7-5所示。\n\n在代码7-5中，使用 Doc2Vec 函数训练段落向量，第一个参数 documents 表示预处理 后用于训练的语料； dm=0 表示使用 DBOW 模型训练段落向量， dm=1 则表示使用 DM 模 型训练段落向量；参数 size 表示段落向量的维度，参数 window 、min  count 、workers的含\n\n义与 Word2Vec  函数相同。\n\n当模型训练完成后，计算词语“番茄”与“西红柿”,以及“卡车”与“货车”的相似\n\n123\n\nPython 中文自然语言处理基础与实战\n\n度，如代码7-6所示。\n\n代码7-6计算词语“番茄”与“西红柿”,以及“卡车”与“货车”的相似度\n\n7.4.3  计算文本的相似度\n\n本小节分别使用 Word2Vec 模型和 Doc2Vec 模型计算两份文本的相似度。\n\n1. 使用Word2Vec 模型计算文本相似度\n\n使用Word2Vec 模型计算文本相似度包括以下3个步骤。\n\n(1)提取文本中的关键词。\n\n(2)使用Word2Vec   模型将关键词向量化，并将所有关键词的词向量相加，得到文本\n\n的向量化表示。\n\n(3)利用文本的向量化表示计算文本的相似度。\n\n关键词提取算法在第6章有详细的介绍，本小节使用jieba 库里的 tfidf 函数提取关键\n\n词，如代码7-7所示。\n\n代码7-7 使用 tfidf 函数提取关键词\n\nimport  sys\n\nimport        codecs        # 加载 codecs 库进行编码转换\n\nimport gensim\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom jieba import analyse\n\n# 使 用TF-IDF   算法提取句子的关键词\n\ndef.keyword_extract(data):\n\ntfidf        =analyse.extract_tags\n\nkeywords                   =tfidf(data)\n\nreturn     keywords\n\n#对文档中的每句话进行关键词提取并保存\n\ndef                                   segment(file,keyfile):\n\nwith                            open(file,'r',encoding='utf-8')as                            f,open(\n\nkeyfile,'w',encoding='utf-8')as                                        k:\n\nfor doc in f:\n\nkeywords               =keyword_extract(doc[:len(doc)-1])\n\nfor        wor.w te(ds:    +'')\n\nk.write('\\n')\n\n#获取字符串中某字符的位置和出现的总次数\n\ndef get   char pos(string,char):\n\nchPos             =[]\n\ntry:\n\nchPos=list(((pos)for                pos,val                 in                enumerate(string)if(val                 ==char)))\n\nexcept:\n\n124\n\n第 ⑦ 章  文本向量化\n\n在代码7- 7中，keyword     extract 函数的功能是提取句子的关键词，使用的算法是TF-IDF 算法； segment 函数用于对文档中的每句话进行关键词提取；get_char_pos 函数用于进行有\n\n关空格的操作。\n\n利用训练好的词向量模型获取关键词的词向量，如代码7-8所示。\n\n代码7-8 获取关键词的词向量\n\n125\n\nPython 中文自然语言处理基础与实战\n\n在代码7-9中，自定义的 similarity 函数用于计算向量间余弦值，该余弦值用于表示两\n\n个向量的相似度。运行该代码后可得出两份文本的相似度为92.04%.\n\n2. 使用 Doc2Vec 模型计算文本相似度\n\n使用Doc2Vec 模型计算文本相似度与使用Word2Vec 模型计算文本相似度的操作类似， 主要包括3个步骤，即首先对数据预处理，然后将文档向量化，最后计算文本相似度。使\n\n用 Doc2Vec 模型计算文本相似度", "metadata": {}}, {"content": "，该余弦值用于表示两\n\n个向量的相似度。运行该代码后可得出两份文本的相似度为92.04%.\n\n2. 使用 Doc2Vec 模型计算文本相似度\n\n使用Doc2Vec 模型计算文本相似度与使用Word2Vec 模型计算文本相似度的操作类似， 主要包括3个步骤，即首先对数据预处理，然后将文档向量化，最后计算文本相似度。使\n\n用 Doc2Vec 模型计算文本相似度，如代码7-10所示。\n\n代码7- 10  使 用 Doc2Vec  模型计算文本相似度\n\nimport\n\nimport\n\nimport gensim\n\nimport numpy as np\n\nimport pandas as pd\n\ndef                       segment(doc:str):\n\nquotio=p3_,words            =pd.read_csv('../data/stopwords.txt',index_col=False,\n\nnames=['stopword'],sep='\\n',encoding='utf-8')\n\nstop_words          =list(stop_words.stopword)\n\nreg_html=re.compile(r¹<[^>]+>',re.S)#                                     去除HTML标签数字等\n\ndoc       =reg_html.sub('',doc)\n\ndoc                                              =re.sub('[0-9]','',doc)\n\nord_list  =list(\\ )doc))\n\nout               str=\n\nfor      word      in      word_list:\n\nif     word     not     in     stop_words:\n\nout_str     +=word\n\nout        str        +=\n\n126\n\n第 ⑦ 章 文本向量化\n\nsegments                             =out_str.split(sep='')\n\nreturn  segments\n\ndef        doc2vec(file_name,model):\n\nra_lpoch =1\n\nc_vec_all=model.infer=_s.( aamrte,'_=infer_epoch)\n\nreturn doc_vec_all\n\n#计算两个向量余弦值\n\ndef similarity(a  vect,b  vect):\n\n127\n\ndot_val\n\na_norm\n\n=0.0\n\n_\n\nb_norm  =0.0\n\ncos                 =None\n\nfor  ad,botinv p+(t,b_vect):\n\na_norm        +=a        **2\n\nb_norm    +=b    **2\n\nif  a_norm   ==0.0   or   b_norm   ==0.0:\n\ncos=-1\n\nelse:\n\ncos         =dot_val         /((a_norm*b_norm)**0.5)\n\nreturn                     cos\n\ndef           test_model(filel,file2):\n\nprint( '导入模型')\n\nmodel_path   ='../tmp/zhwk_news.doc2vec'\n\nmodel       =gensim.models.Doc2Vec.load        (model_path)\n\nvect1=doc2vec(filel,model)#                              转成句子向量\n\nvect2          =doc2vec(file2,model)\n\nprint(sys.ge         tsizeof(vect1))#                  查看变量占用的空间大小\n\nprint(sys.getsizeof(vect2))\n\ncos   =similarity(vectl,vect2)\n\nprint(    ' 相 似 度 ： 8 0 . 2f% 号 ! 名(cos*100)\n\nif          name                      =                  main_  _':\n\nfile1='../data/corp_test/t1.txt'\n\nfile2='../data/corpus_test/t2.txt'\n\ntest_model(filel,~file2)\n\n在代码7-10中， segment  函数用于去停用词和其他的数据预处理操作； doc2vec  函数中 的第5行为句子的向量化操作，通过加载训练好的模型迭代找出合适的向量代表并输入文 档； similarity 函数使用余弦值计算两个向量的相似度。运行该代码后可得出两份文本的相\n\n似度为74.99% c\n\n3. 两种相似度计算方法分析\n\n使用Word2Vec 和 Doc2Vec 两种模型计算文本相似度的结果显示，利用 Word2Vec 模型\n\nPython 中文自然语言处理基础与实战\n\n计算的相似度为92.04%,高于 Doc2Vec 模型得到的74.99%。查看这两份文本可以发现， 它们之间相似度确实较高，因此得到的这两个结果都是较为合理的。但是这并不能说明 Word2Vec  模型更胜一筹。通常情况下， Doc2Vec 模型的效果可能会更加准确，这是因为 Doc2Vec模型除了利用词语的语义信息外，还综合了上下文的语序信息，而 Word2Vec 模型 在训练时会丢失语序信息。查看 Word2Vec  模型提取的两份文本的关键词可以发现它们的 多数词语相同，可能因为这两份文本本身较为相似。另外，Word2Vec 模型中的关键词提取\n\n算法准确率达到了较高的水平，保留了许多关键信息。\n\n小结\n\n本章主要介绍了文本向量化的基本概念和两种表示方法。首先介绍了文本向量化的基 本概念。接着分别介绍了文本向量化的离散表示和分布式表示，其中离散表示介绍了独热\n\n表 示 、BOW 模型和TF-IDF 表示3种表示方法，分布式表示介绍了Word2Vec 模型和Doc2Vec\n\n模型两个模型。最后结合代码详细介绍了利用gensim 库进行文本向量化的模型训练和应用。\n\n实训\n\n实 训 1  实现基于 Word2Vec   模型的新闻语料词向量训练\n\n1.  训练要点\n\n(1)掌握对新闻语料进行预处理的方法。\n\n(2)掌握使用Word2Vec 模型进行词向量训练的方法。\n\n2. 需求说明\n\n对2019年《人民日报》每日新闻的语料库中的语料进行预处理，并通过 Word2Vec 模\n\n型实现文本向量化。\n\n3. 实现思路与步骤\n\n(1)读取 TXT 文件形式的新闻语料库。\n\n(2)提取新闻文本内容并进行预处理，查看数据是否存在缺失值，并对其进行去重\n\n操作。\n\n(3)使用jieba 分词默认字典切分新闻内容。\n\n(4)对分词后的结果去停用词，并处理数据中的一些无意义的“\\u3000”\\和“\\xa0”\n\n形式的空格。\n\n(5)保存处理后的数据集。\n\n(6)获取保存后的训练数据集，并使用gensim 库中的 word2vec 函数训练词向量。\n\n实 训 2 实现基于 Doc2Vec 模型的新闻语料段落向量训练\n\n1.  训练要点\n\n掌握使用 Doc2Vec 模型进行段落向量训练的方法。\n\n2. 需求说明\n\n针对实训1中已经处理好的新闻文本语料，使用Doc2Vec 模型实现文本向量化。\n\n128\n\n第⑦章  文本向量化\n\n3. 实现思路与步骤\n\n(1)读取实训1中已保存的训练数据集。\n\n(2)将每个标签或编号与训练语料库中对应的 document   相关联。\n\n( 3 ) 使 用gensim 库中的 Doc2Vec 函数训练段落向量。\n\n实 训 3 使 用Word2Vec 模型和 Doc2Vec 模型计算新闻文本的相似度\n\n1.  训练要点\n\n(1)掌握使用 Word2Vec模型计算文本相似度的方法。\n\n(2)掌握使用 Doc2Vec 模型计算文本相似度的方法。\n\n2. 需求说明\n\n分别使用 Word2Vec 模型和 Doc2Vec 模型计算两个新闻文本的相似度。\n\n3. 实现思路与步骤\n\n使用Word2Vec 模型计算文本相似度包括以下3个步骤。\n\n(1)提取新闻文本中的关键词。\n\n(2)使用 Word2Vec 模型将关键词向量化，并将所有关键词的词向量相加，得到文本\n\n的向量化表示。\n\n(3)利用文本的向量化表示计算文本的相似度。\n\n使用 Doc2Vec 模型计算文本相似度主要包括3个步骤。\n\n(1)对新闻文本进行去停用词等数据预处理。\n\n(2)将句子向量化", "metadata": {}}, {"content": "，并将所有关键词的词向量相加，得到文本\n\n的向量化表示。\n\n(3)利用文本的向量化表示计算文本的相似度。\n\n使用 Doc2Vec 模型计算文本相似度主要包括3个步骤。\n\n(1)对新闻文本进行去停用词等数据预处理。\n\n(2)将句子向量化，通过加载训练好的模型迭代找出合适的向量代表并输入文档。\n\n(3)使用余弦值计算文本相似度。\n\n课后习题\n\n1. 选择题\n\n(1)独热表示的缺点不包括(    )。\n\nA.  构造简单    B.  维数过高    C.   可以保留语义    D.  矩阵稀疏\n\n(2)BOW     模型其中的一个缺点是(    )。\n\nA.  可以保留语义                B.  维数低\n\nC.  没有忽略文档的词语顺序      D.   矩阵稀疏\n\n(3)不属于分布式表示模型的是(    )。\n\nA.   分类模型                     B.LS   A 矩阵分解模型\n\nC.PLSA     潜在语义分析概率模型   D.Word2Vec    模型\n\n(4)下列关于 Word2Vec 模型说法正确的是(    )。\n\nA.  得到的训练结果不能度量词与词之间的相似性\n\nB.  当这个模型训练好以后，需要用这个训练好的模型处理新的任务\n\nC.  真正需要的是这个模型通过训练数据所得的参数\n\nD.  Word2Vec 模型其实就是简化的遗传算法模型\n\n129\n\nPython 中文自然语言处理基础与实战\n\n(5)DM     模型与 CBOW 模型的区别为(    )。\n\nA.DM    模型的输入包括上下文\n\nB.DM   模型预测目标词出现的概率\n\nC.DM   模型输入不仅包括上下文，而且还包括相应的段落\n\nD.CBOW   模型输入包括上下文\n\n2. 操作题\n\n(1)调整 gensim 库 中 word2vec  函数的参数，并训练模型，利用训练好的模型计算词\n\n语“番茄”与“西红柿”,以及“卡车”与“货车”的相似度。\n\n(2)调整 gensim 库 中 doc2vec  函数的参数，并训练模型，利用训练好的模型计算词\n\n语“番茄”与“西红柿”,以及“卡车”与“货车”的相似度。\n\n(3)另找两份文本，利用调整参数后训练好的模型，计算文本的相似度。\n\n130\n\n第 8 章 文本分类与文本聚类\n\n文本分类和文本聚类是NLP 任务中的基础性工作，其目的是对文本资源进行整理和归 类，是解决文本信息过载问题的关键环节。文本分类常采用机器学习中的朴素贝叶斯、决 策树、逻辑回归和支持向量机等算法，文本聚类则常采用K-means算法。本章首先介绍文 本挖掘的基本概念，然后介绍文本分类和文本聚类的常用算法与步骤，最后通过实例演示\n\n文本分类算法和文本聚类算法在NLP 中的实际应用。\n\n学习目标\n\n(1)了解文本挖掘的基本概念。\n\n(2)熟悉常用的文本分类和文本聚类算法。\n\n(3)掌握实现文本分类和文本聚类的步骤。\n\n8.1  文本挖掘简介\n\n随着网络时代的到来，用户可获得的信息包含从技术资料、商业信息到新闻报道、娱 乐资讯等多种类别和形式的文档，这些文档构成了一个异常庞大的具有异构性、开放性特 点的分布式数据库，这个数据库中存放的是非结构化的文本数据。结合人工智能研究领域\n\n中的 NLP技术，数据挖掘派生出了文本挖掘这个新兴的研究领域。\n\n文本挖掘是提取有效、新颖、有用、可理解的，散布在文本文件中的有价值知识，并\n\n且利用这些知识更好地组织信息的过程。文本挖掘是NLP 中的重要内容。\n\n文本挖掘的基本技术有6大类，包括文本信息提取、文本分类、文本聚类、摘要抽取、\n\n文本数据压缩、文本数据处理。\n\n文本挖掘从数据挖掘发展而来，但并不是说简单地将数据挖掘技术运用到包含大量文 本的集合上即可实现文本挖掘，还需要做很多准备工作。文本挖掘的准备工作由文本收集、 文本分析和特征修剪3个步骤组成。准备工作完成后，就可以开展文本挖掘工作。文本挖\n\n掘的工作流程如图8-1 所示。\n\n从目前文本挖掘技术的研究和应用状况来看，从语义的角度实现文本挖掘的还很少，\n\n目前应用最多的几种文本挖掘技术有文本分类、文本聚类和摘要抽取。\n\nPython 中文自然语言处理基础与实战\n\n文本结构分析\n\n文本摘要生成\n\n图 8 - 1 文本挖掘的工作流程\n\n(1)文本分类将带有类别的文本集合按照每一类的文本子集合共有的特性，归纳出分 类模型，再按照该模型将其他文档迁移到已有类中，最终实现文本的自动分类。这样既可\n\n以方便用户查找信息，又可以缩小查找文本的范围。\n\n(2)文本聚类将文本集合分为若干个簇，要求同簇内的文本尽量相似度高，而不同簇 的文本尽量相似度低，从而挖掘整个数据集的综合布局。例如，用户浏览的相关的内容一 般挨得比较近，而与用户浏览的无关的内容往往会离得比较远。因此，用户可运用聚类算 法将需要筛选的文本内容聚成若干簇，将与用户浏览内容相关性不高的簇去除，只保留与\n\n用户浏览内容相关性高的簇，从而提高浏览文本的效率。\n\n(3)摘要抽取利用计算机自动从原始文档中提取出能够准确地反映该文档中心内容的简 单连贯的短文。摘要抽取能够生成简短的关于文档内容的指示性信息，将文档的主要内容呈\n\n现给用户，以便用户决定是否要阅读文档的原文，这样能够节省用户大量的浏览时间。\n\n利用文本挖掘技术处理大量的文本数据，无疑能够给企业带来巨大的便利。文本挖掘 在商业智能、信息检索、生物信息处理等领域都有广泛的应用，如客户关系管理、邮件自 动回复、垃圾邮件过滤、简历自动评审、搜索引擎等。因此，目前企业对文本挖掘的需求\n\n非常高，文本挖掘技术的应用前景很广阔。\n\n8.2   文本分类常用算法\n\n文本分类是指按照一定的分类体系或规则实现让文本自动划归类别的过程，文本分类 在信息索引、数字图书管理、情报过滤等领域有广泛的应用。文本分类一般分为基于知识 工程的分类方法和基于机器学习的分类方法。基于知识工程的分类方法是指通过专家经验， 依靠人工提取规则进行的分类。基于机器学习的分类方法是指通过计算机自主学习提取规 则进行的分类。最早应用于文本分类的机器学习算法是朴素贝叶斯算法，之后几乎所有重 要的机器学习算法在文本分类领域都得到了应用，如支持向量机算法、神经网络算法、决\n\n策树算法和K 最近邻算法等。各分类算法的优缺点如表8-1所示。\n\n表 8 - 1 各分类算法的优缺点\n\n算法 优点 缺点 朴素贝叶斯算法 算法简单，分类效果稳定；适用于小规模数 据的训练；所需估算的参数少，对缺失数据 不敏感 算法假设属性之间相互独立，而实际 中往往难以成立；属性过多或属性之 间相关性较大时，分类效果不好；分 类效果依赖于先验概率；对输入数据 的表达形式很敏感\n\n132\n\n第 8 章 文本分类与文本聚类\n\n续表\n\n算法 优点 缺点 支持向量机算法 可用于小样本数据学习；具有较高的泛化能 力；可用于高维数据的计算；可以解决非线 性问题；可以避免神经网络结构选择和局部 极小点问题 对缺失数据敏感；对非线性问题没有 通用解决方案 神经网络算法 并行处理能力强；学习能力强、分类准确度   高；对数据噪声有较强的顽健性和容错能力； 能解决复杂的非线性关系，具有记忆功能 神经网络训练过程中有大量的参数  需要确定；不能观察网络之间的学习 过程，输出结果难以解释；学习时间 长，且效果不可保证 决策树算法 易于理解，逻辑表达式生成较简单；数据预 处理要求低；能够处理不相关的特征；可通 过静态测试对模型进行评测；能够在短时间 内对大规模数据进行处理；能同时处理数据 型和常规型属性，可构造多属性决策树 易倾向于具有更多数值的特征；处理 缺失数据时存在困难；易出现过拟 合；易忽略数据集属性的相关性 K最近邻算法 训练代价低，易处理类域交叉或重叠较多的 样本集；适用于样本容量较大的文本集合 时空复杂度高，样本容量较小或数据 集偏斜时容易误分，K值的选择会影 响分类性能\n\n在文本挖掘中，文本分类有着广泛的应用场景，常见的应用场景如下。\n\n(1)Web  文档自动分类。随着互联网的发展， Web 已成为拥有庞大信息资源的分布式 信息空间，拥有各式各样海量的 Web 文档。为了有效地组合和处理 Web 文档", "metadata": {}}, {"content": "，易处理类域交叉或重叠较多的 样本集；适用于样本容量较大的文本集合 时空复杂度高，样本容量较小或数据 集偏斜时容易误分，K值的选择会影 响分类性能\n\n在文本挖掘中，文本分类有着广泛的应用场景，常见的应用场景如下。\n\n(1)Web  文档自动分类。随着互联网的发展， Web 已成为拥有庞大信息资源的分布式 信息空间，拥有各式各样海量的 Web 文档。为了有效地组合和处理 Web 文档，人们希望按\n\n照 Web 文档的内容对其进行分类，网页自动分类技术也随之诞生。\n\n(2)新闻分类。新闻网站中涵盖大量的新闻报道，随着电子传播手段在新闻报道中的 广泛运用，新闻体裁的分类趋于多样化，各类新闻都有其定位和表现内容需要的体裁。为 此需要根据新闻内容对新闻网站中的新闻按照一定的分类标准进行分类，如政治、军事、\n\n经济、娱乐和体育等。\n\n(3)情感分析。情感分析是对带有主观感情色彩的文本内容进行分析和处理的过程， 它挖掘人们针对不同的人物、产品或事件的观点、态度和情绪。互联网中有大量用户参 与并发表评论的平台，如淘宝、京东和微博等，这些评论表达了用户的喜、怒、哀、乐 等情绪。当需要对这些评论进行情感分析时，文本分类可以按照不同情感将其划分为若\n\n干类。\n\n(4)信息检索。信息检索是指用户采用一定的方法，借助搜索引擎从中查找所需信息 的过程。信息检索同样采用了文本分类方法，先判断用户查找内容的所属类别，然后从该\n\n类别的信息集合中再做进一步检索。\n\n8.3  文本聚类常用算法\n\n文本聚类主要是从杂乱的文本集合中挖掘对用户有价值的信息，这些蕴含在文本集中\n\n的未被发现的信息能够用于更合理地组织文本集合。文本聚类的主要思想是对无类别标示\n\n133\n\nPython 中文自然语言处理基础与实战\n\n的文本文档集合进行分析，通过对文本特性的分析探索其应有的信息，再对集合中的文本 按照特性分析的结果进行标识类别，发现文本内容中潜在的信息。文本聚类是对文本数据 进行组织、过滤的有效手段，广泛应用于主题发现、社团发现、网络舆情监测、网络信息\n\n内容安全监测等领域。\n\n传统的文本聚类方法使用 TF-IDF 技术对文本进行向量化，然后使用 K-means 等聚类 手段对文本进行聚类处理。文本向量化表示和聚类算法是提升文本聚类精度的重要环节，\n\n选择恰当的文本向量化表示和聚类算法是文本聚类的关键。\n\n聚类算法是机器学习中的一种无监督学习算法，它不需要对数据进行标记，也不需要 训练过程，通过数据内在的相似性将数据点划分为多个子集，每个子集称为一个簇，对应 着潜在的类别，而同一类别中的数据相似性较高，不同类别之间的数据相似性较低。聚类 实质上就是将相似度高的样本聚为一类，并且期望同类别样本之间的相似度尽可能高，不\n\n同类别样本之间的相似度尽可能低。\n\n聚类算法主要分为基于划分的聚类算法、基于层次的聚类算法、基于密度的聚类算法、\n\n基于网格的聚类算法、基于模型的聚类算法和基于模糊的聚类算法，具体介绍如下。\n\n(1)基于划分的聚类算法。这种算法是聚类算法中原理最为简单的算法，划分法的基 本思想为给定一个有n 个记录的数据集合，将数据集划分为K 个组，每一个组称为一个簇。 对于给定的K 个组，同一个组内的数据记录距离越近越好，不同组之间的距离则越远越好。 以划分法为基本思想的算法包括 K-means 算法、Single-Pass 增量聚类算法、K-medoids 算\n\n法和 CLARANS算法等，其中最为经典、应用最多的是K-means算法。\n\n(2)基于层次的聚类算法。这种算法的主要思想是将样本集合合并成凝聚度更高或分 割成更细的子样本集合，最终样本集合形成一棵层次树。层次聚类算法不需要预先设定聚 类数，只需要样本集合通过不断迭代达到聚类条件或迭代次数即可。基于层次的经典聚类 算法有变色龙算法、凝聚层次聚类 (Agglomerative     Nesting,AGNES) 算法、基于代表的\n\n聚类 (Clustering  Using  Representatives,CURE) 算法等。\n\n(3)基于密度的聚类算法。这种算法的主要思想是首先找出密度较高的点，然后将 周围相近的密度较高的样本点连成一片，最后形成各类簇。基于密度的聚类算法中比较 具有代表性的3种算法为 DBSCAN算法、OPTICS 算法和 DENCLUE算法。此类算法的 优点是鲁棒性强，对任意形状的聚类都适用，但是结果的精度与参数设置关系密切，实\n\n用性不强。\n\n(4)基于网格的聚类算法。这种算法的出发点不再是平面，而是空间，空间中的有限 个网格代表数据，聚类过程就是按一定的规则将网格合并。由于该算法在处理数据时是独 立的，仅依赖网格结构中每一维的单位数，因此处理速度很快。但是此算法对参数十分敏\n\n感，速度快的代价是精确度不高，通常需要与其他聚类算法结合使用。\n\n(5)基于模型的聚类算法。这种算法的思路是假设每个类为一个模型，然后再寻找与 该模型拟合最好的数据，通常有基于概率模型和基于神经网络模型的算法。概率模型即概 率生成模型，假设数据是由潜在的概率分布产生的，典型的算法是高斯混合模型。这类聚\n\n类算法在样本数据量大的时候执行率较低，不适合大规模聚类场合。\n\n(6)基于模糊的聚类算法。这种算法的主要思想是以模糊集合论作为数学基础，用模\n\n134\n\n第 8 章  文本分类与文本聚类\n\n糊数学的方法进行聚类分析。此算法的优点是对于满足正态分布的样本数据而言效果会很 好。但是此算法过于依赖初始聚类中心，为确定初始聚类中心需要多次迭代以寻找最佳点，\n\n遇到大规模数据样本时会大大增加时间复杂度。\n\n上述的聚类算法各有优缺点，在面对不同的数据集时会得到不同的效果。其中部分聚\n\n类算法在性能方面的差异如表8-2所示。\n\n表8 - 2 部分聚类算法在性能方面的差异\n\n聚类算法 处理大规模 数据的能力 处理高维 数据的能力 发现任意形状簇 的能力 数据顺序 敏感度 处理噪声的 能力 基于层次的 聚类算法 弱 较强 强 不敏感 较弱 基于划分的 聚类算法 较弱 强 较强 不敏感 弱 基于密度的 聚类算法 较强 弱 强 不敏感 强\n\n8.4   文本分类与文本聚类的步骤\n\n利用机器学习算法进行文本分类或文本聚类时， 一般包含数据准备、特征提取、模型\n\n选择与训练、模型测试、模型融合等步骤，具体介绍如下。\n\n(1)数据准备。文本数据一般是非结构化的数据，这些数据或多或少存在数据缺失、 数据异常、数据格式不规范等情况，这时需要对其进行预处理，包括数据清洗、数据转换、\n\n数据标准化、缺失值和异常值处理等。\n\n(2)特征提取。特征提取是进行文本分类前要做的工作之一，有几种经典的特征提取 方法，分别是 BOW模型、TF 、TF-IDF 、n-gram 和 Word2Vec。BOW模型拥有过大的特征 维度，数据过于稀疏。TF 和 TF-IDF 运用统计的方法将词汇的统计特征作为特征集，但效\n\n果与 BOW模型相差不大。\n\n(3)模型选择与训练。指对处理好的数据进行分析，判断适合用于训练的模型。先判 断数据是否属于监督学习，即数据中是否存在类标签；如果存在，那么将该数据归为监督 学习问题，否则划分为无监督学习问题。在模型的训练过程中，通常会将数据划分为训练\n\n集和测试集，训练集用于训练模型，测试集用于后续验证模型效果。\n\n(4)模型测试。测试数据可以对模型进行验证，分析产生误差的原因，包括数据来源、 特征、算法等。进行模型测试可以找出测试数据中的错误样本，发现特征或规律，从而找\n\n到提升算法性能、减少误差的方法。\n\n(5)模型融合。模型融合是指同时训练多个模型，综合考虑不同模型得到的结果，再 根据一定的方法集成模型，从而得到更好的结果。模型融合是提升算法准确率的一种方法， 当模型效果不太理想时，可以考虑使用模型融合的方式进行改善。单个机器学习算法的准\n\n确率不一定比多个模型集成的准确率高。\n\n135\n\nPython 中文自然语言处理基础与实战\n\n8.5   任务：垃圾短信分类\n\n在微信、微博、QQ  等社交软件盛行的今天，短信依旧是信息沟通的常用方式，因 运营商技术缺陷、商家的不良使用手段等原因造成的垃圾短信也日益增多。目前", "metadata": {}}, {"content": "，再 根据一定的方法集成模型，从而得到更好的结果。模型融合是提升算法准确率的一种方法， 当模型效果不太理想时，可以考虑使用模型融合的方式进行改善。单个机器学习算法的准\n\n确率不一定比多个模型集成的准确率高。\n\n135\n\nPython 中文自然语言处理基础与实战\n\n8.5   任务：垃圾短信分类\n\n在微信、微博、QQ  等社交软件盛行的今天，短信依旧是信息沟通的常用方式，因 运营商技术缺陷、商家的不良使用手段等原因造成的垃圾短信也日益增多。目前，某运 营商积累了大量的垃圾短信数据，数据已经过加工处理，共80万条。数据包括标签列 和内容，标签列中0表示非垃圾短信、1表示垃圾短信，部分垃圾短信的信息如表8-3\n\n所示。\n\n表8 - 3 部分垃圾短信的信息\n\n短信ID 审核结果 短信文本内容 1 0 商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一 2 1 南口新春第一批限量春装到店啦!春暖花开淑女裙、冰蓝色公…… 3 0 带给我们大常州一场壮观的视觉盛宴 4 0 有原因不明的泌尿系统结石等 5 0 23年从盐城拉回来的妈妈的嫁妆 6 0 感到自减肥、跳减肥健美操 7 1 感谢致电源华利烧烤店，本店位于金城路×××号。韩式…… 8 0 这款UVe智能杀菌机器人是扫地机器人的最佳伴侣 9 1 一次价值×××元王牌项目；可充值×××元店内项目卡一张；可以参与…… 10 0 此类皮肤特别容易产生粉刺、黑头等\n\n本小节将运用朴素贝叶斯模型，分别采用自定义函数和调用Python 内置函数这两种方\n\n法对垃圾短信进行分类。垃圾短信分类的流程包括以下步骤。\n\n(1)数据读取。读取原始短信数据，共有80万条。\n\n(2)文本预处理。对原始数据进行预处理，包括去重、脱敏和分词等操作，最后进行 词频统计。分别统计垃圾短信与非垃圾短信的词频，随后绘制相应的词云图。由于原始数\n\n据量较大，需要对数据进行采样，共抽取2万条数据进行模型训练与分类。\n\n(3)分类。分别采用两种方式对短信内容进行分类，第一种方式是自定义朴素贝叶斯 函数，第二种方式是调用Python  内置函数实现朴素贝叶斯分类，两种方式的实现步骤基本\n\n一致，将最终结果与测试集进行比较，得到模型的分类情况和准确率。\n\n(4)模型评价。使用处理好的测试集进行预测，对比真实值与预测值，获得准确率并\n\n进行结果分析。\n\n1. 数据读取\n\n加载库并读取数据，如代码8-1 所示。统计可知，数据中的垃圾短信有8万条，非垃\n\n圾短信有72万条。\n\n代码8-1 加载库并读取数据\n\n136\n\n第 8 章  文本分类与文本聚类\n\n2. 文本预处理\n\n文本预处理包括以下几个步骤。\n\n(1)单独提取短信内容进行预处理，对其进行去重和脱敏操作。\n\n(2)由于原始数据中的敏感信息已用统一字符替换，因此进行脱敏时只需减去相应的\n\n字符即可，脱敏后共减少了899271个字符。\n\n( 3 ) 采 用jieba 分词切分短信内容，由于分词的过程中会切分部分有用信息，需要加载\n\n自定义词典 newdic1.txt  以避免过度分词，文件中包含了短信内容的几个重要词汇。\n\n(4)对分词后的结果去停用词，去停用词后共减少了22824034个字符。\n\n(5)此时数据中还存在一些无意义的空列表，需要对其进行删除。其中， lambda  函数\n\n是自定义函数，可以借助 apply  函数实现并返回相应的结果。\n\n(6)使用自定义函数统计词频，其中非垃圾短信仅留下词频大于30的词，垃圾短信则\n\n留下词频大于5的词。分别对垃圾短信与非垃圾短信绘制词云图，查看短信内容分布情况。\n\n进行文本预处理，如代码8-2所示。\n\n137\n\nPython 中文自然语言处理基础与实战\n\nheader=None,engine='python')\n\np=wdo_cut.as[lbda x:len(x)).sum()\n\ndata_qustop   =data_cut.apply(lambda  x:[i   for   i   in   x   if  i   not   in   stopword]) 14        =data_qustop.astype('str').apply(lambda        x:len(x)).sum()\n\nta(_q'atr(_.loc[  data_qustop.index if data_qustop[i]!=\n\n[]]]\n\n#词频统计\n\nlab        =[data.loc[i,'类别'] for  i  in  data_qustop.index]\n\nlab1 =pd.Series(lab,index=data   qustop.index)\n\ndefcipin(data   qustop,num=10):\n\ntemp    =[''.join(x)for    x    in    data_qustop]\n\np2 =pd.S (pl.split()).value_counts()\n\nreturn temp2[temp2 >num]\n\ndata   gar =data   qustop.loc[lab1 ==1]\n\ndata  nor  =data   qustop.loc[lab₁==0]\n\ndata   garl =cipin(data   gar,num=5)\n\ndata_nor1 =cipin(data_nor,num=30)\n\n#绘制垃圾短信词云图\n\nback_pic=imageio.imread('../data/background.jpg') ·\n\nwc           =WordCloud(font_path='C:/Windows/Fonts/simkai.ttf',   # 字 体\n\nbackground_color='white',         #背景颜色\n\nmax_words=2000,     #最大词数\n\nmask=back_pic,         # 背景图片\n\nmax_font_size=200,#             字体大小\n\nrandom_state=1234)           #  设置随机配色方案的数量\n\nal_fwiisi (.o)s(data_gar1\n\nplt.imshow(gar_wordcloud)\n\nplt.axis('off')\n\nplt.savefig('../tmp/spam.jpg')\n\nplt.show()\n\n#绘制非垃圾短信词云图\n\nsiz . ords(data_nor1)\n\nplt.imshow(nor_wordcloud)\n\nplt.axis('off')\n\nplt.savefig('../tmp/non-spam.jpg')\n\nplt.show()\n\n运行代码8-2后，得到的垃圾短信和非垃圾短信的词云图分别如图8-2、图8-3所示。\n\n138\n\n第 8 章  文本分类与文本聚类\n\n喷标专业\n\n电 话\n\n3\n\n免费\n\n图8-2 垃圾短信词云图\n\n飞税南京\n\n微轻\n\n设计 时   旅\n\n百 “ 度 2 0 1 5\n\n图8-3 非垃圾短信词云图\n\n从图8-2、图8-3中可以看出，垃圾短信内容中的“活动”“您好”“优惠”“电话”等 词出现频次很高，非垃圾短信内容中的“手机”“电脑”“南京”“飞机”等词出现频次很高。 因此可以认为，绝大多数垃圾短信内容是与推销、诈骗等有关的内容，通过介绍优惠活动 吸引用户，达到推销或其他目的。而非垃圾短信则与生活息息相关，内容包括常见的电子 设备，如手机、电脑，它们是生活中不可或缺的一部分，还有外出旅游、办公等方面的词，\n\n这些词在非垃圾短信中出现频次较高。\n\n由于原始数据量过大，为了方便后续建模与分类，这里采用简单随机抽样，对垃圾短\n\n信与非垃圾短信信息各采样一万条，如代码8-3所示。\n\n代码8-3 数据采样\n\n其中， sample 函数为简单随机抽样，这里设置了随机状态，每次运行程序时的抽样方\n\n式与上一次相同。\n\n3. 分类\n\n朴素贝叶斯分类可以通过调用 MultinomialNB 函数实现。首先划分训练集和测试集， 分别输入数据集的短信内容与标签、测试集所占比例和随机状态，然后利用训练集生成词 库，分别构建训练集和测试集的向量矩阵，最后利用内置朴素贝叶斯函数预测分类。调用\n\nMultinomialNB 函数进行分类和预测", "metadata": {}}, {"content": "， sample 函数为简单随机抽样，这里设置了随机状态，每次运行程序时的抽样方\n\n式与上一次相同。\n\n3. 分类\n\n朴素贝叶斯分类可以通过调用 MultinomialNB 函数实现。首先划分训练集和测试集， 分别输入数据集的短信内容与标签、测试集所占比例和随机状态，然后利用训练集生成词 库，分别构建训练集和测试集的向量矩阵，最后利用内置朴素贝叶斯函数预测分类。调用\n\nMultinomialNB 函数进行分类和预测，如代码8-4所示。\n\n139\n\nPython 中文自然语言处理基础与实战\n\n代码8-4 调用 MultinomialNB 函数进行分类和预测\n\nx#划_,_和t,_train,y_test    =train_test_split(\n\nmy_data.message,my_data.label,test_size=0.2,random_state=123)#\n\n词频向量矩阵\n\n#训练集\n\ncv                 =CountVectorizer()   # 将文本中的词语转化为词频矩阵\n\ntrain_cv                                   =cv.fit_transform(x_train)#  拟合数据，将数据转化为标准化格式\n\ntrain   cv.toarray()                                                    \n\ntrain_cv.       shape       #  查看数据大小\n\ncv.vocabulary_   #查看词库内容\n\n#测试集\n\ncv1  =CountVectorizer(vocabulary=cv.vocabulary_)\n\n=cvl.fit_transform(x_test)\n\ntest_cv.shape\n\n#  朴素贝叶斯\n\nnb               =MultinomialNB()       #   朴素贝叶斯分类器\n\nnb.fit(train_cv,y_train)              #  训练分类器\n\npre =nb.predict(test   cv)  # 预 测\n\n4. 模型评价\n\n分类和预测完成后，需要对模型进行评价，如代码8-5所示。\n\n代码8-5 模型评价\n\n代码8-5的运行结果如下。\n\n结果显示，测试集中正确分类的数据共有3732条，错误分类的数据共有268条。其中， 非垃圾短信和垃圾短信被正确分类的数据分别有1784条和1948条，垃圾短信被预测为非  垃圾短信的有37条，非垃圾短信被预测为垃圾短信的有231条，模型的分类准确率为0.93, 分类效果较好。非垃圾短信的预测精确度高于垃圾短信，而垃圾短信的召回率要高于非垃 圾短信， F1 值基本一致。精确度和召回率分别表示模型对垃圾短信、非垃圾短信的识别能 力 ，F1  值是两者的综合，值越高说明模型越稳健，这里模型总体的精确度、召回率和 Fl\n\n值大致为0.93,模型较为稳健。\n\n140\n\n第 8 章 文 本 分 类 与 文 本 聚 类\n\n8.6 任务：新闻文本聚类\n\n数据来自新闻网站的新闻数据合集，该数据总共有15个类别标签，分别为政治、国际、 经济、体育、房产、观点、健康、教育、旅游、汽车、社会、数码、文娱、消费和反腐前\n\n沿，每个标签分别有500条新闻数据。新闻文本聚类的具体流程如下。\n\n(1)数据读取。读取文件列表中的新闻文本并给定标签，划分训练集与测试集，读入\n\n的每条新闻作为一行，以便后续数据处理和词频矩阵的转化。\n\n(2)文本预处理。对每个新闻文本进行jieba 分词和去停用词处理，去掉文本中无用的\n\n停用词，降低处理维度，加快计算速度。\n\n(3)特征提取。使用sklearn 库调用 CountVectorizer 和 TfidfTransformer 函数计算 TF-\n\nIDF 值，将文本转化为词频矩阵。\n\n(4)聚类。根据导入数据类型标签个数定义分类个数，导入训练数据集后调用 sklearn.\n\ncluster 训练模型，并保存聚类模型。\n\n(5)模型评价。使用处理好的测试集进行预测，对比真实值与预测值，获得准确率并\n\n进行结果分析。\n\n1. 数据读取\n\n为了方便计算和考虑计算机内存及运算速度的问题，仅采用其中的4个数据集进行运\n\n算，分别为政治、国际、经济和体育。\n\n获取文件列表信息，逐一读取数据，在获得文本内容的同时去除文本中的换行符、制 表符等特殊符号，最后对每个类别的新闻文本进行划分，将80%的数据作为训练集、20%\n\n的数据作为测试集。读取数据并划分数据集，如代码8-6所示。\n\n代码8-6读取数据并划分数据集\n\nimport          re\n\nimport os\n\nimport json\n\nimport             jieba\n\nimport pandas as pd\n\nfrom            sklearn.cluster            import            KMeans\n\nimport              joblib\n\nfrom   sklearn.feature_extraction.text   import    TfidfTransformer\n\nfrom   sklearn.feature_extraction.text   import    CountVectorizer\n\n#读取数据\n\nfiles                         =os.listdir('../data/json/')     #读取文件列表\n\ntrain_data              =pd.DataFrame()\n\ntest_data        =pd.DataFrame()\n\nfor            file            in            files:\n\nwith             open('../data/json/'+file,'r',encoding='utf-8')as\n\ncontent               =[]\n\nwhil odruef:1 =load   f.readline()\n\nif   load_f1:\n\nload   dict =json.loads(load   f1)\n\n141\n\nPython 中文自然语言处理基础与实战\n\n2. 文本预处理\n\n使用自定义函数 seg_word 对预处理后的内容进行封装，以便对训练集与测试集进行相 关的处理。由于使用pandas 库的 read_csv 函数读取文件时会默认将空格符号去除，因此在 加载停用词后需要加上空格符号。读取数据中的每个新闻文本，使用jieba  库进行分词处理 并去除停用词。本小节划分了训练集与测试集，在数据处理时要分别对其进行数据的预处\n\n理和后续的特征提取。定义自定义函数 se  gword,  如代码8-7所示。\n\n代码8-7 定义自定义函数 seg_word\n\n3. 特征提取\n\n特征提取环节调用 CountVectorizer  函数将文本中的词语转化为词频矩阵，矩阵元素 a[i][j]表示j 词在i 类文本下的词频；调用 TfidfTransformer 函数计算 TF-IDF 权值并转化为 矩阵，矩阵元素 w[i][j]表示j 词 在i 类文本中的 TF-IDF 权重。特征提取的过程如代码8-8\n\n所示。\n\n代码8-8 特征提取的过程\n\n142\n\n第 8 章 文本分类与文本聚类\n\n4. 聚类\n\n本小节选取了4个数据集，因此这里选用4个中心点，随后进行模型的训练。调用fit 函数将数据输入分类器中，训练完成后保存模型，并查看训练集的准确率。训练聚类模型\n\n并查看模型准确率，如代码8-9所示。\n\n代码8-9 训练聚类模型并查看模型准确率\n\n运行代码8-9后，输出结果如下。\n\n143\n\nPython 中文自然语言处理基础与实战\n\n第一个输出结果为聚类的中心点，表示4个类别的聚类中心点；第二个输出结果表示\n\n模型训练集的准确率为0.43125;第三个输出结果为每个数据样本的簇，即类别标签。\n\n5. 模型评价\n\n输入测试数据进行模型评价，查看测试集的准确率，如代码8-10所示。\n\n代码8- 10  查看测试集的准确率\n\n运行代码8-10后，输出结果如下。\n\n测试集准确率为：0.35\n\n使用K-means  聚类时，由于每次的起始点是随机选取的，因此可能存在多次运行程序 后得到的准确率不一致的情况。本节模型的训练集与测试集的准确率不是很高，读者可以\n\n尝试通过不同的特征提取方式提高准确率。\n\n小结\n\n本章主要介绍了文本分类与文本聚类的基本概念", "metadata": {}}, {"content": "，如代码8-10所示。\n\n代码8- 10  查看测试集的准确率\n\n运行代码8-10后，输出结果如下。\n\n测试集准确率为：0.35\n\n使用K-means  聚类时，由于每次的起始点是随机选取的，因此可能存在多次运行程序 后得到的准确率不一致的情况。本节模型的训练集与测试集的准确率不是很高，读者可以\n\n尝试通过不同的特征提取方式提高准确率。\n\n小结\n\n本章主要介绍了文本分类与文本聚类的基本概念，以及相应的 Python 实现方法。首先 介绍了文本挖掘的基本概念和应用场景。接着介绍文本分类和文本聚类的常用算法。随后 介绍了文本分类与文本聚类的步骤。最后实现了文本分类与文本聚类对应的 Python案例\n\n分别为垃圾短信的分类和新闻文本的聚类。\n\n实训\n\n实 训 1  基于朴素贝叶斯的新闻分类\n\n1.  训练要点\n\n(1)掌握词频统计方法。\n\n144\n\n第 8 章  文本分类与文本聚类\n\n(2)掌握自定义朴素贝叶斯函数。\n\n(3)掌握分类和预测完成后模型的评价方法。\n\n2. 需求说明\n\n每一条新闻都包含一个主题类别，可通过分析主题进行新闻分类。目前有一个通过收 集新闻网站上的新闻得到的新闻文本数据集，已知网站上的新闻共有46类，该新闻文本数\n\n据集包含一万多条新闻，需要对已经过脱敏处理的新闻数据进行分类。\n\n3. 实现思路与步骤\n\n(1)读取数据并进行文本预处理。\n\n(2)调用内置的朴素贝叶斯函数对新闻文本进行分类。\n\n(3)对完成分类和预测的模型进行评价。\n\n实 训 2  食品种类安全问题聚类分析\n\n1.  训练要点\n\n(1)掌握文本预处理的方法，划分训练集与测试集。\n\n(2)掌握特征提取的方法。\n\n(3)掌握K-means 聚类算法。\n\n(4)掌握计算测试集准确率的方法。\n\n2. 需求说明\n\n关于食品安全的网络新闻蕴含着大量的食品安全信息，这是食品药品监督管理局设置 各类食品监督方向的重要参考。然而，网络新闻数据虽丰富但杂乱无章。目前从某新闻网 站上收集了一份包含有食品种类安全问题的文本，需要对收集的包含有食品种类安全问题\n\n的文本进行聚类分析。\n\n3. 实现思路与步骤\n\n(1)使用自定义函数 seg_word 对预处理的内容进行封装，并划分训练集与测试集。\n\n( 2 ) 调 用CountVectorizer 函数将文本中的词语转换为词频矩阵。\n\n( 3 ) 调 用TfidfTransformer  函数计算 TF-IDF 权值并转化为数组。\n\n(4)训练聚类模型并查看模型准确率。\n\n(5)输入测试数据进行模型评价，计算测试集的准确率。\n\n课后习题           \n\n1. 选择题\n\n(1)不属于文本挖掘的基本技术分类的是(    )。\n\nA.     文本信息抽取                B.  文本分类\n\nC.    文本聚类                  D.  文本数据挖掘\n\n(2)适用于样本容量较大的文本集合的文本分类算法是(    )。\n\nA.  朴素贝叶斯算法              B.  支持向量机算法\n\n145\n\nPython 中文自然语言处理基础与实战\n\nC.   神经网络算法                 D.K  最近邻\n\n(3)决策树算法的缺点是(    )。\n\nA.  学习时间长，且效果不可保证\n\nB.  易出现过拟合，易忽略数据集属性的相关性\n\nC.  时空复杂度高，样本容量较小或数据集偏斜时容易误分\n\nD.  对非线性问题没有通用解决方案\n\n(4)对于满足正态分布的样本数据来说效果会很好，但是过于依赖初始聚类中心的算\n\n法是基于(    )的聚类算法。\n\nA. 模型        B. 网格        C. 模糊             D.   密度\n\n(5)属于特征提取方法的是(    )。\n\nA.BOW   模型   B.   数据标准化  C.    训练模型         D.  模型融合\n\n2. 操作题\n\n(1)更改垃圾短信分类中testingNB 函数的参数，查看运行结果。\n\n(2)参考垃圾短信分类，选用机器学习中几个常用的分类算法实现垃圾短信分类，并\n\n对这几个算法进行比较。\n\n(3)将新闻文本聚类原先采用的4个数据集改为房产、观点、健康、教育这4个新数\n\n据集，并给出运行结果。\n\n146\n\n第 9 章 文本情感分析\n\n情感是人们对诸如产品、服务、组织等的态度，文本情感分析是对带有情感色彩的文 本进行处理、分析、归纳和推理的过程。互联网中存在大量的用户对诸如事件、产品等有 价值的评论信息，这些信息表达了各种情感色彩和情感倾向，反映了大众对某一事件或产 品的看法。情感分析技术已经得到越来越多的应用。本章主要介绍情感分析的主要内容、\n\n常见应用和常用方法，并通过实例实现商品评论信息的情感分析。\n\n学习目标\n\n(1)了解文本情感分析的基本概念和主要内容。\n\n(2)熟悉基于情感词典的方法、基于文本分类的方法和基于LDA 主题模型的方法。\n\n(3)掌握基于情感词典、基于文本分类和基于LDA 主题模型的情感分析的实现过程。\n\n9.1 文本情感分析简介\n\n文本情感分析是指使用 NLP、 文本挖掘和计算机语言等方法对带有情感色彩的主 观性文本进行分析、处理、归纳和推理的过程。文本情感分析着眼于确定某个人或某 些人对某些特定主题的态度，针对用户对某个主题的看法或评论进行文本挖掘，从而 得到该看法或评论属于用户对该事物的积极态度还是消极态度。文本情感分析的快速 发展得益于社交媒体，如论坛、微博、微信等的快速发展。2000 年以来，情感分析已 经成为 NLP 中最活跃的研究领域之一，在数据挖掘、Web 挖掘、文本挖掘和信息检索\n\n领域有着广泛的应用。\n\n9.1.1 文本情感分析的主要内容\n\n文本情感分析的主要内容包括主客观分类、情感分类、情感极性判断等。主客观性 文本是指用户对某一事物的观点和看法，情感分析的对象是含有情感倾向的文本，文 本的情感分类是情感分析的基础性工作。情感分类是指将一份文本分为积极、消极、 中性等类别。情感极性判断是指分析一份文本的总体态度是肯定还是否定，是褒义还\n\n是贬义。\n\n1. 主客观分类\n\n主观性文本是相对于客观性文本而言的一种文本表达形式，主要描述人们对事物的想\n\nPython 中文自然语言处理基础与实战\n\n法或看法。识别出有主观情感的句子之后，才能对主观句子进行极性判断，判断其为褒义 或贬义。由于文本表现方式比较自由，主、客观文本的特征不明显，在很多情况下，文本\n\n的主客观识别比主观文本的情感分类更有难度。\n\n文本的主客观分类能够有效提高文本情感分析的准确率，目前主要通过文本中是否出 现情感词或短语模式简单地判断句子的主客观性，客观句子的识别准确率一般在80%左右，\n\n而主观句子的识别准确率在60%左右。\n\n2. 情感分类\n\n情感分类是一种特殊的文本分类问题。目前，情感分类主要有基于情感词典和基\n\n于机器学习两种方法。\n\n基于情感词典的情感分类是利用已有语义词典资源构建领域词典，再通过比对文本中 所包含的正向情感词、负向情感词，标记文本的标签为正、负整数值作为情感值，同时考 虑一些特殊的词性规则、句法结构对情感判断的影响，如否定句、递进句、转折句等对情\n\n感值进行修正。这种方法需要规模较大的情感词典作为分析的基础。\n\n基于机器学习的情感分类的关键在于特征选择、特征权重量化、分类器模型这3 个要素。特征选择主要有基于信息增益、基于卡方统计、基于文档频率等方法。常见 的特征权重量化方式包括布尔权重、词频 (TF) 、 逆文档频率 (IDF) 、TF-IDF 、 熵权重 等。分类器模型包括朴素贝叶斯、支持向量机、K   近邻、神经网络、决策树、逻辑回\n\n归等。\n\n3. 情感极性判断\n\n情感极性判断就是对文本内容反映的正面或负面、肯定或否定、褒义或贬义的色彩的 判断。相对于情感分类，情感极性判断是二分类问题，而前者属于多分类问题。极性判断 主要包括基于情感词典的方法和基于机器学习的方法。情感极性判断主要集中于情感词语\n\n极性判断和情感文本极性判断两个方面。\n\n情感词语极性判断主要有两个研究方向， 一个是基于语义词典进行判断", "metadata": {}}, {"content": "，情感极性判断是二分类问题，而前者属于多分类问题。极性判断 主要包括基于情感词典的方法和基于机器学习的方法。情感极性判断主要集中于情感词语\n\n极性判断和情感文本极性判断两个方面。\n\n情感词语极性判断主要有两个研究方向， 一个是基于语义词典进行判断，另一个是 基于大规模语料库进行判断。情感文本极性判断类似于文本情感分类，但是文本情感分 类还依赖于系统的情感分类体系，目前情感分类体系还没有一个权威的标准。相对于情 感分类，情感极性判断在商业，尤其在舆情监控、商品评论分析、微博评论分析等方面\n\n有着更广泛的应用。\n\n9.1.2  情感分析的常见应用\n\n情感分析在信息检索、社交网络、舆情监控、语音识别、机器翻译、推荐系统中有着\n\n广泛的应用，以下以商品评论分析、舆情分析和信息预测为例进行介绍。\n\n(1)商品评论分析是情感分析技术应用最频繁的一个领域。目前，电子商务发展迅 速，越来越多的消费者选择在网上进行购物。因此，带有主观色彩的商品评论的文本 数量正在迅速增长。这些文本里面蕴含着大量有商业价值的信息，人们可以从互联网 上的商品主观性评论信息中提取出产品的特征或属性。消费者可以了解其他人对某种 商品的态度倾向分布，优化购买决策。生产商和销售商可以了解消费者对其商品和服\n\n务的反馈信息，以及消费者对厂商和竞争对手的评价，从而改进产品和改善服务，获\n\n148\n\n第⑨章 文本情感分析\n\n得竞争优势。\n\n(2)舆情分析主要是分析民众对热点事件或新闻事件的看法。现在最具代表性的舆 情平台是微博和微信。由于现在用户能更多地参与到信息的产生中去，越来越多的具 有个人观点的内容出现微博和微信等网络平台上。这些内容对了解民众对新闻人物和 新闻事件的总体评价，掌握当前的舆情信息，特别是热点事件的舆情信息，有着重要 作用。当前网络舆情对社会的直接影响越来越大，直接关系到网络的信息安全。但通 过人工手段难以处理网络中出现的海量信息，因此自动化的情感分析技术在该领域非\n\n常有实用价值。\n\n(3)信息预测是指根据过去和现在已经掌握的有关某一事物的信息资料，运用科学的 理论和技术，深入分析和认识事物演变的规律性，从已知信息推出未知信息，从现有信息 导出未来信息，从而对事物的未来发展做出科学预测的方法。某一个新事件的发生或网络 上对某个事件的热议都在很大程度上左右着人们的思维和行动。因此，信息预测变得非常 必要。情感分析技术可以帮助用户通过对互联网上的新闻、帖子等信息源进行分析，预测\n\n某一事件的未来状况。\n\n9.2  情感分析的常用方法\n\n情感分析技术的核心问题是情感分类，判断一份文本中的情感取向是一种分类问题。 情感类别的划分方式一般可以分为两种， 一种是正面、负面二分类或正面、中立、负面 三分类；另一种是多元分类，是指具有4种以上分类，如四分类有悲伤、忧愁、快乐和 兴奋，七分类有高兴、悲伤、喜欢、生气、厌恶、恐惧和惊讶，可以根据实际需要划分 情感种类和设置情感词。分类方法中的主要方法有基于情感词典的方法、基于文本分类\n\n的方法和基于LDA 主题模型的方法等。\n\n9.2.1 基于情感词典的方法\n\n基于情感词典的方法是在文本中查找相应的情感词、否定词和程度副词，结合情感词 典中情感词的得分情况、否定情况和程度级别进行相应的打分，最后得分的总和即为文本 的情感分。该方法在较大程度上依赖于情感词典的内容，因此，词典的准确性和灵活度会\n\n对结果产生较大的影响。\n\n情感词是主体对某一个客体表达带有情感色彩的内在评价的词语，具有极性和强度两 种属性。极性是指情感词表达出的褒贬词义，即正负面情感，如“好吃”“喜欢”是褒义词， 表示正面情感；“难吃”“讨厌”是贬义词，表达负面情感。强度是指情感的强弱，如“我 感到害怕”和“我感到恐惧”,“恐惧”表达出的情感要比“害怕”强，通常使用数字表示\n\n情感的强度，数值越大，强度越大。\n\n情感词典有多种不同的版本，人们可以根据自己的需求选择相应的情感词典。本小节 使用的是 BosonNLP 情感词典，该词典基于微博、新闻、论坛等数据来源构建，其部分内\n\n容如表9-1 所示，左边为情感词，右边为该情感词的情感分值。\n\n149\n\nPython 中文自然语言处理基础与实战\n\n表 9 - 1 BosonNLP   情感词典的部分内容\n\n情感词 情感分值 疼爱 1.86843297836 美貌 1.86903161791 和睦相处 1.86909683411 小清新 1.87585892189\n\n副词用于限制或修饰动词、形容词或整个句子，其中用于表示程度的副词称为程度副  词。程度副词本身没有任何的情感倾向性，但能够增强或减弱情感强度。程度副词不一定  能改变情感倾向性的结果，但能改变情感倾向的程度。例如，“非常开心”和“有点开心” 两个程度副词“非常”和“有点”都是表达“开心”的程度，但“非常开心”的情感倾向\n\n程度更强。\n\n否定词用于表示否定意义。否定词本身没有任何的情感倾向性，但是它能够改变情感 的极性。例如，“开心”和“不开心”,否定词“不”改变了情感的极性，将正面情感转变\n\n为了负面情感。\n\n基于情感词典的情感分析流程如下。\n\n(1)对文本进行分词和去停用词，去除与情感词无关的词语。\n\n(2)对分词结果进行分类，找出其中的情感词、程度副词和否定词。\n\n(3)计算情感词的得分，得分函数的公式如式(9-1)所示。\n\n(9-1)\n\n其中w 为权重，默认为1; s(i) 为情感词得分； p(i) 为情感词对应的程度副词和否定词的乘\n\n积，程度副词和否定词默认为1。\n\n9.2.2  基于文本分类的方法\n\n基于文本分类的方法采用标注了情感类别的文本进行训练，获得情感分类器，最后对 情感分类器进行测试，输出结果为多个概率值(正面概率、负面概率或正面概率、中立概\n\n率、负面概率),选择概率最高的情感倾向作为分类结果。\n\n基于文本分类的方法是情感分析中较常用的一种方法，具体有特征选取、文本转换为 特征向量、划分训练集与测试集、构建分类器、验证分类器等步骤。由于分类器根据文本\n\n的特征进行分类，因此特征选取是影响分类器准确率的关键一步。\n\n(1)特征选取。特征就是分类对象所展现的部分特点，是实现分类的依据。例如，人 们在评价一道菜时，通常考虑的是“色”“香”“味”,如果有一道菜外观好看、香味足、味 道好，那么这道菜无疑是一道美食。其中“外观好看”“香味足”“味道好”就是一道美食 的特征。需要根据实际情况选择有助于判断的特征，有时还需要一定的人工参与。此外， 当文本量较庞大时，特征量也会随之增大。这无疑会影响运行速度，因此需要对特征进行 降维。可以采用统计词频、统计文档频率等方式进行特征降维。其中统计词频的方法是选\n\n择出现频率较高的词作为特征，统计文档频率的方法则是选择在不同文档中出现的频率较\n\n150\n\n第 9 章  文本情感分析\n\n高的词作为特征。\n\n(2)文本转换为特征向量。机器学习无法直接将中文文本作为输入数据，在使用分类 算法时，需要将输入文本转换为特征向量的表示形式。例如，将“这道菜非常好吃!”转换\n\n为特征，结果为[{\"这道\":True,\"  菜\":True,\"  非常\":True,” 好吃\":True,\"!\":True},positive]。\n\nTrue 表示该文本具有此特征， False 则表示不具有此特征。\n\n(3)划分训练集与测试集。机器学习通常需要对数据划分训练集与测试集，训练集用\n\n于训练文本，测试集用于测试分类算法的效果。\n\n(4)构建分类器。构建分类器是指运用机器学习的算法训练数据集，得到分类器。选 用机器学习算法时，可以根据实际情况调用合适的算法构建分类器，也可以同时采用多种 算法，然后选用准确率最高的算法构建分类器。 一般情况下，不同的文本所需要采用的分 类器有所不同，所以需要采用多种算法进行训练，然后选用效果最佳的算法进行下一步的\n\n测试。\n\n(5)验证分类器。分类器构建完成后，需要进行分类器的验证。使用测试集对分类器\n\n进行测试，比对测试结果，获得测试集的准确率，分析测试结果，给出改进建议。\n\n9.2.3  基于 LDA 主题模型的方法\n\n鉴于LDA 主题模型在文本挖掘领域的优势，基于主题的文本情感分析技术也成为人们 关注的热点。基于主题的文本情感分析主要通过挖掘用户评论所蕴含的主题", "metadata": {}}, {"content": "，所以需要采用多种算法进行训练，然后选用效果最佳的算法进行下一步的\n\n测试。\n\n(5)验证分类器。分类器构建完成后，需要进行分类器的验证。使用测试集对分类器\n\n进行测试，比对测试结果，获得测试集的准确率，分析测试结果，给出改进建议。\n\n9.2.3  基于 LDA 主题模型的方法\n\n鉴于LDA 主题模型在文本挖掘领域的优势，基于主题的文本情感分析技术也成为人们 关注的热点。基于主题的文本情感分析主要通过挖掘用户评论所蕴含的主题，以及用户对\n\n这些主题的情感偏好来提高文本情感分析的准确率。\n\n基于LDA 主题模型的文本情感分析主要包括以下几个部分。\n\n(1)评论信息采集与预处理(如网页爬取、中文分词、停用词处理等)。\n\n(2)主题提取、情感词提取(可能涉及情感词典的构建)。\n\n(3)主题的情感分类或评分。\n\n( 4)主题情感摘要生成(方便用户直接了解主题)。\n\n(5)系统评测。\n\n9.3   任务：基于情感词典的情感分析\n\n基于情感词典的情感分析是最简单的情感分析方法。该方法首先对文档分词，找出文 档中的情感词、否定词和程度副词；然后判断每个情感词的前面是否存在否定词和程度副  词，将否定词和程度副词划分为一个组，如果有否定词那么就将情感词的情感权值乘以-1, 如果有程度副词则乘以程度副词的程度值；最后对所有组的得分求和，大于0的归于正向，\n\n小于0的归于负向。\n\n基于情感词典的情感分析的过程如代码9-1 所示。首先定义 seg_word 函数对句子进行 分词并去停用词，仅保存重要的词语。接着定义 sort_word 函数加载情感词典、否定词词典、 程度副词词典，将分词结果分到3种词典中，并计算各词的得分情况。最后结合各词的得\n\n分情况定义 socre_sentiment 函数，汇总情感词的总得分。\n\n代码9-1 基于情感词典的情感分析的过程\n\n151\n\nPython 中文自然语言处理基础与实战\n\nimport codecs\n\nfrom        collections         import         defaultdict        #导 入collections          用于构建空白词典\n\ndef seg_word(sentence):\n\ng_lis =[] =jieba.cut(sentence)\n\nfor  word  in  seg_list:\n\nseg_result.append(word)\n\nstopwords                =set()\n\nstopword                                          =codecs.open('../data/stopwords.txt','r',\n\nencoding='utf-8')               # 加载停用词\n\nfor word in stopword:\n\nstopwords.add(word.strip())\n\nstopword.close()\n\nreturn          list(filter(lambda          x:x          not          in           stopwords,seg_result))\n\ndef      sen_ word(wo('../data/BosonNLP_sentiment_score.txt','r+',\n\nencoding='utf-8')                  #  加载 Boson 情感词典\n\nsen   list  =sen   file.readlines()\n\nsen   dict     =defaultdict()#构建词典\n\n152\n\nfor       s       in\n\ns=re.s\n\nsen_list:\n\nub('\\   n','',s)#\n\n去除每行最后的换行符\n\nif          s:\n\nn建_[lt、(''以)  i(的'')词[ ]\n\nnot   file   =open('../data/否定词.txt','r+',\n\nenco ding='utf-8') not   list  =not   file,readlines()\n\nfor           i           in           range(len(not_list)):\n\n# 加载否定词词典\n\nnot_list[i]=re.sub('\\n','',not_list[i])\n\ndegree_file                       =open('../data/程度副词(中文).txt','r+',\n\ndegree_list =degree_filain!())\n\ndegree   dic =defaultdict()\n\nfor d in degree   list:\n\nd                        =re.sub('\\n','',d)\n\nif          d:\n\ndegree_dic[d.split('')[0]]=d.split('')[1]\n\nsen   file.close()\n\ndegree_file.close()\n\nnot   file.close()\n\nsen_word   =dict()\n\nnot_word               =dict()\n\ndegree_word    =dict()\n\n# 分 类\n\nfor    word    in    word_dict.keys():\n\nif      word      in       sen_dict,keys()and      word      not\n\ndegree_dic.keys():\n\n#  加载程度副词词典\n\nin      not_list      and      word      not      in\n\n第⑨章  文本情感分析\n\n153\n\n的词\n\nsenword[word_dict[word]]=sen_dict[word]#                                情感词典中的包含分词结果\n\nelif word in not   list and word not in degree   dic.keys():\n\nnot_word[word_dict[word]]=-1                          # 程度副词词典中的包含分词结果的词\n\nelif         word         in         degree_dic.keys();\n\n#否定词词典中的包含分词结果的词\n\ndegree_word[word_dict[word]]=degree_dic[word]\n\nreturn              sen_word,not_word,degree_word             #返回分类结果\n\ndef                    list_to_dict(word_list):\n\ndata        ={}\n\nfor         x         in         range(0,len(word_list)):\n\ndata[word_list[x]]=x\n\nreturn                     data\n\ndef socre   sentiment(sen  word,not  word,degree  word,seg  result): W=1      #  初始化权重\n\nscore  =0\n\nsentiment_index         =-1         # 情感词下标初始化\n\nfor i in range(0,len(seg  result)):\n\nif  i   in   sen_word.keys():\n\nscore          +=W*float(sen_word[i])\n\ntseg#_eu个lt)词\n\nif  j   in   not_word.keys():\n\nscore      *=-1       # 否定词反转情感\n\nelif  j   in   degree_word.keys():\n\nscore                           *=float(degree_word[j])# 乘 以 程 度 副 词\n\nreturn score\n\ndef setiment(sentence):\n\n对eg文_词g_和wo去情感词无关的词语\n\n对en分_t分_,情_ _词st_to_dict(seg_list))\n\ne感_ent(sen_word,not_word,degree_word,seg_list)\n\nreturn    seg   list,sen  word,not  word,degree  word,score\n\nif   _name_==    mn\n\nprint(          setiment(              ' 我 今 天 特 别 开 心 ‘)\n\nprint(setim        ent(    '我今天很开心、非常兴奋'))\n\nprint(setiment(            '我昨天开心，今天不开心'))\n\n运行代码9-1后，输出的结果如下。\n\n(['特别','开心],{1:'2 .61234173173'},{},{0:'1 .5'},3 .918512597595) (['开心','兴奋'],{0:'2.61234173173',1:'0.865173758618'},{},{},\n\n3.477515490348)\n\nPython 中文自然语言处理基础与实战\n\n从运行结果可以看出，这种情感分析方法对情感词、程度副词和否定词的识别较为准 确，大致上能够将情感词的正负面区别开。“我今天特别开心”句子中“开心”的情感得分 约为2.612,“特别”是程度副词，赋予的程度级别为1.5,句中没有否定词，最终相乘结果\n\n约为3.919。\n\n在实际应用中，基于情感词典的情感分析方法对情感词典的依赖较大，对程度副词和 否定词的依赖也是如此。读者在实际应用中时，可以寻找词典内容更为完善的情感词典进\n\n行情感分析，使情感分析的情感分更准确。\n\n9.4     任务：基于文本分类的情感分析\n\n9.1.2小节介绍了情感分析的一种常见应用", "metadata": {}}, {"content": "，赋予的程度级别为1.5,句中没有否定词，最终相乘结果\n\n约为3.919。\n\n在实际应用中，基于情感词典的情感分析方法对情感词典的依赖较大，对程度副词和 否定词的依赖也是如此。读者在实际应用中时，可以寻找词典内容更为完善的情感词典进\n\n行情感分析，使情感分析的情感分更准确。\n\n9.4     任务：基于文本分类的情感分析\n\n9.1.2小节介绍了情感分析的一种常见应用，即对消费者的评论文本数据进行情感分析。\n\n本节以某平板电脑的评论数据为语料库，对情感分析做了一个简单的实现。\n\n9.4.1 基于朴素贝叶斯分类的情感分析\n\n首先将文本转换为特征并进行特征提取，读取积极和消极文本数据，并进行分词，对 积极词与消极词赋予标签作为特征。然后划分80%的数据作为训练集，剩余20%的数据作 为测试集。接着构建朴素贝叶斯分类器，使用训练集进行训练，同时使用测试集进行测试 并验证其准确率，输出信息量较大的10个特征。最后输入评论数据对分类器进行验证。基\n\n于朴素贝叶斯分类的情感分析的过程如代码9-2所示。\n\n代码9-2  基于朴素贝叶斯分类的情感分析的过程\n\nimport nltk.classify as cf\n\nimport nltk.classify.util as cu\n\nimport            jieba\n\ndef setiment(sentences):\n\n#文本转换为特征和特征提取\n\npos   data  =[]\n\nwith                            open('../data/pos.txt','r+',encoding='utf-8')as                            pos:# 读取积极评论\n\nwhile True:\n\nwords =pos.readline()\n\nif             words:\n\npositi     ve    ={}#        创建积极评论的词典\n\nwords                       =jieba.cut(words)#对评论数据进行jieba    分词\n\nfor           sitisd:]=True\n\npos_data.append((positive,'POSITIVE'))#                           对积极词赋予POSITIVE 标签\n\nwords  =neg.readline()\n\nif              words:\n\n154\n\n第⑨章 文本情感分析\n\nnegative             ={)  # 创建消极评论的词典\n\nwords                      =jieba.cut(words)#  对评论数据进行 jieba    分 词\n\nfor          word           in           words:\n\nnegative[word]=True\n\nneg_data.append((negative, 'NEGATIVE¹))#           对消极词赋予\n\nNEGATIVE  标 签\n\nelse:\n\nbreak\n\ns分_0g8 的_n)的_ )*0.8),int(len(neg_data)*0.8)\n\ntrain_data                                              =pos_data[:pos_num]+neg_data[:neg_num]#抽取80名的数据\n\ntest_data                 =pos_data[pos_num:]+neg_data[neg_num:]# 剩余20%的数据\n\n#构建分类器(朴素贝叶斯)\n\nmodel   =cf.NaiveBayesClassifier.train(train_data)\n\nac    =cu.accuracy(model,test_data)\n\nprint( '准确率为：!+ str(ac))\n\ntops                                =model.most_informative_features()#  信息量较大的特征\n\nprint('              \\n    信 息 量 较 大 的 前 1 0 个 特 征 为 ： ')\n\nfor top in tops[:10]:\n\nprint(top[0])\n\nfor  sentence  in  sentences:\n\nfeature             ={}\n\nwords                        =jieba.cut(sentence)\n\nfor word  in words:\n\nfeature[word]=True\n\npcls                          =model.prob_classify(feature)\n\nsent=pcls.max()#                    情绪面标签(POSITIVE   或 NEGATIVE)\n\nprob                       =pcls.prob(sent)#情绪程度\n\nprint('\\n','',sentence,'',        '的情绪面标签为',sent,  '的概率为\n\n','号.2f%S'   round(prob    *100,2))\n\nif       name    =='          main_':\n\n#测试\n\nsentences             =['破烂平板’,‘手感不错，推荐购买’,‘刚开始还不错，但是后面越来越卡，\n\n差评',‘哈哈哈哈，我很喜欢’,'今天很开心']\n\nsetiment(sentences)\n\n运行代码9-2后，输出结果如下。\n\n准确率为：0.8432956381260097\n\n信息量较大的前10个特征为：\n\n刚买\n\n月\n\n差评\n\n退\n\n失望\n\n联系\n\n不卡\n\n结果\n\n155\n\nPython 中文自然语言处理基础与实战\n\n输出结果分别为准确率、信息量较大的前10个特征、文本情感正负面情绪判断和概率 值。结果显示，测试数据的准确率约为84.33%,从信息量较大的前10个特征中发现大众 对产品的评价不高，分类器的分类结果较好。由于语料是来自只对某个产品的评价，因此 适用的范围也限制于相关内容的文本。如果采用书评评论进行测试，判断正负面情绪时可\n\n能会发生错判，这也是基于机器学习的缺点。由于机器学习的方法依赖于语料库，因此\n\n训练数据时应尽量使用较为全面的语料库。\n\n9.4.2  基于 SnowNLP 库的情感分析\n\nSnowNLP   库是一个用 Python  编写的类库，可以方便地处理中文文本内容。由于 现在大部分的 NLP 库是针对英文的， SnowNLP  库的作者编写了一个方便处理中文的 类库，没有使用 NLTK,  并且所有的算法都是作者自己编写和实现的，还自带了一些\n\n训练好的字典。\n\nSnowNLP 库的主要功能有中文分词、词性标注(原理是3-gram) 、 情感分析、文本分 类(原理是朴素贝叶斯)、转换拼音、繁体转简体、提取文本关键词(原理是 TextRank)、\n\n提取摘要(原理是TextRank) 、 分割句子和文本相似等。\n\nSnowNLP 库中的情感分析选用的语料是购物类评论的数据，对购物类评论的情感 分析准确率较高，也可以自己构建相关领域的语料库去替换 SnowNLP  库中原来的语 料库。 SnowNLP   库中分类器算法选用了朴素贝叶斯算法，有兴趣的读者可以到 SnowNLP  库的官网进行了解。在 SnowNLP 库中调用函数进行情感分析非常简单，如\n\n代码9-3所示。\n\n代码9- 3  在 SnowNLP   库中调用函数进行情感分析\n\n运行代码9-3后，输出结果如下。\n\n156\n\n第 9 章 文本情感分析\n\n返回值为正、负面情绪的概率，越接近于0表示越强的负面情绪，越接近于1表示越\n\n强的正面情绪。\n\n9.5   任务：基于LDA 主题模型的情感分析\n\n9.4节使用了文本分类进行情感分析，本节使用 LDA 主题模型对电商商品的评论进行\n\n情感分析，具体步骤如下。\n\n9.5.1 数据处理\n\n本小节使用的数据是京东某品牌热水器的商品评论，数据量是9669条。在获取数据后，\n\n需要对数据进行一定的处理，如文本去重、语料压缩、短句删除，如代码9-4所示。\n\n代码9-4 数据处理\n\nimport pandas as pd\n\nfrom snownlp import SnowNLP\n\ndata=pd.read_csv('../data/comment.csv',sep=',',encoding='utf-8',header=0)\n\n157\n\ncomment_data\n\n#去除重复值\n\ncomment_data\n\n#短句删除\n\ncomments_data\n\ncomments\n\n=data.loc[:,[   '评论']]#只提取评论数据\n\n=comment_data.drop_duplicates()\n\n=comment_data.iloc[:,0]\n\n=comments_data[comments_data.apply(len)>=、4]# 剔除字数少于4的数据\n\n#语料压缩，句子中常出现重复语句", "metadata": {}}, {"content": "，句子中常出现重复语句，需要进行压缩\n\ndef\n\nyasuo(string):\n\nfor        i        in        [1,2]:\n\nj=0\n\nwhile                j<len(string)-2\n\ni:\n\nif\n\nk\n\nstring[j:j+i]==string[j+i:j+2\n\nstring[j+i:j+2*i]==string[j+i:j+3*i]):\n\n=j+2    *i\n\ni]and(\n\nwhile                         k+i<len(string)and                          string[j:j+i]==string[j:j+\n\n2    *i]:\n\nk     +=i\n\nstring                             =string[:j+i]+string[k                             +i:]\n\nj+=1\n\nfor         i         in          [3,4,5]:\n\nj=0\n\nwhile                    j<len(string)-2                     *i:\n\nif                        stringti:j+i]==string[j+i:j+2                        *i]:\n\nk    =j+2    *i\n\nwhile    k+i<len(string)and    string[j:j+i]==string[j:\n\nj+2         *i]:\n\nk  +=i\n\nstring                        =string[:j+i]+string[k                       +i:]\n\nj+=1\n\nif                                                                      string[:int(len(string)/2)]==string[int(len(string)/2):]:\n\nstring                                        =string[:int(len(string)/2)]\n\nreturn   string\n\nPython 中文自然语言处理基础与实战\n\ncomments                              =comments.astype('str').apply(lambda                               x:yasuo(x))\n\n经过数据处理后，数据量由原来的9669条变为9344条。\n\n9.5.2  模 型 训 练\n\n虽然 LDA 主题模型可以直接对文本做主题分析，但是文本的正面评价和负面评价混淆 在一起，并且由于分词粒度的影响(否定词、程度副词等),可能在一个主题下会生成一些 令人迷惑的词语。因此，将文本分为正面评价和负面评价两个数据集，再分别进行LDA 主\n\n题分析。\n\n在将数据集分为正面评价和负面评价两个数据集后，还需要对数据集分别进行分词和\n\n去停用词处理，然后才可以作为LDA 主题分析的输入数据。\n\n在利用LDA 主题模型进行情感分析时，首先对语句进行 SnowNLP 情感分析，划分语 句的正负面情感倾向。然后对句子分别进行分词、去停用词处理。最后对正负面情感分别 建立 LDA 主题模型并输入数据进行训练，分别输出3个主题。利用LDA 主题模型进行模\n\n型训练，如代码9- 5所示。\n\n代码9-5 模型训练\n\nfrom             gensim             import             corpora,models,similarities\n\n#情感分析\n\ncoms          =[]\n\ncoms =comments.apply(lambda x:SnowNLP(x).sentiments)\n\n#情感分析，coms 在0～1之间，以0.5分界，大于0.5则为正面情感\n\n158\n\npos_data\n\nneg_data\n\n#分词\n\n=comments[coms\n\n=comments[coms\n\n>=0.6]# 正面情感数据集，取0.6是为了加强情感\n\n<0.4]# 负面情感数据集\n\n_t data =sm_bata.app('ieba.cut(x))# 自定义简单分词函数\n\nneg_data =neg_data.apply(mycut)\n\npos_data.head(5)\n\nneg_data.tail(5)\n\nprint(len(pos   data))\n\nprint(len(neg_data))\n\n#去停用词\n\nteoapd=ecsv('../data/stopwords.txt',sep='bucunzai',encoding='utf-8',\n\nstop                                          =['`,'']+list(stop[0])#添加空格符号，pandas     过滤了空格符号\n\npos      =pd.DataFrame      (pos_data)\n\nneg     =pd.DataFrame(neg_data)\n\npos[1]=pos[’            评论!],apply(lambda                                  s:s.split(‘'))#空格分词\n\npos[2]=pos[1].apply(lambda    x:[i    for    i     in    x    if    i    not     in    stop])# 去停用词\n\nneg[1]=neg[     '评论'].apply(lambda                s:s.split(''))\n\nneg[2]=neg[1].apply(lambda        x:[i         for         i         in         x         if        i         not         in         stop])\n\n#正面主题分析\n\npos_dict\n\npos_corpus\n\npos_ 1da\n\n=corpora.Dictionary(pos[2])#建立词典\n\n=[pos_dict.doc2bow(i)for               i                in               pos[2]]#  建立语料库\n\n=models.LdaModel(pos_corpus,num_topics=3,id2word=pos_dict)#LDA\n\n第 9 章 文本情感分析\n\n运行代码9-5后，正面评价和负面评价的3个主题输出如下。\n\npos_topic0\n\n0.050*\"安装\"+0.032*\"不错\"+0.032*\"史密斯\"+0.028*\"买\"+0.022*\"热水器\"+0.022*\n\n“很快”+0.021*“师傅”+0.020*“价格”+0.016*\"送货”+0.013*\"品牌” pos_topicl\n\n0.095*“不错”+0.030*“安装”+0.024*\"热水器”+0.017*”服务”+0.015*\"速度\"+0.012*\n\n“很快”+0.011*“买”+0.010*“满意”+0.010*\"加热”+0.010*\"挺”\n\npos_topic2\n\n0 .053*\"不错\"+0 .051*\"安装\"+0 .043*\"品牌\"+0 .030*\"值得\"+0 .027*\"信赖\"+0 .024* \"东西\"+0.022*“送货”+0.020*“服务”+0.019*\"京东”+0.016*\"满意\"\n\nneg_topic0\n\n0.045*\"安装\"+0.021*\"史密斯”+0.019*\"热水器”+0.015*\"京东\"+0.015*”师傅\"+0.015* \"说\"+0.015*“不”+0.014*”不错”+0.012*\"买”+0.009*\"服务”\n\nneg_topicl\n\n0.038*\"安装\"+0.024*\"师傅\"+0.016*“买”+0.015*”热水器”+0.014*\"史密斯\"+0.013*\n\n\"送货\"+0.007*\"服务态度\"+0.007*\"上门\"+0.007*\"满意”+0.006*\"客服\"\n\nneg_topic2\n\n0.078*“安装”+0.031*“师傅”+0.022*\"热水器”+0.012*\"不\"+0.009*\"不错\"+0.008*\n\n\"说\"+0 .008*\"元”+0 .008*\"服务\"+0 .008*\"买\"+0 .008*\"态度\"\n\n9.5.3  结果分析\n\n根据输出结果，可以得到正面评价每个主题的含义。\n\n在主题0中，关键词有“安装”“不错”“史密斯”“买”“热水器”“很快”“师傅” “价格”“品牌”等关键词", "metadata": {}}, {"content": "，可以得到正面评价每个主题的含义。\n\n在主题0中，关键词有“安装”“不错”“史密斯”“买”“热水器”“很快”“师傅” “价格”“品牌”等关键词，说明主题0主要反映该品牌的热水器有安装师傅在安装、性\n\n价比高。\n\n在主题1中，关键词有“不错”“安装”“热水器”“服务”“速度”“很快”“买”“满意”\n\n“加热”“挺”等词语，说明主题1主要反映该品牌热水器送货快、加热快。\n\n在主题2中，关键字有“不错”“安装”“品牌”“值得”“信赖”“东西”“送货”“服务”\n\n“京东”“满意”,说明主题2主要反映该品牌热水器品牌值得信赖、服务不错。\n\n综合以上正面评价的主题关键词可以看出，该品牌的热水器具有安装师傅安装、性价 比高、送货快、加热快、品牌值得信赖、服务不错等优点。而从负面评价的主题关键字可\n\n以看出，该品牌的热水器安装费用较高、售后服务欠佳等。\n\n159\n\nPython 中文自然语言处理基础与实战\n\n结合该品牌的主题分析结果，可以对该商家提出一些长远性的建议。例如，在保证 热水器质量好、性能强的同时，适当提高上门安装服务人员和售后客服的整体素质，适 当降低零配件材料和上门安装的费用，对上门安装师傅乱收费等违规现象要及时发现并\n\n做出处理等。\n\n小结\n\n本章主要介绍了情感分析的基本概念、常用的情感分析方法和电商评论的情感分 析。首先介绍了情感分析的主要内容和常见应用，然后介绍了基于情感词典、基于文本 分类和基于 LDA  主题模型的情感分析方法，并分别通过实例介绍这3种情感分析方法\n\n的使用。\n\n实训\n\n实 训 1 基于词典的豆瓣评论文本情感分析\n\n1. 训练要点\n\n掌握基于词典对文本进行情感分析的方法。\n\n2. 需求说明\n\n根据《流浪地球》的两条影评“电影比预期要更恢宏磅礴”“煽情显得太尴尬”,基于\n\n词典对文本进行情感分析。\n\n3. 实现思路与步骤\n\n( 1 ) 定 义 seg_word 函数对句子进行分词，仅保存重要词语。\n\n( 2 ) 定 义 sort_word 函数加载词典，计算各词的得分情况。\n\n(3)结合各词的得分情况定义 socre_sentiment  函数，汇总情感词的总得分。\n\n实 训 2  基于朴素贝叶斯算法的豆瓣评论文本情感分析\n\n1.   训练要点\n\n(1)掌握将文本转换为特征和提取特征的方法。\n\n(2)掌握构建朴素贝叶斯分类器的方法。\n\n2. 需求说明\n\n根据《流浪地球》的积极和消极影评文本数据，基于朴素贝叶斯算法对豆瓣影评文本\n\n进行情感分析。\n\n3. 实现思路与步骤\n\n(1)读取文本数据并分词，对积极词与消极词赋予标签作为特征。\n\n(2)划分测试集与训练集。\n\n(3)构建朴素贝叶斯分类器。\n\n(4)输入评论数据对分类器进行验证。\n\n160\n\n第 9 章 文本情感分析\n\n实训3 基于 SnowNLP 的豆瓣评论文本情感分析\n\n1. 训练要点\n\n掌握在 SnowNLP 中调用函数进行情感分析的方法。\n\n2. 需求说明\n\n根据《流浪地球》的两条影评“电影比预期要更恢宏磅礴”“华语真正意义上的第一部\n\n科幻大片”,在 SnowNLP  中调用函数对影评进行情感分析。\n\n3. 实现思路与步骤\n\n(1)创建 SnowNLP对象，设置要测试的语句。\n\n( 2 ) 调 用sentiments  函数获取积极情感概率。\n\n实 训 4 基于 LDA 主题模型的豆瓣评论文本情感分析\n\n1.  训练要点\n\n掌握使用LDA  主题模型对电商商品的评论进行情感分析的方法。\n\n2. 需求说明\n\n根据《流浪地球》的两条影评“电影比预期要更恢宏磅礴”“华语真正意义上的第一部\n\n科幻大片”,基于LDA  主题模型对文本进行情感分析。\n\n3. 实现思路与步骤\n\n(1)对文本数据进行文本去重、语料压缩、短句删除处理。\n\n(2)进行 SnowNLP情感分析，划分语句的正负面情感倾向。\n\n(3)对句子分别进行分词、去停用词处理。\n\n(4)对正负面情感分别建立LDA 主题模型并输入数据进行训练，再分别输出3个主题。\n\n课后习题\n\n1. 选择题\n\n(1)情感分析的基础性工作是(    )。\n\nA.  文本信息抽取               B.   文本的主客观分类\n\nC. 情感分类                    D. 情感极性判断\n\n(2)基于机器学习的情感分类，关键在于特征选择、(    )、分类模型。\n\nA.  标记词性    B.   特征提取     C.   特征权重量化    D.  情感极性判断\n\n(3)不属于情感分析应用的是(    )。\n\nA.  信息检索   B.  远程通信   C.   机器翻译         D.  语音识别\n\n(4)情感分析技术的核心问题是(    )。\n\nA. 情感分类    B. 信息预测    C. 舆情分析        D.  文本抽取\n\n161\n\nPython 中文自然语言处理基础与实战\n\n( 5 ) 基 于LDA 主题模型的文本情感分析不包括(    )。\n\nA.   文本转换                     B.   主题提取和情感词提取\n\nC. 主题情感摘要生成             D.   系统评测\n\n2. 操作题\n\n(1)尝试更换基于情感词典的情感得分的计算公式，计算相似的情感得分，并给出程\n\n序和运行结果。\n\n(2)参考文本分类的情感分析，选用机器学习中几个常用的分类算法，并对这几个算\n\n法的结果进行比较。\n\n162\n\n 第10章 NLP  中的深度学习技术\n\n随着近几年深度学习的兴起，许多国内外的学者将深度学习技术应用于自然语言生成 和自然语言理解方面的研究，并取得了一些突破性的成果。其中比较有代表性的是 Sequence to Sequence(Seq2Seq)模型，它是目前自然语言处理技术中一种重要且流行的模型。该技 术突破了循环神经网络的限制，将经典的循环神经网络模型运用于机器翻译、智能问答这 一类序列型任务，有着非常突出的表现。本章主要介绍循环神经网络模型、长短期记忆网 络模型、Seq2Seq  模型和常见的深度学习工具，以及使用它们实现文本分类、情感分析和\n\n机器翻译的案例。\n\n学习目标\n\n(1)了解循环神经网络的基本概念。\n\n(2)熟悉循环神经网络模型、长短期记忆网络模型和 Seq2Seq模型的结构。 (3)了解常用的深度学习工具。\n\n(4)掌握使用本章介绍的工具实现文本分类、情感分析和机器翻译的方法。\n\n10.1 循环神经网络概述\n\n在传统的神经网络模型中，从输入层到隐含层再到输出层，层与层之间是全连接的， 每层内部的节点是无连接的，这种神经网络处理时间序列问题的效果很差。在NLP 中，有 时需要预测句子的下一个词语是什么,由于句子中的前后词语并不是独立的，传统的神经\n\n网络模型处理这种预测问题就比较困难。\n\n循环神经网络(Recu rrent   Neural    Network,RNN) 是一种特殊的神经网络结构，它是 根据“人的认知基于过往的经验和记忆”这一观点提出的。RNN 不仅考虑前一时刻的输入， 而且赋予网络对以往内容的一种“记忆”功能。RNN 是一类以序列数据为输入，在序列的 演进方向进行递归，并且所有节点按链式连接的递归神经网络。在 RNN 中，神经元不但可\n\n以接收其他神经元的信息，还可以接收自身的信息，形成具有环路的网络结构。\n\nRNN 的研究始于20世纪80年代，并在21世纪初发展为深度学习算法之一，其中常 见的RNN有双向循环神经网络 (Bi-directional   RNN,Bi-RNN)  和长短期记忆网络(Long\n\nShort-Term Memory Network,LSTM 网络)。\n\nRNN 适合用于处理视频、语音、文本等与时序相关的问题。常见的应用领域有文本生\n\nPython 中文自然语言处理基础与实战\n\n成、语言模型、图像处理、机器翻译、语音识别、图像描述生成、文本相似度计算、音乐\n\n推荐和商品推荐等。\n\n10.2 RNN  结 构\n\nRNN 结构按输入和输出的序列长度可划分为多对一、等长的多对多和非等长3种\n\n164\n\n结构。\n\n10.2.1\n\n多对一结构\n\nRNN 的多对一结构是指输入是一个序列，输出是一个单独的值而不是序列", "metadata": {}}, {"content": "，输出是一个单独的值而不是序列，如图10-1 所示。这种结构通常用于处理序列分类问题，如输入一段文字判别它所属的类别，输入一\n\n个句子判断其情感倾向等。\n\n图10-1 RNN  的多对一结构\n\n图10-1 中变量和符号的含义如下。\n\n(1)x;,x₂,…,x,      为输入，y 为输出。\n\n(2)结构图中圆圈和方块表示的是向量。\n\n(3)不同步骤中的网络连接的权重矩阵U 、W 、V   都是一样的，即每个步骤的参数都\n\n是共享的。\n\n10.2.2  等长的多对多结构\n\n等长的多对多结构的输入和输出序列是等长的，这种结构是RNN 的经典结构，可以用 于生成文章、诗歌，甚至代码。等长的多对多结构如图10-2所示。序列当前的输出只与序 列上一时刻的输出有关，具体的表现形式为网络对上一时刻的信息进行记忆并应用于当前 输出的计算中。隐藏层之间的节点是单向连接，隐藏层的输入包括输入层的输出和上一时\n\n刻隐藏层的输出。\n\n图10-2 等长的多对多结构\n\n第 0 章 NLP 中的深度学习技术\n\n图10-2中变量和符号的含义如下。\n\n(1)x;,x₂,…,x,     为输入，y,,y₂,…,y,  为输出，输入和输出序列必须要是等长的。\n\n(2)一个箭头表示对该向量做一次变换。例如， 和x;分别有一个箭头连接，表示对\n\nh₀和x;各做一次变换。\n\n1.RNN   语言模型\n\n将等长的多对多结构应用于语言模型时，需要将词依次输入网络中，每输入一个词， RNN  语言模型将输出目前下一个最可能的词。例如，当依次输入“我”“想”“外出”“旅 游”时，RNN 语言模型的输入与输出如图10-3 所示，其中S 和 E 是两个特殊的词，分别\n\n表示序列的开始与结束。\n\n图10-3 RNN 语言模型的输入与输出\n\nRNN 语言模型可以描述为给定词序列xj,x₂,…,x ,,   通过计算式(10-1),预测序列下\n\n一个词x    的概率。\n\np(x=v)x ,…,x;)                                     (10-1)\n\n由 于RNN 只能接收数字向量，无法直接接收语言文本，因此需要将词表达为向量的形\n\n式。为了将词向量化，需要执行下面的步骤。\n\n(1)建立一个包含所有词的词典，每个词在词典中具有唯一的编号。\n\n(2)使用K 维的独热向量表示词典中的每一个词，其中K 为词典中包含词的个数。\n\n设RNN  的每个输入单元的输入是长度为K 的向量，输入的序列长度为T, 隐藏层神经\n\n元的个数为H,   输出向量的长度为K,   则 RNN 的输入、输出和隐藏层如图10-4所示。\n\n图10-4  RNN 的输入、输出和隐藏层\n\n165\n\nPython 中文自然语言处理基础与实战\n\nRNN 参数和变量之间的关系如下。\n\n(1)t 时刻输入的x,为独热向量，x,∈R*。\n\n(2)输入层与隐藏层之间的共享权重矩阵为U(U∈RH×K)。\n\n(3)上一个时刻隐藏层与当前时刻隐藏层之间的共享权重矩阵为W(W∈RxH)。\n\n(4)隐藏层与输出层之间的共享权重矩阵为V(V∈R×H) 。\n\n(5)t   时刻隐藏层输入为s,(s,∈R),           如式(10-2)所示。\n\nS,=Wh_+Ux,+b                                                (10-2)\n\n(6)t   时刻隐藏层的输出为h,(h∈R  ^*),   如式(10-3)所示。其中， σ为sigmoid\n\n函数。\n\nh,=σ(s,)                                                          (10-3)\n\n(7)输出层的输入为o,(o,∈R  K×1), 如式(10-4)所示。\n\no₁=Vh,+c                                                     (10-4)\n\n(8)t   时刻经过softmax层的输出为y,(y,∈R   *×1), 如式(10-5)所示。\n\ny,=g(o,)                                                       (10-5)\n\n2. 模型的训练\n\n设 t 时刻经过softmax 层的输出为y,,   实际样本对应的词向量为， (j,∈R×) 。RNN\n\n的前向传播过程如图10-5所示。\n\n166\n\nY₁-1\n\nY₁\n\nY+I\n\nL                    L₄                L₄\n\nY₁-1\n\nJ₁\n\nY+I\n\nO₁-1\n\nV\n\n0₁\n\nV\n\n0+1\n\nV\n\nh\n\nW\n\nU\n\nX₁-1\n\n图10-5\n\nh₂            h₃\n\nW                  W\n\nU                U\n\nx;                X₁+\n\nRNN 的前向传播过程\n\n使用交叉熵计算模型输出y,与实际样本，的误差，如式(10-6)所示。\n\n(10-6)\n\n第10章 NLP 中的深度学习技术\n\n167\n\n总的损失函数定义为所有交叉熵的均值，如式(10-7)所示。\n\n(10-7)\n\nRNN  中的权重矩阵U 、W 、V    是未知参数，这几个参数需要通过模型训练获得。模\n\n型训练的过程如下。\n\n(1)加载一个大的文本语料库。\n\n(2)把语料库放到RNN 语言模型中，计算每个时刻t 的输出。\n\n(3)计算损失函数。\n\n( 4)将反向传播应用于RNN,  计算函数的导数。\n\n由于在整个语料库上计算需要花费大量的时间，在实际中常采用随机梯度下降法批量\n\n计算损失和梯度然后进行更新。\n\n这些年研究者们又提出了多种复杂的RNN 去改进RNN 模型的性能，常见的有Bi-RNN\n\n和 LSTM 模型。\n\n3. 双向 RNN 结构\n\n基本的 RNN 只考虑了预测词前面的词，即只考虑了上下文中的“上文”,并没有考虑 该词后面的内容。这可能会错过一些重要的信息，使预测的内容不够准确。双向 RNN 不仅 从前往后保留该词前面的词的信息，而且还从后往前保留该词后面的词的信息，然后基于 这些信息预测该词。例如，如果预测一个语句中缺失的词语，那么需要根据上下文来进行 预测。双向RNN 是由两个 RNN 上下叠加在一起组成的，输出由这两个 RNN 的隐藏层的\n\n状态决定。双向RNN 的结构如图10-6所示。\n\n图10-6 双向 RNN 的结构\n\n4. 多层 RNN 结构\n\nRNN 的另一种改进是多层 RNN, 其结构和双向 RNN 类似，只是对每一步的输入增加 了多层网络。该网络有更强大的表达与学习能力，但是复杂性也提高了，同时需要更多的\n\n训练数据。三层RNN 结构示例如图10-7所示。\n\nPython 中文自然语言处理基础与实战\n\nyri           y;           Y-t\n\nX₇-t                  X,                X₇I\n\n图10-7 三 层 RNN 结构示例\n\n多层 RNN 每一层的参数和基本的 RNN 结构一样，参数共享且不同层的参数一般不共 享。与经典的RNN 结构相比，多层RNN 的泛化能力更强，但是训练的时间复杂度和空间\n\n复杂度也更高。\n\n5.LSTM   结构\n\n基本的RNN 参数通过基于时间的反向传播 (Back   Propagation   Through   Time,BPTT)   算法实现，也就是将输出端的误差值反向传递，运用梯度下降法进行更新。需要注意的是， 在训练中，因为 BPTT 算法会带来梯度消失或梯度爆炸问题，所以 BPTT 算法无法解决长 距离依赖问题。RNN 结构面对“长距离依赖”时性能就开始变差，虽然理论上可以通过细 调参数来解决，但是在实践中这个问题很难克服，RNN 很难学习到长距离依赖的信息。LSTM\n\n是 RNN 的一种变形", "metadata": {}}, {"content": "，也就是将输出端的误差值反向传递，运用梯度下降法进行更新。需要注意的是， 在训练中，因为 BPTT 算法会带来梯度消失或梯度爆炸问题，所以 BPTT 算法无法解决长 距离依赖问题。RNN 结构面对“长距离依赖”时性能就开始变差，虽然理论上可以通过细 调参数来解决，但是在实践中这个问题很难克服，RNN 很难学习到长距离依赖的信息。LSTM\n\n是 RNN 的一种变形，是为了克服RNN 无法很好处理长距离依赖而提出的。\n\nLSTM  对 RNN 结构进行了改进，使RNN  具备避免梯度消失的特性，从而让RNN  自身 具备处理长期序列依赖的能力。LSTM  网络对梯度消失采用了特殊的方式存储“记忆”,以前 梯度比较大的“记忆”不会像简单的RNN 那样马上被抹除，可以在一定程度上克服梯度消失 问题。 LSTM  网络通过梯度剪裁技术克服梯度爆炸问题，当计算的梯度超过阈值 c 或者小于 阈值-c 的时候，便把此时的梯度设置成c 或-c 。3 个神经网络的LSTM 内部结构如图10-8所 示，图中×表示乘法，+表示加法， tanh 表示 tanh 函数，σ表示 sigmoid 函数；把数据压缩到\n\n0到1的范围内，0表示信息无法通过该层，1表示信息可以全部通过。\n\n图10-8   3个神经网络的 LSTM  内部结构\n\n168\n\n第⑩章 NLP  中的深度学习技术\n\n由于LSTM 神经网络模型使用门结构实现了对序列数据中的遗忘与记忆，它不仅能够 刻画出输入数据中的短距离的相关信息，还能够捕捉到具有较长时间间隔的依赖关系，因 此能够很好地应用于文本数据的处理。使用大量的文本序列数据对LSTM  模型训练后，可\n\n以捕捉文本间的依赖关系，训练好的模型就可以根据指定的文本生成后续的内容。\n\n10.2.3  非等长结构 (Seq2Seq 模型)\n\nSeq2Seq 模型是由谷歌大脑团队和约书亚·本吉奥所在的团队在2014年各自独立提出 的模型结构，主要用于解决机器翻译问题。最基础的Seq2Seq 模型包含3个部分，分别为 编码器 (Encoder) 、 解码器 (Decoder)    和连接两者的中间状态向量 C,   如图10-9所示。 Encoder 通过学习输入，将其编码成一个固定大小的状态向量 C,  继而传给 Decoder,Decoder\n\n再通过对向量C 的学习进行输出。\n\n图10-9 Encoder-Decoder 结 构 1\n\n经典的 RNN 结构要求序列等长，然而大部分问题序列是不等长的。例如，在机器翻译 中，源语言和目标语言的句子往往没有相同的长度。Seq2Seq  模型的 Encoder  和 Decoder 分别由两个RNN 构成，其中 Encoder 的结构如图10-10所示，模型先将输入数据通过第一\n\n个 RNN 编码成一个上下文向量 C。\n\n图10-10 Encoder 的结构\n\n得到C 之后，用另一个RNN 对其进行解码，这部分RNN 被称为 Decoder。一种做法是将\n\nC 当作初始状态输入Decoder  中，此时 Encoder-Decoder   的结构如图10-11 所示。\n\n图10-11 Encoder-Decoder 结构2\n\n169\n\nPython 中文自然语言处理基础与实战\n\n另一种做法是将C 当作每一步的输入，此时 Encoder-Decoder  的结构如图10-12所示。\n\n图10-12 Encoder-Decoder 结构3\n\n由于 Encoder-Decoder 结构没有输入和输出要等长的限制，因此应用的范围非常广泛。\n\n(1)机器翻译。这是Encoder-Decoder  的经典应用之一，事实上这一结构就是在机器翻\n\n译领域最先提出的。\n\n(2)文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。\n\n(3)阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。\n\n(4)语音识别。输入是语音信号序列，输出是文字序列。\n\n1.Attention    机制\n\n在 Encoder-Decoder  结构中， Encoder  把所有的输入序列都编码成一个统一的语义特征 向量C 再解码。因此，向量 C 中必须包含原始序列中的所有信息，它的长度就成了限制模 型性能的瓶颈。例如，在机器翻译问题中，当翻译的句子较长时， 一个向量C 可能存不下\n\n那么多信息，就会造成翻译精度的下降。\n\n注意力 (Attention)    机制通过在解码 (Decode)    过程中对每个节点输入不同的向量 C\n\n来解决这个问题，带有 Attention 机制的 Encoder-Decoder结构如图10-13所示。\n\n图10-13 带有 Attention 机制的 Encoder-Decoder 结构\n\n2.Seq2Seq    模型的目标函数\n\n用x={xj,x₂,…,x,}      代表输入的语句， y={yγ,y₂,…,y.}       代表输出的语句， y, 代表当\n\n170\n\n第 0 章 NLP 中的深度学习技术\n\n前输出词。Seq2Seq 模型的目标函数如式(10-8)所示。\n\n(10-8)\n\n即输出的y,不仅依赖之前的输出{y₁,y₂,…,y,},   还依赖输入语句x。\n\n式(10-8)存在数值下溢问题，原因是式(10-8)中每一项p(y,|yj,y₂,…,y,_,x)都小 于1,甚至远远小于1,乘起来会得到很小的数字。因此，在实际中一般取 log值，求其概\n\n率的对数和而不是概率的乘积。Seq2Seq模型最大化目标函数如式(10-9)所示。\n\n                      (10-9)\n\n在深度学习初始阶段，每个深度学习研究者都需要编写大量的重复代码。为了提高工 作效率，有些研究者就将这些代码写成了一个框架放到互联网供研究者使用。于是，网上 就出现了不同版本的深度学习框架，使用率较高的几个框架从而流行了起来。目前，最为\n\n流行的深度学习框架有Paddle、TensorFlow、Caffe、Theano、MXNet、Torch 和 PyTorch。\n\n深度学习框架提供了一系列的深度学习的组件，降低了入门的门槛，不需要从复杂的 神经网络开始编写代码。可以根据需要选择已有的模型，通过训练得到模型参数；也可以 选择自己需要的分类器和优化算法，然后调用深度学习框架的函数接口使用用户自定义的\n\n新算法。\n\n10.3.1    TensorFlow  简介\n\nTensorFlow 是一个使用数据流图进行数值计算的开源软件库，是谷歌公司研发的第二 代人工智能学习系统，名称源于其本身的运行原理。Tensor(张量)表示 N 维数组，Flow (流)表示基于数据流图的计算，TensorFlow为张量从数据流图的一端流动到另一端的计算\n\n过程。TensorFlow 将复杂的数据结构传输至人工智能神经网络进行分析和处理。\n\nTensorFlow 是全世界使用人数最多、社区最为庞大的一个框架，其维护与更新比较频 繁，并且有着 Python和 C++的接口。TensorFlow的学习资料也非常完善，很多论文和项目\n\n也是基于 TensorFlow 编写的。\n\n1.TensorFlow     的特点\n\nTensorFlow的特点如下。\n\n(1)高度的灵活性。TensorFlow 是一个“神经网络”库，用户可以自己用Python 描绘\n\n计算图，然后放到计算核心之中。\n\n(2)可移植性。TensorFlow 可以在 CPU 、GPU上运行，如台式计算机、服务器、集群 和移动设备。可以在单机上简单地验证想法，而在需要大规模计算的时候可以将其简单地\n\n扩展到集群上，最后将训练好的模型放到移动设备上。\n\n(3)综合了科研和产品。以往的机器学习算法的研究中往往需要编写大量代码，而现\n\n在可以通过 TensorFlow 简单地验证想法，并直接输出产品。\n\n(4)自动计算梯度导数。TensorFlow 可以自动计算函数导数，用户不必纠结于具体的\n\n171\n\nPython 中文自然语言处理基础与实战\n\n求解细节，只需关心模型的定义与验证。\n\n(5)性能最优化。TensorFlow 底层为线程、队列、异步操作给予了良好的支持，使其 可以很好地发挥出硬件的全部性能。而在多计算单元控制上，可以将不同的计算任务分配\n\n到不同的单元之中。\n\n(6)多语言支持。TensorFlow 支持C++、Python、Java 、Go、JavaScript API。\n\n2.Windows  环境下 TensorFlow 的安装\n\nTensorFlow 既支持 CPU,  又支持 CPU+GPU。前者的环境需求简单", "metadata": {}}, {"content": "，只需关心模型的定义与验证。\n\n(5)性能最优化。TensorFlow 底层为线程、队列、异步操作给予了良好的支持，使其 可以很好地发挥出硬件的全部性能。而在多计算单元控制上，可以将不同的计算任务分配\n\n到不同的单元之中。\n\n(6)多语言支持。TensorFlow 支持C++、Python、Java 、Go、JavaScript API。\n\n2.Windows  环境下 TensorFlow 的安装\n\nTensorFlow 既支持 CPU,  又支持 CPU+GPU。前者的环境需求简单，后者需要额外的 支持。TensorFlow是基于VC+2015  开发的，所以需要下载并安装 Visual C++Redistributable\n\nfor Visual Studio 2015  获取 MSVCP140.DLL的支持。\n\n在 Windows 环境下安装 TensorFlow 的步骤如下。\n\n(1)在“开始”菜单中找到 “Anaconda  Prompt”, 单击运行。\n\n(2)在安装 TensorFlow 之前，先将 TensorFlow 源更换为国内镜像(安装速度更快)。\n\n在终端输入如下命令。\n\n(3)使用 “python --version” 命令检查新环境中的Python 版本。\n\n(4)使用 “pip install tensorflow=2.0.0-alpha0” 命令安装 Python 版本对应的 TensorFlow\n\n版本 (Python3.8  对应的版本为 TensorFlow2.0及以上)。\n\n(5)等待安装完成后，验证一下 TensorFlow是否安装成功。先在终端输入 “python” 命令， 进入 Python  交互界面，然后输入 “import  tensorflow  as  tf” 命令。若没有报错，则说明\n\nTensorFlow 安装成功。\n\n10.3.2  基于 TensorFlow 的深度学习库 Keras\n\nKeras 是一个高层神经网络 API,  是一个上层封装的工具库。Kears 由纯 Python 编写而 成，是一个基于TensorFlow 、Theano 、MXNet 和 CNTK的框架。Keras非常容易上手，适 用于小型环境(实验室、数据竞赛)。Keras 具有简易和快速的原型设计、支持CNN 与 RNN\n\n以及能够无缝在 CPU 与 GPU 间切换的优点。\n\n为了避免因版本升级带来的一些功能函数的变化的修改，可以选择安装指定版本的 Keras。如果安装的 Python 版本为3.8,TensorFlow 版本为2.0及以上，则需要安装的Keras\n\n版本为2.3.1及以上，可使用 “pip install keras==2.3.1” 命令进行安装。\n\n10.4   任务：基于LSTM的文本分类与情感分析\n\n本节通过 LSTM网络实现文本分类和情感分析，借助 Seq2Seq模型实现机器翻译。\n\n10.4.1 文本分类\n\n文本分类模型采用两层 LSTM  网络，训练数据集为 THUCNews 的一个子集。文本分\n\n类主要包括以下4个步骤。\n\n(1)语料预处理。读取 THUCNews 数据，并进行语料预处理的工作，包括统计词频、\n\n172\n\n第 ⑩ 章 NLP  中的深度学习技术\n\n生成字库、根据字库将每个文字转化为一个向量。\n\n(2)模型构建。定义LSTM  网络框架，设置相关参数后查看模型结构。\n\n(3)模型训练。对模型进行训练并保存。\n\n( 4)模型测试。使用训练好的模型对验证集进行测试和评价。\n\n1. 语料预处理\n\n训练集使用THUCNews  数据中的10个分类，每个分类包含6500 条数据。类别包括体 育、财经、房产、家居、教育、科技、时尚、时政、游戏和娱乐。每个分类中，训练集包\n\n含5000条数据，验证集包含500条数据，测试集包含1000条数据。\n\n语料预处理程序cnews_loaderpy  定义了7个函数： open_file  函数用于打开一个文件； read_file  函数用于读取文件数据； build_vocab   函数用于构建词汇表；readvocab     函数用于 读取上一步存储的词汇表，并将其转换为{词：id}表示； read_category  函数用于将分类目录 固定，并将其转换为{类别：id}表示； to_words  函数用于将一条由 id 表示的数据重新转换为 文字； process_file  函数用于将数据集从文字转换为固定长度的id  序列表示。自定义语料预\n\n处理函数，如代码10-1所示。\n\n代码10 - 1  自定义语料预处理函数\n\nfrom collections import Counter\n\nfrom tensorflow import keras\n\n173\n\n#打开文件\n\ndef\n\nopen_file(filename,mode='r'):\n\n111\n\nfilenam    e: 表示读取/写入的文件路径\n\nmode:'r!or'              w'表示读取/写入文件\n\n111\n\nreturn                                                    open(filename,mode,encoding='utf-8',errors='ignore')\n\n#读取文件数据\n\ndef   read_file(filename):\n\n711\n\nfilename:       表示文件路径\n\nT!T\n\ncontents,labels                           =[],[]\n\nwith   open_file(filename)as   f:\n\nfor line in f:\n\ntry:\n\nlabel,content\n\n割字符串\n\n=line.strip().split('\\t')# 按照制表符分\n\nif                   content:\n\ncontents.append(list(content))\n\nlabels.append(label)\n\nexcept:\n\npass\n\nreturn      contents,labels\n\nPython 中文自然语言处理基础与实战\n\n表_vocab(train_dir,vocab_dir,vocab_size=5000):\n\n111\n\ntrain_dir        : 训练集文件的存放路径\n\nvocab_dir     ; 词汇表的存放路径\n\nvocab_size:        词汇表的大小\n\n111\n\ndata  train,lab =read   file(train   dir)\n\nall   data   =[]\n\nfor  content  in  data_train:\n\nall_data.extend(content)\n\ntt_ea a_(vocab_size -1)\n\nwords,temp=list(zip(*count_pairs))#                     获取 key\n\nwords                             =['<PAD>']+list(words)#添 加一个<PAD>将所有文本pad  为同一长度\n\nopen_file(vocab_dir,mode='w').write('\\n'.join(words)+'\\n')\n\n#读取词汇表\n\ndef read  vocab(vocab  dir):\n\n111    ____                                                                                 ____\n\nvocab_dir:       词汇表的存放路径\n\nwith       open_file(vocab_dir)as       fp:\n\nwords       =[i.strip()for       i       in       fp.readlines()]\n\nword_to_id=dict(zip(words,range(len(words))))\n\nreturn words,word  to   id\n\n#读取分类目录\n\ndef read_category():\n\ncategories=[    '体育',‘财经','房产!,'家居','教育','科技!,'时尚','时政!,\n\n游戏!,'娱乐\n\n到__别i编 e nge(len(categories))))\n\n\treturn     categories,cat_to_id\n\n# 将id   表示的内容转换为文字\n\ndef  to_words(content,words):\n\n11\n\ncontent:i        d 表示的内容\n\nwords:  文本内容\n\nY1\n\nreturn''.join(words[x]for                   x                    in                    content)\n\n#将文件转换为id 表 示\n\ndef     process_file(filename,word_to_id,cat_to_id,max_length=600):\n\n11\n\nfilename: 文件路径\n\n174\n\n第 0 章 NLP   中的深度学习技术\n\n调用代码10-1中的自定义函数，加载训练数据、验证数据、测试数据并分别进行预处\n\n理，如代码10-2所示。\n\n代码", "metadata": {}}, {"content": "，加载训练数据、验证数据、测试数据并分别进行预处\n\n理，如代码10-2所示。\n\n代码，10-2  加载数据并进行预处理\n\nimport  os\n\n#设置数据读取路径和模型、结果保存路径\n\nndi_r dir =os.oin(base_dir,'cnews.train.txt')\n\ntest_dir                                                  =os.path.join(base_dir,'cnews.test.txt')\n\nval_dir                   =os.path.join(base_dir,'cnews.val.txt')\n\nvocab_dir                 =os.path.join(base_dir,'cnews.vocab.txt')\n\nve_edir_path   ./.join(save_dir,'best_validation')\n\n#若不存在词汇表，则重新建立词汇表\n\nvocab_size      =5000\n\nif               not               os.path.exists(vocab_dir):\n\nbuild  vocab(train   dir,vocab   dir,vocab   size)\n\n#读取分类目录\n\ncategories,cat  to   id    =read   category()\n\n#读取词汇表\n\nwords,word_to_id                             =read_vocab(vocab_dir\n\n#词汇表大小\n\nvocab_size    =len(words)\n\n#数据加载\n\nseq_length        =600        #  序列长度\n\n175\n\n#获取训练数据\n\nx  train,y  train\n\n#获取验证数据\n\n=process   file(train   dir,word  to   id,cat  to   id,seq   length)\n\nPython 中文自然语言处理基础与实战\n\n2. 模型构建\n\n训练过程中先对 LSTM  模型的参数进行了设置。词向量维度为128,输入词序列长度 为600,类别数为10,词汇表大小为5000,隐藏层层数为2,隐藏层神经元个数分别为256\n\n个和128个，学习率为10³,每批训练数据个数为64个，总迭代轮次为20次。设置模型\n\n参数并构建模型，如代码10-3 所示。                     \n\n代码10-3  设置模型参数并构建模型\n\nimport tensorflow as tf\n\nfrom  matplotlib.pyplot  import  MultipleLocator\n\n# 构 建LSTM模型\n\ndef      f.keras.Sequential()\n\nmodel.add(tf.keras.layers.Embedding(vocab_size+1,128,input_length=600))\n\n# 使 用LSTM的单向循环神经网络\n\nmodel.add(tf.keras.layers.LSTM(128))\n\n#标准化处理\n\nmodel.add(tf.keras.layers.BatchNormalization(epsilon=le-6,axis=1))\n\ndayers.Dense(256,activation='relu'))\n\n#d  ro pout   正则化，随机丢弃30%的神经元，防止过拟合\n\nmodel.add(tf.keras.layers.Dropout(0.3))\n\ndl.Dense(128,activation='relu'))\n\n#全连接层，激活函数为softmax\n\nmodel.add(tf.keras.layers.Dense(10,activation='softmax'))\n\nreturn model\n\n#模型实例化\n\nmodel                  =TextRNN()\n\n设置完模型参数后查看模型架构，并通过可视化方法将模型的输入与输出生成列表，\n\n如代码10-4所示。\n\n代码10-4 查看模型架构\n\n#查看模型架构\n\nprint(     '模型的架构为：\\n',model.summary()\n\n运行代码10-4后，输出结果如下。\n\n176\n\n第 0 章 NLP  中的深度学习技术\n\n由结果可知，模型的第一层为嵌入层 (Embedding),    输入词向量的长度为600,神经 元个数为128个，参数的个数为640128个；第二层为长短期记忆(LSTM)    网络，神经元 个数为128个，参数个数为131584个；第三层为批处理标准化层 (BatchNormalization), 神经元个数为128个，参数个数为512个；第四层和第五层是全连接层(Dense)   和 Dropout  正则化层，神经元个数分别为256个和128个，参数个数分别为33024个和32896个；最\n\n后一层是全连接层(Dense), 输出的维度为10,参数个数为1290个。\n\n3. 模型训练\n\n对构建好的模型进行训练参数的设置，设置损失值指标为 categorical_crossentropy,  优 化器为 RMSprop,  评价指标为 categorical_accuracy 。然后进行模型训练，设置训练次数为\n\n20次，如代码10-5所示。\n\n177\n\nPython 中文自然语言处理基础与实战\n\ndef plot   ace   loss(history):\n\nItt\n\nhistory       : 模型训练的返回值\n\n+11\n\nplt.subplot(121)\n\nplt.   title(      ' 准确率趋势图')\n\nplt.plot(range(1,21),history.history['categorical   accuracy'], linestyle='-',color=g',label=                     '训练集')\n\nplt.plot(range(1,21),history.history['val   categorical   accuracy'],\n\n178\n\nlinestyle='-.',color='b',label=\n\nplt.legend(loc='best')#\n\n'测试集')\n\n设置图例\n\n#x   轴按1刻度显示\n\nx_major_locator =MultipleLocator(1)\n\nax                =plt.gca()\n\nkis._rt(hx'_,wmha)',labelsize=7)\n\nplt.xlabel( plt.ylabel(\n\n'迭代次数(次))\n\n'准确率)\n\nplt.subplot(122)\n\niot(r(a'history.history['loss'],linestyle=-',color='g', label=  p'lrange(1,21),history.history['val_loss'],linestyle='-.',\n\ncolor='b',label=                  '测试集')\n\nplt.legend(loc='best')\n\n =MultipleLocator(1)\n\nax.xaxis.set_major_locator(x_major_locator)\n\nplt.tick_params(axis='both',which='major',labelsize=7)\n\nplt.xlabel        ( '迭代次数(次)')\n\nplt.ylabel(       '损失值')\n\nplt.tight   layout()\n\nplt.show()\n\nplot   acc   loss(history)\n\n运行代码10-5后，得到的结果如图10-14所示。\n\n由图10-14可以看出，在第6个周期时，模型在测试集上的效果产生较大的变化。随 着迭代次数的上升，模型在训练集上的准确率逐渐上升，损失值逐渐下降，模型预测效果\n\n越来越好。保存训练好的模型，如代码10-6所示。\n\n第 0 章  NLP 中的深度学习技术\n\n179\n\n准确率趋势图\n\n迭代次数(次)\n\n损失趋势图\n\n迭代次数(次)\n\n图10-14  模型训练的结果\n\n代码10-6 模型保存\n\n4. 模型测试\n\n调用训练好的模型进行测试，如代码10-7所示。\n\n代码10-7 模型测试\n\nPython 中文自然语言处理基础与实战\n\n在代码10-7中，使用sklearn 库下的 metrics 模型评价方法对模型的预测结果进行评价，\n\n包括详细的混淆矩阵，得到的模型评价结果如下，混淆矩阵如图10-15所示。\n\n真实标签\n\n图10-15 混淆矩阵\n\n由代码10-7得到的模型评价结果可以看出，模型在测试集上的准确率 (accuracy)    达 到了0.96,且各类的精确率 (precision) 、 召回率 (recall)    和 F1  值 (fl-score),      除了家居\n\n(编号为3)的精确率，都超过了0.9。\n\n180\n\n第 0 章  NLP  中的深度学习技术\n\n10.4.2  情感分析\n\n数据来源于某电商平台的热水器评论数据", "metadata": {}}, {"content": "，模型在测试集上的准确率 (accuracy)    达 到了0.96,且各类的精确率 (precision) 、 召回率 (recall)    和 F1  值 (fl-score),      除了家居\n\n(编号为3)的精确率，都超过了0.9。\n\n180\n\n第 0 章  NLP  中的深度学习技术\n\n10.4.2  情感分析\n\n数据来源于某电商平台的热水器评论数据，其中正面评论10677 条，负面评论10428 条。本案例的目的是通过构建模型识别各条评论的情感倾向，即进行情感分析，正面评论\n\n为1,负面评论为0。\n\n情感分析步骤如下。\n\n(1)读取正负情感语料。\n\n(2)评论词语向量化。\n\n(3)模型构建。设置 LSTM 模型的参数，嵌入层输入的维度为字典的长度加1,输出 维度为256,输入词的序列长度为50,类别数为2,隐藏层的层数为1,隐藏层的神经元个\n\n数为128个。\n\n(4)模型训练。每批训练数据的个数为16个，总迭代轮次为10。\n\n(5)模型测试。\n\n1. 读取正负情感语料\n\n读取正负情感的语料库，由于情感分析的目的是将数据划分为正面和负面两类，故还 需对正负语料的数据进行“贴标签”处理，正面评论的标签为1,负面评论的标签为0,读\n\n取语料数据如代码10-8所示。\n\n代码10-8 读取语料数据\n\n2. 评论词语向量化\n\n数据为评论文本的形式，不能直接用于建模。因此先对评论数据进行分词处理，计算 每个词语出现的频次，对分词结果进行向量化，变为模型可以识别的数据形式。因为每一 串索引的长度并不相等，所以为了方便模型的训练，需要将索引的长度标准化。这里每条 评论取50个词作为标准，再通过 sklearn 库的 sequence 函数进行标准化。最后将数据划分\n\n为训练集和测试集以便建模训练。将评论文本中的词语向量化，如代码10-9所示。\n\n代码10-9 词语向量化\n\n181\n\nPython 中文自然语言处理基础与实战\n\npn_all['words']=pn_all[0].apply(cut_word)#                                       对情感语料分词\n\ncomment                                   =pd.read_excel('../data/sum.x1s')#  读入评论内容，增加语料\n\ncomment                =comment                 [comment['rateContent'].notnull()]   # 仅读取非空评论\n\ncomment['words']=comment['rateContent'].apply(cut_word)#                                            对评论语料分词 pn_comment=pd.concat([pn_all['words'],comment['words']],ignore_index=True)\n\n#合并所有的数据\n\n 感评论词语向量化\n\nfor  i  i .ent:\n\ndicts                                       =pd.DataFrame(pd.Series(w).value_counts())#  建立统计词典\n\ncts i,ran (除1临,lni ent\n\nget_sent       =lambda       x:list(dicts['id'][x])\n\npn_all['sent']=pn_all['words'].apply(get_sent)\n\n#评论词语向量标准化，对样本进行padding    ( 填 充 ) 和truncating         (修剪)\n\nanxle_ll[nt'= _sequences(pn_all['sent'],maxlen=maxlen))\n\n#正负情感评论词语向量化\n\n#训练集、测试集\n\nx   all      =np.array(list(pn   all['sent']))\n\ny   all =np.array(list(pn   all['mark']))\n\nx  train,x  test,y  train,y  test      =train  test   split(x   all,y   all,\n\ntest_size=0.25)\n\nprint(      '训练集的特征数据形状为：',x_train.shape)\n\nprint(      '训练集的标签数据形状为：',y_train.shap            e)\n\nprint(      '测试集的特征数据形状为：',x_test. shape)\n\nprint(      '测试集的标签数据形状为：',y_t est.shape)\n\nprin       t(    ' 训 练 集 的 特 征 数 据 为 ： \\n',  x_train\n\n运行代码10-9后，输出结果如下。\n\n训练集的特征数据大小为：(15828,50    )\n\n训练集的标签数据大小为：(15828,)\n\n测试集的特征数据为： (5277,50      )\n\n测试集的标签数据为： (52 77,)\n\n训练集的特征数据为：\n\n[[\n\n[\n\n[\n\n550416426                      346      19 902]]\n\n182\n\n第 0 章  NLP  中的深度学习技术\n\n3. 模型构建\n\n设置 LSTM 模型的参数，嵌入层输入的维度为字典的长度加1,输出维度为256,输 入词的序列长度为50,类别数为2,隐藏层的层数为1,隐藏层的神经元个数为128个。\n\n设置模型参数并构建LSTM 模型，如代码10-10所示。\n\n代码10-10 设置模型参数并构建 LSTM  模型\n\n查看到的模型结构如下。\n\n由结果可知，模型的第一层为嵌入层 (Embedding),    输入词向量的长度为50,神经元个 数为256个，参数的个数为13572096个；第二层为长短期记忆(LSTM)   网络，神经元个数为 128个，参数个数为197120个；第三层为dropout正则化层，神经元个数为128个；第四层为\n\n全连接层(Dense)   并采用sigmoid 函数进行激活，神经元个数为1个，参数个数为129个。\n\n4. 模型训练\n\n搭建好的模型通过compile 设置损失值为 “binary_crossentropy”,    优化器为 “Adam”,\n\n评价指标为 “accuracy”,    每批训练数据个数为16个，总迭代轮次为10。设置模型超参数\n\n并进行模型训练，如代码10-11 所示。\n\n183\n\nPython 中文自然语言处理基础与实战\n\n代码10-11 设置模型超参数并进行模型训练\n\n运行代码10-11后，输出结果如下。\n\n由代码10- 11 的运行结果可以看出，模型在测试集上的准确率 (accuracy)     达到了\n\n99.77%,损失值 (loss)    随着模型的训练不断下降。\n\n5. 模型测试\n\n对训练好的模型使用测试数据进行测试，输出模型的评价结果，如代码10-12所示。\n\n184\n\n第10章 NLP  中的深度学习技术\n\n在代码10-12中，通过sklearn 库的模型评价输出模型的评价结果，运行结果如下。\n\n模型在测试集上的准确率 (accuracy)    达到了0.89,且各类的精确率 (precision) 、 召 回率(recall)    和 F1  值 (fl-score)     的平均值都达到了0.89,可以认为情感分析模型的效果\n\n较好。当然，之后可以通过调整模型的参数以达到更好的效果。\n\n10.5    任务：基于 Seq2Seq 的机器翻译\n\n本节采用 Seq2Seq 模型，借助 GPU构建中英文机器翻译模型。模型构建和训练的环境\n\n配置如下。\n\n(1)Linux    操作系统为 ubuntu  16.04。\n\n(2)GPU     为 Nvidia  GTX  1080Ti。\n\n(3)Python      3.8.3。\n\n(4)TensorFlow     2.3。\n\n机器翻译包括以下5个步骤。\n\n(1)语料预处理。读取原始数据并解析文件，分别对中英文内容进行分词，筛选规定\n\n词数的句子，保存至新文件中。\n\n(2)构建模型。定义LSTM 层，添加 Attention 机制，并配置模型的相关参数。\n\n(3)定义优化器和损失函数。\n\n(4)训练模型。待数据输入构建好的模型进行训练，并查看训练后的模型性能。\n\n(5)翻译。配置测试用的参数，输入句子进行翻译", "metadata": {}}, {"content": "，分别对中英文内容进行分词，筛选规定\n\n词数的句子，保存至新文件中。\n\n(2)构建模型。定义LSTM 层，添加 Attention 机制，并配置模型的相关参数。\n\n(3)定义优化器和损失函数。\n\n(4)训练模型。待数据输入构建好的模型进行训练，并查看训练后的模型性能。\n\n(5)翻译。配置测试用的参数，输入句子进行翻译，对模型进行测试。\n\n10.5.1 语料预处理\n\n原始数据以 TXT 格式存储数据，包含部分中英文句子互译的内容。语料预处理过程\n\n如下。\n\n( 1 ) 利 用io  对原始数据文本内容进行读取，并在每个句子中添加一个开始和结束的\n\n标记。\n\n(2)由于语料数据太多，从语料库选用一部分数据进行数据处理和建模即可。选用数\n\n据后，删除特殊字符以清理句子，并返回格式为[英语，中文]的单词对。\n\n(3)创建一个单词索引和一个反向单词索引(从单词 →id 和 id→ 单词到字典的映射), 构造英语与中文的映射，即将单词转换为数字的字典；构造英语与中文的反向映射，即将\n\n数字转换为单词的字典。\n\n(4)将每个句子填充到最大长度，并创建一个tf.data 数据集，以提升运算速度。\n\n185\n\nPython 中文自然语言处理基础与实战\n\n对文本进行语料预处理，如代码10-13所示。其中，preprocess_sentence 函数用于处理 句子标点符号和添加开始与结束的标记， create_dataset 函数用于删除特殊字符和返回单词\n\n对格式等。\n\n代码10-13 语料预处理\n\nimport       re\n\nimport io\n\nimport tensorflow as tf\n\nfrom sklearn.model   selection import train  test   split\n\n#准备数据集\n\ndef preprocess_sentence(w):\n\n111\n\nw:  句子\n\n111\n\nw =re.sub(r'([?.!,])',r'\\1',w)# 在句子中的标点符号的前后加空格 w=re.sub(r\"['']+\",'',w) #将句子中的多空格去重 w=¹<start>¹+w+¹<end>' #给句子加上开始和结束的标记，以便进行模型预测 return         w\n\nen   sentence ='I like this book'\n\nsp_sentence         ='  我喜欢这本书’\n\nprint(     '预处理前的输出为：','\\n',preprocess_sentence(en_sentence))\n\np'\\r()  '预处理前的输出为；','\\n',str(preprocess_sentence(sp_sentence)),'utf-8',\n\n#清理句子，删除特殊字符，返回格式为[英文，中文]的单词对\n\ndef       create_dataset(path,num_examples):\n\n111\n\npath:  文件路径\n\nnum_e xamples:     选用的数据量\n\n111\n\nrd_pairs=[[preprocess_sentenc ath,eodin UTF 't('\\n')l     in\n\nlines[:num_examples]]\n\nreturn zip(*word_pairs)\n\npath_to_file='../data/en-ch.txt'#                                      读取文件的路径\n\nen,sp                                         =create_dataset(path_to_file,None)# 整合并读取数据\n\n#句子的最大长度\n\ndef max_length(tensor):\n\n111\n\ntensor:      文本构成的张量\n\n111\n\nreturn             max(len(t)for             t             in             tensor)\n\n186\n\n#tokenize\n\n量表示\n\n函数用于对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向\n\n第 0 章 NLP  中的深度学习技术\n\ndef tokenize(lang):\n\nI1t\n\nlang:     待处理的文本\n\nttt\n\nlang_tokenizer         =tf.keras.preprocessing.text.Tokenizer(filters='') lang  tokenizer.fit   on  texts(lang)\n\ntensor   =lang_tokenizer.texts_to_sequences(lang)\n\ntensor    =tf.keras.preprocessing.sequence.pad_sequences(tensor,padding= 'post')\n\nreturn tensor,lang  tokenizer\n\n#创建清理的输入输出对\n\ndef      load_dataset(path,num_examples=None):\n\n111\n\npath:     文件路径\n\nnum_examples:      选用的数据量\n\nTtt\n\n#建立索引，并输入已经清洗过的词语，输出词语对\n\ntarg_lang,inp_lang              =create_dataset(path,num_examples)\n\nt中_词in_量l 所_e进izki i_一lg)\n\n英t_文t,向rg量_ an对g所_g样_lang)\n\nreturn   input_tensor,target_tensor,inp_lang_tokenizer,targ_lang_tokenizer\n\ntxa_mtor,et_en大p小_ targ_lang  =load_dataset(path_to_file,\n\nnum_examples)\n\n算x_目lt量h_的t 度ax(ma_egntghth_in =max_length(target_tensor),max_length(\n\ninput_tensor)\n\n_8 例_和_to集r_val,target_tensor_train,target_tensor_val=\n\ntrain  test   split(input  tensor,target  tensor,test   size=0.2)\n\n#验证数据正确性，也就是输出词与词语映射索引的表示\n\ndef    convert(lang,tensor):\n\n111\n\nlang:     待处理的文本\n\ntensor:      文本构成的张量\n\n111                                                  \n\nfor         t         in         tensor:\n\nif      t       !=0:\n\nprint('&d         ---->8s':(t,lang.index  word[t]))\n\nprint     ('预处理前的输出为： ')\n\nprint(      '输入语言：词映射索引')\n\nconvert(inp_lang,input_tensor_train[0])\n\nprint(     '目标语言：词语映射索引')\n\nconvert(targ_lang,target_tensor_train[0])\n\n187\n\nPython 中文自然语言处理基础与实战\n\n运行代码10-13后，输出结果如下。\n\n10.5.2  构建模型\n\n构建模型包括以下3个步骤。\n\n(1)定义编码器的输入，定义LSTM 层，调用编码器，得到编码器的输出和状态信息； 定义解码器的输入，将编码器输出的状态作为初始解码器的初始状态，添加全连接层，定\n\n义整个模型。\n\n( 2 ) 添 加 Attention    机制。由于原始编解码模型的编码过程会生成 一 个中间向量 C,   用 于保存原序列的语义信息。但是这个向量长度是固定的，当输入原序列的长度比较长时， 向量 C 无法保存全部的语义信息，上下文语义信息受到了限制，这也限制了模型的理解能\n\n力。所以需要使用Attention 机制打破这种原始编解码模型对固定向量的限制。\n\n(3)模型中相关参数的配置。设置训练次数为10次，每批数据量大小为256,每次随\n\n机抽取256条内容。\n\n构建基于 Seq2Seq 的机器翻译模型，如代码10-14所示。其中， Encoder 类编写编码器，\n\nBahdanauAttention  类添加 Attention 机 制", "metadata": {}}, {"content": "，这也限制了模型的理解能\n\n力。所以需要使用Attention 机制打破这种原始编解码模型对固定向量的限制。\n\n(3)模型中相关参数的配置。设置训练次数为10次，每批数据量大小为256,每次随\n\n机抽取256条内容。\n\n构建基于 Seq2Seq 的机器翻译模型，如代码10-14所示。其中， Encoder 类编写编码器，\n\nBahdanauAttention  类添加 Attention 机 制 ，Decoder 类编写解码器。\n\n188\n\n第 0 章 NLP  中的深度学习技术\n\n代码10- 14  构建基于 Seq2Seq   的机器翻译模型\n\n#编码器\n\nclass                            Encoder(tf.keras.Model):\n\ndef         ini t        (self,vocab_size,embedding_dim,enc_units,batch_sz):\n\nsuper(Encoder,self)                              init            ()\n\nself.batch_sz                     =batch_sz    # 每次训练所选取的样本数\n\nself.enc_units              =enc_units              # 神经元数量\n\n#输入层\n\negkeras. re(rendcdi_nugn(ab_size,embedding_dim)\n\nreturn_sequences=True,\n\nreturn   state=True,\n\nrecurrent_initializer='glorot_uniform')\n\ndef                                 call(self,x,hidden);\n\nx=self.embedding(x)\n\noutput,state                                             =self.gru(x,initial_state=hidden)\n\nreturn                              output,state\n\ndef                             initialize_hidden_state(self):\n\nreturn  tf.zeros((self.batch   sz,self.enc  units))\n\n#构建编码器网络结构\n\nencoder               =Encoder(vocab_inp_size,embedding_dim,units,BATCH_SIZE)\n\nl_eh_iut,sample_h er.iltea_t t输_ple_hidden)\n\nprint(     '编码器输出形状：','\\n','(batch_size,sequence_length,units){}'.format\n\n(sample_output.shape))\n\nprint(      '编码器隐藏状态形状：','\\n',!(batch_size,units){}'.format\n\n(sample_hidden.shape))\n\n#Attention 机制\n\nclass                                             BahdanauAttention(tf.keras.layers.Layer):\n\n(nt l )yers.Dense(us)  ()\n\nself.W2    =tf.keras.layers.Dense(units)\n\nself.V=tf.keras.layers.Dense(1)\n\ndef                                            call(self,query,values):\n\n#quer     y 为上次的 GRU 隐藏层\n\n#values        为编码器的编码结果(enc_output)\n\n# 在Seq2Seq 模 型 中 ，St 是后面的query 向量，而编码过程的隐藏状态hi 是 value s hidden_with_time_axis      =tf.expand_dims(query,1)\n\n#计算注意力权重值\n\nscore                                        =self.V(tf.nn.tanh(\n\nself.W1(values)+self.W2(hidden  with  time   axis)))\n\n_重w(an. 长度，1)\n\n#上下文向量(context_vector)             求和之后的形状为(批大小，隐藏层大小)\n\n189\n\nPython 中文自然语言处理基础与实战\n\ncontext  vector =attention  weights *values\n\ncontext_vector          =tf.reduce_sum(context_vector,axis=1)\n\nreturn                                  context_vector,attention_weights\n\n_ayreersult,attentio=nBa_tto构n建_le_hidden,\n\nsample_output)\n\nprint(     '注意力结果形状：','\\n','(batch_size,units){}'.format(attention_\n\nresult.shape))\n\nprint(      '注意力权重形状：','\\n','(batch_size,sequencelength,1){}'.format\n\n(attention_weights.shape))\n\n190\n\n#解码器\n\nclass\n\nDecoder(tf.keras.Model):\n\ndef             ini t         (self,vocab_size,embedding_dim,dec_units,batch_sz): super(Decoder,self).     init     ()\n\nself.batch_sz            =batch_sz             # 每次训练所选取的样本数\n\nself.dec_units              =dec_units              #神经元数量\n\n#输入层\n\nself.embedding                                   =tf.keras.layers,Embedding(vocab_size,embedding_dim)\n\nself.gru=tf.keras.layers.GRU(self.dec  units,\n\nreturn_sequences=True, return_state=True,\n\nrecurrent_initializer='glorot_\n\nuniform')\n\nself.fc =tf.keras.layers.Dense(vocab   size)\n\n#调用注意力模型\n\nself.attention                                    =BahdanauAttention(self.dec_units)\n\ndef                                  call(self,x,hidden,enc_output):\n\nt输_tion的_,enc_output)\n\n#  x  在通过嵌入层后的形状为(批大小，1,嵌入维度)\n\nx                 =self.embedding(x)\n\n  f.pand后_i形m_，i\n\n#将合并后的向量传送到 GRU\n\noutput,state                      =self.gru(x)\n\n#输出的形状为(批大小*1,隐藏层大小)\n\noutput                                                                =tf.reshape(output,(-1,output.shape[2]))\n\n#输出的形状为(批大小，词表大小)\n\nx=self.fc(output)\n\nreturn x,state,attention  weights\n\n#构建解码器网络结构\n\ndecoder  =Decoder  (vocab_tar_size,embedding_dim,units,BATCH_SIZE) sample   decoder   output,states,attention  weight =decoder(\n\ntf.random.uniform((64,1)),sample_hidden,sample_output)\n\nprint(     '解码器输出形状：','\\n','(batch_size,vocab_size){}'.format(sample\n\ndecoder_output.shape))\n\n第 0 章 NLP   中的深度学习技术\n\n运行代码10-14后，输出结果如下。\n\n10.5.3  定义优化器和损失函数\n\n定义优化器和损失函数包括以下步骤。\n\n(1)选择 Adam  优化器，它是一种可以替代传统随机梯度下降过程的一阶优化算法，\n\n能基于训练数据迭代地更新神经网络权重。\n\n( 2 ) 选 择 SparseCategoricalCrossentropy   作为损失函数，它是指用于计算标签值和预测\n\n值之间差异的函数，可以最小化每个训练样例的误差。\n\n定义模型的优化器和损失函数，如代码10-15所示。其中， loss   function 函数用于实现\n\n优化器和损失函数。\n\n代码10-15 定义模型的优化器和损失函数\n\n10.5.4    训练模型\n\n训练模型包括以下步骤。\n\n(1)将数据集输入编码器，编码器返回编码器输出和编码器隐藏状态。\n\n(2)编码器输出、编码器隐藏状态和解码器输入(即开始令牌)被传递到解码器。\n\n(3)解码器返回预测和解码器隐藏状态，然后将解码器隐藏状态传递回模型，并使用\n\n191\n\nPython 中文自然语言处理基础与实战\n\n预测计算损耗。\n\n( 4 ) 利 用 Attention   机制决定解码器的下 一 个输入， Attention   机制是将目标单词作为下\n\n一个输入传递给解码器的技术。\n\n(5)计算梯度并将其应用于优化器和反向传播。\n\n训练构建好的模型，如代码10- 16所示。其中， train   函数用于实现训练和验证模型", "metadata": {}}, {"content": "，然后将解码器隐藏状态传递回模型，并使用\n\n191\n\nPython 中文自然语言处理基础与实战\n\n预测计算损耗。\n\n( 4 ) 利 用 Attention   机制决定解码器的下 一 个输入， Attention   机制是将目标单词作为下\n\n一个输入传递给解码器的技术。\n\n(5)计算梯度并将其应用于优化器和反向传播。\n\n训练构建好的模型，如代码10- 16所示。其中， train   函数用于实现训练和验证模型，\n\n训练次数为50次。\n\n代码10- 16   训练构建好的模型\n\nimport  os\n\nimport  time\n\n#检查点(基于对象的保存),准备保存训练模型\n\ncheckpoint_dir     ='../tmp/training_checkpoints'\n\ncheckpoint_prefix                                             =os.path.join(checkpoint_dir,'ckpt')\n\n#保存模型\n\ncheckpoint             =tf.train.Checkpoint(optimizer=optimizer,encoder=encoder, decoder=decoder)\n\n#训练模型\n\ndef                            train(inp,targ,enc_hidden):\n\n111\n\ninp:   批次\n\ntarg: 标签\n\nenc  hidden: 隐藏样本\n\n_\n\n711\n\nloss=0\n\nwith                 tf.GradientTape()as                 tape:\n\nenc_output,enc                     hidden                     =encoder(inp,enc_hidden)#  构建编码器\n\ndec_hidden  =enc_hidden\n\ndec_input             =tf.expand_dims([targ_lang.word_index['<start>']]*\n\nBATCH_SIZE,1)\n\n#将目标词作为下一个输入\n\nfor             t             in             range(1,targ.shape[1]):\n\n#将编码器输出传送至解码器\n\npredictions,dec_hidden,dec_predictions  =decoder(dec_input,\n\ndec_hidden,outloss_function(targ[:,t],predictions)\n\ndec_input                                   =tf.expand_dims(targ[:,t],1)\n\nes =enc lo .net(t_pbe) +decoder.trainable_variables\n\ngradients        =tape.gradient(loss,variables)\n\noptimizer.apply_gradients(zip(gradients,variables))\n\nreturn batch   loss\n\n#开始训练\n\nEPOCHS      =50\n\nloss            =[]\n\nfor        epoch        in        range(EPOCHS):\n\nstart                             =time.time()\n\nd ss  =0                 =encoder.initialize_hidden_state()# 初始化隐藏层\n\n_\n\n192\n\n第 ⑩ 章 NLP  中的深度学习技术\n\n代码10- 16的运行结果如下，得到的损失趋势图如图10- 16所示。\n\n损失趋势图\n\n迭代次数(次)\n\n图10-16 损失趋势图\n\n由图10-16 可以看出，模型在训练集上的损失值 (loss)   随着迭代次数 (epoch)    的增\n\n加而缓慢下降，并逐渐趋于稳定，误差值越来越小，模型预测的结果越来越精确。\n\n193\n\nPython 中文自然语言处理基础与实战\n\n10.5.5 翻译\n\n翻译包括以下两项内容。\n\n(1)与训练模型时类似，设置测试时的相关参数配置。\n\n(2)输入5条句子，利用模型进行翻译，评价模型翻译的准确性。\n\n使用训练好的模型对语句进行翻译，如代码10-17所示。其中 evaluate 函数用于实现\n\n模型测试，translate 函数用于实现语句的翻译。\n\n代码10- 17  使用训练好的模型对语句进行翻译\n\nimport numpy as np\n\n# 翻 译\n\ndef evaluate(sentence):\n\n1\n\nsentence:       需要翻译的句子\n\nT+*\n\nattention  plot =np.zeros((max   length  targ,max   length   inp)) sentence   =preprocess_sentence(sentence)\n\nts       _ng.seqence.p _sequee.split('')]\n\ninput inputs]=,mtfmt_taox__t adding='post')\n\nresult               =\n\n__indcd_ehniddeencoder(inputs,hidden)\n\ndec_input                              =tf.expand_dims([targ_lang.word_index['<start>']],0)\n\nfor   t   ir n_tihd_dttention_weights =decoder(dec_input,\n\ndec_hidden,enc_out)predicted   id =tf.argmax(predictions[0]).numpy()\n\nresult   +=targ   lang.index  word[predicted   id]+''\n\niftarg   lang.index  word[predicted   id]=='<end>':\n\nreturn    result,sentence,attention  plot\n\n测_id_dims([predicted_id],0)\n\nreturn    result,sentence,attention  plot\n\n#执行翻译\n\ndef         *11               translate(sentence):\n\nsentenc    e:   要翻译的句子\n\n plot =evaluate(sentence)\n\nprint('输人：8s'号(sentence))\n\nprint(     '翻译结果：{}'.format(result))\n\nprint(translate('我生病了。'))\n\nprint(translate('为什么不?'))\n\nprint(translat           e ( '让我一个人待会儿。'))\n\nprint(translate(            '打电话回家!'))\n\nprint(translate(  '我了解你。'))\n\n194\n\n第 ⑩ 章  NLP  中的深度学习技术\n\n调用 evaluate  函数，并执行 translate  函数对输入的句子进行翻译，最终得到翻译的结\n\n果如下。\n\n可能是由于模型在训练过程中存在一定的损失值(loss) 或者迭代次数(epoch) 较少， 影响了翻译的准确性，因此可以适当调整网络层数、节点数和训练次数，并优化模型编译\n\n参数，以提升模型翻译的准确性。\n\n小结\n\n本章主要介绍了 NLP  中使用的深度学习技术，包括循环神经网络 (RNN) 、 长短期记 忆 (LSTM)     网络和 Seq2Seq 模型。首先介绍了 RNN 的基本概念，然后引入了单向 RNN 结构、LSTM 模型结构和 Seq2Seq 模型结构，随后介绍了常见的深度学习工具，最后将LSTM\n\n模型应用于文本分类和情感分析，将Seq2Seq 模型应用于机器翻译。\n\n实训\n\n实训1  实现基于 LSTM  模型的新闻分类\n\n1.  训练要点\n\n(1)熟悉利用LSTM 模型进行文本分类的流程和方法。\n\n(2)掌握利用 TensorFlow 构建LSTM 模型进行文本分类的过程。\n\n2. 需求说明\n\n某网站的新闻语料库包含大量的新闻文本语料，需要依据该语料库构建LSTM 模型实 现文本分类。提供的新闻语料为 CSV 格式的数据，其中 text 是新闻文章的详细内容，即样 本自变量； category  是新闻主题分类，即样本因变量，共5个分类。根据新闻语料库对数 据进行相应的预处理，并利用深度学习构建和训练LSTM 模型对相应的新闻文本内容进行\n\n分类。\n\n3. 实现思路与步骤\n\n(1)语料预处理。根据新闻语料库构建相应的字库，将数据序列化并填充为长度一致。\n\n195\n\nPython 中文自然语言处理基础与实战\n\n(2)构建 LSTM 模型。构建合适的LTSM模型框架。\n\n(3)模型训练。调用构建好的模型，选择合适的参数(如优化器、损失函数、评价函\n\n数等)进行模型训练。\n\n(4)模型评价。选择合适的评价指标对训练好的模型进行评价。\n\n实训2 实现基于 LSTM  模型的携程网评论情感分析\n\n1.   训练要点\n\n(1)熟悉基于LSTM模型对文本进行情感分析的方法。\n\n(2)熟悉使用 TensorFlow 构建 LSTM模型进行情感分析的过程。\n\n2. 需求说明\n\n为了对酒店的评论文本进行情感分析，需要使用 TensorFlow 构建基于 LSTM模型的情 感分析模型。提供的酒店评论数据语料是从携程网上自动采集，并经过整理而成的中文情 感语料。评论数据分为 neg 和 pos 两种情感极性并自带标签。根据提供的酒店相关的评论 数据进行相应的语料预处理，利用TensorFlow 框架构建 LSTM模型实现对评论数据的情感\n\n分析", "metadata": {}}, {"content": "，需要使用 TensorFlow 构建基于 LSTM模型的情 感分析模型。提供的酒店评论数据语料是从携程网上自动采集，并经过整理而成的中文情 感语料。评论数据分为 neg 和 pos 两种情感极性并自带标签。根据提供的酒店相关的评论 数据进行相应的语料预处理，利用TensorFlow 框架构建 LSTM模型实现对评论数据的情感\n\n分析，并对训练好的模型进行评价。\n\n3. 实现思路与步骤\n\n(1)数据预处理。读取正负情感语料并分词，构建词语向量并填充为统一长度。\n\n(2)构建 LSTM模型。构建合适的 LTSM 模型框架。\n\n(3)模型训练。调用构建好的模型，选择合适的参数(如优化器、损失函数、评价函\n\n数等)进行模型训练。\n\n(4)模型评价。选择合适的评价指标对训练好的模型进行评价。\n\n实训-3  实现基于 Seq2Seq 和 GPU 的机器翻译\n\n1.   训练要点\n\n(1)了解语料库的基本处理方法，学会一般NLP的方法。\n\n(2)熟悉RNN 及LSTM的各个结构。\n\n(3)掌握基于 Seq2Seq 与 GPU 实现机器翻译的方法。\n\n2. 需求说明\n\n为了实现机器翻译，需要利用 TensorFlow 框架实现 Seq2Seq神经网络中英文机器翻译 模型。其中 en-zh-test.txt 是包含中英互译的文件，对该数据先进行语料库预处理，然后构 建基于Attention 机制的 Seq2Seq模型，最后对模型进行训练并调用模型进行测试，从而实\n\n现机器翻译。\n\n3. 实现思路与步骤\n\n(1)语料库预处理。在 Python 中导入中英文句子互译的语料库文件，并进行分词、构\n\n建映射关系等操作。\n\n(2)构建模型。定义编码、解码和 Attention 机制的 Seq2Seq模型。\n\n(3)定义优化器和损失函数。根据模型预测的结果衡量模型预测能力的好坏。\n\n196\n\n第 0 章  NLP  中的深度学习技术\n\n(4)训练模型。利用构建好的 Seq2Seq模型对语料库进行训练。\n\n(5)保存模型。checkpoint 是 TensorFlow 官方文档保存检查点，主要通过 tftrain.Saver)\n\n构建实例化检查点，然后通过该检查点保存或导入模型。\n\n(6)实现翻译。输入语句，利用模型实现语句翻译。\n\n课后习题\n\n1. 选择题\n\n(1)RNN   适用于处理视频、语音、文本等与时序相关的问题，其常见的应用领域不包\n\n括  (    )。\n\nA.  图像处理    B.   视频剪辑     C.  语音识别        D. 文本相似度计算\n\n(2)RNN    经典结构的输入和输出的序列长度为(    )。\n\nA.  多对一      B.  一对多       C.  等长的多对多    D.  非等长的多对多\n\n(3)下列关于双向RNN 结构说法正确的是(    )。\n\nA.  只考虑预测词前面的词，并没有考虑该词后面的内容\n\nB. 不仅从前往后保留该词前面的词的信息，而且还从后往前保留该词后面的词\n\n的信息\n\nC.  不是由两个RNN 上下叠加在一起组成\n\nD.  输出与隐藏层的状态无关\n\n(4)下列关于LSTM 说法不正确的是(    )。\n\nA.  通过改进使RNN 具备避免梯度消失的特性\n\nB.LSTM     只能够刻画出输入数据中的短距离的相关信息，不能够捕捉到具有较\n\n长时间间隔的依赖关系\n\nC.LSTM    神经网络模型使用门结构实现了对序列数据中的遗忘与记忆\n\nD.  使用大量的文本序列数据对 LSTM 模型训练后，可以捕捉到文本间的依赖关\n\n系，训练好的模型就可以根据指定的文本生成后序的内容\n\n(5)TensorFlow    的特点不包括(    )。\n\nA.  高速性      B. 性能最优化  C. 多语言支持      D.  可移植性\n\n2. 操作题\n\n(1)调整文本分类中LSTM 模型的参数，训练并测试模型。\n\n(2)调整情感分析中LSTM 模型的参数，训练并测试模型。\n\n(3)增加机器翻译中 模型的训练次数，测试模型并分析其运行结果。\n\n197\n\n第 章 智能问答系统\n\n智能问答系统通过一问一答的形式精确定位用户所需要的知识，并以交互的形式为 用户提供个性化的信息服务。智能问答系统对积累的无序语料信息进行有序和科学的整 理，并建立基于知识的分类模型。这些分类模型可以指导新增加的语料咨询和服务信息， 节约人力资源，实现信息处理的自动化，提高用户的办事效率。本章首先介绍智能问答 系统的基本概念，接着介绍智能问答系统的主要组成部分，最后利用 Seq2Seq 模型构建\n\n一个智能问答系统。\n\n学习目标\n\n(1)了解智能问答系统的基本概念。\n\n(2)熟悉智能问答系统的主要组成部分。\n\n(3)掌握智能问答系统的实现过程。\n\n11.1    智能问答系统简介\n\n随着时代的发展，人们越来越想要快速、准确地获取信息。单纯通过人工客服处理客 户问题需要消耗很多人力和成本，于是自动化的问答系统被提出并逐渐发展起来。智能问 答系统 (Question   Answering    System,QA) 是信息检索系统的一种高级形式，它能用准确、\n\n简洁的自然语言回答用户用自然语言提出的问题。\n\n1950 年，英国数学家、逻辑学家艾伦·麦席森·图灵提出了著名的图灵测试。图灵 测试是指在测试者与被测试者(一个人或一台机器)隔开的情况下，测试者通过键盘或 者其他装置向被测试者随意提问，由被测试者回答，如果在多次测试后仍有超过30%的 测试者不能确定被测试者是人还是机器，那么这台机器就通过测试并被认为具有人类智 能。1990年，为了推动智能问答系统的发展，休·罗布纳设立罗布纳奖。2014年， 一个 带有聊天机器人程序的人工智能软件尤金·古斯特曼成功地让人类相信它是一个13 岁男 孩。目前，智能问答系统是人工智能和NLP 领域中一个倍受关注并具有广泛发展前景的\n\n方向。\n\n11.    2      智能问答系统的主要组成部分\n\n智能问答系统的问答过程和人与人之间的对话过程(首先由一个人提问，然后另一个\n\n第  章  智能问答系统\n\n人在脑海中思考问题，最后组织语言回答问题)是一样的。智能问答系统流程由问题理解、 知识检索、答案生成3个部分组成。其中，问题理解包括问题分类、关键词提取，知识检 索包括结构化和非结构化信息检索、答案生成包括答案提取和答案验证。智能问答系统的\n\n两个主要难题就是对问题的理解和问题与答案之间的匹配程度。\n\n11.2.1  问题理解\n\n问题理解需要理解的是问题在问什么,以及该问题属于哪方面的问题，即问题分类和\n\n关键词提取。\n\n1. 问题分类\n\n通常，每一个提出的问题都包含问题主体。问题主体的类别可以总结为“5W1H”,   即 Who (问人物)、When  (问时间)、Where  (问地点)、What  (问事件)、Why  (问原因)、 How(问怎么做),但是单靠问题主体难以定位问题的分类点，所以为了更好地确定问题的\n\n类别，还需要将问题的分类标准再细分为若干种分类体系。\n\nUIUC  问句分类体系是一个双层次结构体系，主要针对事实类问题，拥有6 个大类和 50 个小类，包括实体(问答的是某种事物，如动植物、食物、体育等)、描述(事物的定 义、描述、事件原因等)、人物(人名、称号等)、地点(国家、城市、山脉等)、数值(数\n\n字、日期、顺序等)、缩写(缩写或略写形式)。\n\n也有单层次的分类体系，如德拉戈米尔·R·拉德夫设计了17个分类类别，包括人物、 数字、描述、原因、地点、定义、缩写、长度、日期等。还可以根据问题所属的垂直领域 进行分类，垂直领域下的相关问题交由相关领域特定的功能处理，如天气类、食物类、百\n\n科类等。\n\n2. 关键词提取\n\n为了获得问题的类别，需要在问句中提取出关键词，从而提取出问题的核心部分以便 准确找到问题的类别。简单直观的提取关键词的方法为基于规则的匹配方法，用于识别可 以归类的查询语句。基于规则的匹配方法虽然简单直接，但是不能灵活应对变化多样的自 然语言。更好的方式是结合词性、句法等关键词提取技术", "metadata": {}}, {"content": "，垂直领域下的相关问题交由相关领域特定的功能处理，如天气类、食物类、百\n\n科类等。\n\n2. 关键词提取\n\n为了获得问题的类别，需要在问句中提取出关键词，从而提取出问题的核心部分以便 准确找到问题的类别。简单直观的提取关键词的方法为基于规则的匹配方法，用于识别可 以归类的查询语句。基于规则的匹配方法虽然简单直接，但是不能灵活应对变化多样的自 然语言。更好的方式是结合词性、句法等关键词提取技术，通过词性标注、命名实体识别\n\n等操作提取关键词。\n\n11.2.2  知识检索\n\n知识库的内容和规模能直接改变智能问答系统的结果，并影响问答能力和效率。 一个 好的智能问答系统，其背后的知识库通常是非常庞大且全面的。 一般来说，知识库是通过 人工的方式整理成的结构化数据，便于计算机进行处理。然而在大数据时代，结构化数据\n\n相对于非结构化数据要少得多，而且有的结构化数据还是通过人工的方式整理的。因此\n\n如果能从非结构化数据中提取有效的答案，可以明显地提高智能问答系统回答问题的效率。\n\n1. 结构化信息检索\n\n智能问答系统的结构化信息检索实质是一个问题与多个答案之间的关系，侧重的是实体\n\n的各个属性之间、实体与各属性之间的关系。结构化信息主要有关系类知识和百科类知识。\n\n199\n\nPython 中文自然语言处理基础与实战\n\n关系类知识可以简化表示为两个事物和它们之间的关系，即有两个事物A 和 B,   它们 之间存在某种关系 R, 表示为 A—R—B,  能够解决一些事实类的问答问题。例如，“珠穆 朗玛峰的高度是多少?”中，“珠穆朗玛峰”是事物A,   “高度”是关系R,   事物 A 需要通 过关系R 去连接另一个事物 B,   利用关系类知识可以得到事物B 为“8848.86米”。比较著 名的关系类知识库有 DBPedia 和 YAGO,  这些数据库通过从互联网上提取数据组织形成关\n\n系结构数据库。\n\n百科类知识是由一个个条目信息组成的，每个条目中都有其简介、属性等相关信息。 百科类条目信息的属性结构性强，内容清晰，但也存在其他非结构化信息。例如百度百 科中“广东省”的条目信息中，包括结构化属性“行政类别”“面积”“人口”“方言”等 信息，也包括非结构化信息“历史沿革”“地理环境”等。百科类条目信息除了常见的百\n\n度百科，还有互动百科等。\n\n2. 非结构化信息检索\n\n智能问答系统的非结构化信息是指没有组织成表格的属性、实体或隐藏在文本中的信息。 可以通过非结构化信息检索的方法搜索和挖掘与问题相关的信息。非结构化信息的检索方式与\n\n搜索引擎技术相同，即以某个关键词为索引，查找与索引相关的信息，再进行答案生成。\n\n如果一篇文档或者一段文本中包含与关键词相关的答案信息，则关键词与关键词之间 的位置相对较近。因此可以以段落为单位，计算连续的少量段落内是否出现了所有的关键 词。这种方法可以去除一些与关键词相关，但与问题答案不相关的文档或文本。在智能问 答系统的实际应用中，通常借助商业化的搜索引擎完成这项工作，特别是现在的很多商业 搜索引擎已经具备了一定的自然语言理解能力。例如， Siri 这个产品便采用了这样的策略， 当输入的句子无法被其识别时，它便将整句话提交给搜索引擎，并将检索到的文档集合列\n\n出来，供用户自行选择。\n\n11.2.3  答案生成\n\n通过非结构化检索得到的信息的结构化特性不高，还需要进行筛选过滤，提取其中最\n\n精准的答案。\n\n1. 答案提取\n\n在问题理解环节中，会对问题进行一定程度的分类，如问题提问的是人物、数值、地点 还是日期等。然后通过NLP 技术，如词性标注、命名实体识别、关键词提取等方式，从文本 中抽取更可能是答案的词或句子。此外，由于问题的关键词和答案词之间必然存在某种联系， 因此还可以考虑问题和候选答案的相似度，如问题关键词和答案词之间语义联系的远近。答 案与问题之间也可能存在句式的联系。例如，问题“广东省的面积是多少?”中，词语“多\n\n少”可以被替换为答案，即可以在答案文本中寻找类似“广东省的面积是×××”的句子。\n\n2. 答案验证\n\n随着候选答案范围的逐步缩小，可以借助其他工具验证答案的可信程度。例如采用其 他的信息源(如知识库),在其中检索问题和答案的相关性。在互联网中检索答案，然后统\n\n计问题与答案同时出现的频率也是一种简单有效的验证方法。\n\n200\n\n第 ① 章  智能问答系统\n\n1  1.3   任务：基于 Seq2Seq 模型的聊天机器人\n\n本节将实现智能问答系统中的对话式问答系统，它是基于 Seq2Seq 模型的聊天机器 人。Seq2Seq 模型在第10章已经介绍，该模型可用于机器翻译、文本摘要、会话建模等。\n\n该技术突破了传统的固定大小输入问题框架，将神经网络模型运用于翻译与智能问答这一\n\n类序列任务，在机器翻译和人机短问快答中得到了广泛的应用。\n\n基于Seq2Seq 模型的聊天机器人的实现流程如下。\n\n(1)读取语料库。\n\n(2)文本预处理。对原始语料文件进行预处理等。\n\n(3)模型构建。搭建聊天机器人模型计算图、添加 Attention 机制。\n\n(4)模型训练。设置训练步，  训练模型，模型测试。\n\n(5)模型评价。\n\n11.3.1 读取语料库\n\n使用的语料样本一共有5个 TXT 文件，存放在 dialog 文件中，且以 UTF-8 的编码方 式保存。语料样本包括问题和回答两部分，其中奇数行为问题，偶数行为回答。每个文档 都具有相同的数据格式，其中 “one.txt”“two.txt”“three.txt”“four.txt”           中的部分内容如\n\n表11-1所示。\n\n表11 - 1 部分语料样本内容\n\none.txt two.txt three.txt four.txt 你好 您好 你吃了吗 吃过了 你多大了 你猜猜 我看你没到20 · 你是谁 聊天机器人 你知道我是谁吗 你还没告诉我 呢 …… 今天天气怎么样? 很好。 你感冒了? 有点难受。\n\n因为语料储存于 dialog 文件的多个语料文件中，所以在读取前需要先批量获取文件名 称，再循环读取文件，如代码11-1所示。为了方便后续运行，将此段代码写入data_utls.py\n\n文件。\n\n代码11-1 读取语料库\n\n201\n\nPython 中文自然语言处理基础与实战\n\n运行代码11-1后，输出结果如下。\n\n2\n\n11.3.2  文本预处理\n\n在构建聊天机器人模型前，需要对原始的语料文件进行预处理，包括分词并构建词典\n\n拆分问句、答句和保存文件，加载词典和数据，数据准备等。\n\n1. 分词并构建词典\n\n对获取的语料文件进行分词并构建词典。Python 中文分词库有很多，本案例采用的是\n\n使用范围最广的jieba 分词。jieba 分词的使用示例如代码11-2所示。\n\n代码11-2 jieba 分词的使用示例\n\n运行代码11-2后，输出的结果如下。\n\n对语料进行分词并构建词典，如代码11-3所示。为了方便后续运行，将此段代码写入\n\ndata_utls.py文件。\n\n代码11-3 对语料进行分词并构建词典\n\n202\n\n第  章  智能问答系统\n\n运行代码11-3后，输出的结果如下。\n\n203\n\nPython 中文自然语言处理基础与实战\n\n3. 加载词典和数据\n\n使用tflookup.StaticHashTable  函数将 all_dict.txt 初始化为一个不可变的通用哈希表。 哈希表是一个散列表，它存储的内容是键值对 (key-value) 映射，在缺少键时通过设置 default  value  确定使用的值。加载完词典后，就加载预处理好的数据。将 source.txt  和 target.txt 文件加载并转化为 Python 的 Mapdataset 格式，如代码11-5所示。data_path 和 CONST 参数设置详见模型构建部分中的参数设置。为了方便后续运行", "metadata": {}}, {"content": "，它存储的内容是键值对 (key-value) 映射，在缺少键时通过设置 default  value  确定使用的值。加载完词典后，就加载预处理好的数据。将 source.txt  和 target.txt 文件加载并转化为 Python 的 Mapdataset 格式，如代码11-5所示。data_path 和 CONST 参数设置详见模型构建部分中的参数设置。为了方便后续运行，将此段代码写入\n\nexecute.py 文 件\n\n代码11 - 5  加载词典和数据\n\nimport tensorflow as tf\n\nimport datetime\n\n#加载词典\n\nprint(f'[{datetime.datetime.now()}]  加载词典...')\n\ndata_path                       ='../data/ids'\n\nCONST={'_BOS':0,'_EOS':1,'_PAD':2,'_UNK':3}\n\ntable                               =tf.lookup.StaticHashTable(# 初始化后为不可变的通用哈希表\n\ninitializer=tf.lookup.TextFileInitializer(\n\nos.path.join(data  path,'all   dict.txt'),\n\ntf.string,\n\ntf.lookup.TextFileIndex.WHOLE_LINE,\n\ntf.int64,\n\ntf.lookup.TextFileIndex.LINE_NUMBER\n\n),#要使用的表初始化程序。有关支持的键和值类型请参见哈希表内核\n\ndefault_value=CONST['_UNK']-len(CONST)                        # 表中缺少键时使用的值\n\n)\n\n204\n\n#加载数据\n\nprint(f'[{datetime.datetime.now()}]\n\n加载预处理后的数据..')\n\nf造t p的(et:字典\n\ntext: 文本\n\nT\"1\n\ntokenized\n\n=tf.strings,split(tf.reshape(text,[1]),sep='')\n\n第 ① 章  智能问答系统\n\ntmp    =table.lookup(tokenized.values)+len(CONST)\n\nreturn           tmp\n\n#增加开始和结束标记\n\ndef  add_start_end_tokens(tokens):\n\nYt\n\ntokens   : 列化的键值对字典\n\n111\n\ntmp    =tf.concat([[CONST['  BOS']],tf.cast(tokens,tf.int32),\n\n[CONST['_EOS']]],axis=0)\n\nreturn tmp\n\n205\n\n#获取数据\n\ndef\n\ntf.data.Dataset:\n\nget_dataset(src_path:str,table:tf.lookup.staticHashTable)->\n\n1?)\n\nsrc_path: 文件路径\n\ntabl   e:   初始化后不可变的通用哈希表。\n\n111\n\ndataset         =tf.data.TextlineDataset(src_path)\n\nt pp(tm_srt_end_tokens)\n\nreturn  dataset\n\n#获取数据\n\nsrc  train\n\ntgt_train\n\n=get   dataset(os.path.join(data  path,'source.txt'),table) =get   dataset(os.path.join(data  path,'target.txt'),table)\n\n4. 数据准备\n\n为了防止模型过拟合，需要在构建模型前对数据进行一些处理。首先将之前得到的 src_train  和 tgt_train  数据用 zip 函数进行打包，得到 train_dataset   数据，然后通过 filter_   instance_by_max_length 函数设置一个最大长度值控制过滤数据实例数。之后调用 shuffle 方法，该方法是 TensorFlow  中数据集类 Dataset  的一个数据处理方法，用于打乱数据集中 数据的顺序，常用在模型训练中。shuffle_buffer_size     参数定义加载数据集时缓冲的实例数， MAX_LENGTH  参数定义句子的最大词长， batch_size  参数则定义一次前向/后向传播中提 供的训练数据样本数，3个参数的设置详见模型构建部分中的参数设置，整个数据准备的\n\n处理过程如代码11-6所示。为了方便后续运行，将此段代码写入execute.py 文件。\n\n代码11-6  数据准备的处理过程\n\nPython 中文自然语言处理基础与实战\n\nsrc: 特征\n\ntgt:  标签\n\n111\n\nreturn tf.logical   and(tf.size(src)<=MAX   LENGTH,tf.size(tgt)<=\n\nMAX_LENGTH)\n\ntrain   dataset =train   dataset.filter(filter   instance  by  max   length)#过滤\n\nn_dataset =train_dataset.cache()\n\ntrain   dataset -train   dataset.shuffle(shuffle  buffer   size) #打乱数据\n\ntrain_dataset      =train_dataset.padded      batch(# 将数据长度变为一致，长度不足用 PAD\n\n补齐\n\nbatch_size,\n\npadded_shapes=([MAX_LENGTH +2],[MAX_LENGTH +2]),\n\npadding_values=(CONST['_PAD'],CONST['_PAD']),\n\ndrop_remainder=True,\n\n#提升产生下一个批次数据的效率\n\ntrain_dataset        =train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n11.3.3 模型构建\n\n使用Seq2Seq 模型解决聊天机器人的搭建问题的主要步骤为构建Encoder-Decoder框架\n\n和引入 Attention 机制。\n\n1. 参数设置\n\n在构建模型之前，需先设置模型参数，如模型训练迭代的次数、加载数据集时缓冲的 实例数和词嵌入维度，以及模型的输入、输出等占位符的定义。需要添加额外字符_UNK、 EOS 、_BOS 和_PAD,   这些额外字符在训练模型的过程中能够起到辅助作用。额外字符的\n\n具体作用如下。\n\n(1)_UNK:      用于替代处理样本时出现的字典中没有的字符、低频词或一些未遇到过\n\n的词等。\n\n(2)_EOS:end     of     sentence,解码器端的句子结束标识符。\n\n(3)_BOS:begin    of    sentence,  解码器端的句子起始标识符。\n\n(4)PAD:        占位符，用于对齐、填充、占位，补全字符。\n\n此外，模型的基本参数还包括每次训练数据样本数、隐藏层神经元个数等。这里将模 型参数保存在 checkpoint_path 变量中的路径下，如代码11-7所示。为了方便后续运行，将\n\n此段代码写入 execute.py 文件。\n\n代码11-7 模型的参数设置\n\n206\n\n第⑪章  智能问答系统\n\n2.GPU   设置\n\n显卡的处理器称为图形处理器 (Graphics    Processing     Unit,GPU),  是显卡的“心脏”, 其功能与 CPU 类似，只不过GPU  是专为执行复杂的数学和几何计算而设计的，这些计算 是渲染图形所必需的。某些机器学习和深度学习的程序常常需要进行大量的计算，这需要 消耗计算机中的大量运算资源，导致计算机运行速度变慢。而如果将 Python  程序设置在 GPU 上运行，则可以提高运行速度。设置 device 参数值可以确定是否使用 GPU,  默认值为 - 1,即不使用 GPU 。如果选择使用 GPU,   需要先获得当前主机上 GPU 运算设备的列表， 再限制 TensorFlow 仅使用指定的 GPU,   如代码11-8所示。为了方便后续运行，将此段代\n\n码写入execute.py 文件。\n\n代码11-8 GPU 设置\n\n3.Seq2Seq    模型\n\n在原始的多对多结构中，要求输入序列与输出序列等长，而现实中遇到的大部分情况 为输入序列与输出序列不等长。如在本案例中，聊天机器人的问题和回答往往没有长度相 等的关系。为了有效解决输入序列与输出序列不等长的问题，需要使用 Seq2Seq  模型\n\n(Sequence  to  Sequence  Model,  也称 Encoder-Decoder 模型)搭建聊天机器人。\n\n(1)定义 Encoder 端。聊天机器人实现过程中常用的是 LSTM 算法，而 GRU 模型比标 准的 LSTM 模型简单，是非常流行的变体。GRU 还包含细胞状态(信息传输的路径)和隐 藏状态等一些其他的改动。虽然 GRU 比 LSTM 少了一个状态输出，但效果几乎一样，因 此在编码时使用 GRU 可以让代码更为简单一些。可以通过 tf.keras  接口设置 GRU 层，定\n\n义 Encoder 端如代码11-9所示。为了方便后续运行", "metadata": {}}, {"content": "，而 GRU 模型比标 准的 LSTM 模型简单，是非常流行的变体。GRU 还包含细胞状态(信息传输的路径)和隐 藏状态等一些其他的改动。虽然 GRU 比 LSTM 少了一个状态输出，但效果几乎一样，因 此在编码时使用 GRU 可以让代码更为简单一些。可以通过 tf.keras  接口设置 GRU 层，定\n\n义 Encoder 端如代码11-9所示。为了方便后续运行，将此段代码写入 Seq2Seq.py 文件。\n\n代码11-9 定义 Encoder 端\n\n207\n\nPython 中文自然语言处理基础与实战\n\nNone:\n\n111\n\nvocab_size: 词库大小\n\nembedding_dim;       词向量维度\n\nenc  units:L        STM层的神经元数量\n\n_\n\n111\n\n(er,inc units\n\n#词嵌入 \n\nself.embedding=tf.keras.layers.Embedding(vocab_size,embedding_dim) #LSTM 层 ，GRU 是简单的 LSTM层\n\n208\n\nself.gru\n\nTrue,return_state=True)\n\n#定义神经网络的传输顺序\n\ndef\n\ntf.Tensor]:\n\n!  !\n\nx:  输入的文本\n\n111\n\nx=self.embedding(x)\n\noutput,state\n\nreturn             output,state\n\n=tf.keras.layers.GRU(self.enc_units,return_sequences=\n\ncall(self,x:tf.Tensor,**kwargs)->typing.Tuple[tf.Tensor,\n\n=self.gru(x)\n\n# 输出预测结果和当前状态\n\n(2)定义 BahdanauAttention 。Encoder-Decoder 有一个缺陷，就是当输入信息太长时， 会丢失一些信息，而引入 Attention 机制就是为了解决这个问题。引入 Attention 机制后， Encoder 不再以整个输入序列编码为固定长度的中间向量，而是编码成一个向量的序列。这 里使用的是 BahdanauAttention 机制定义BahdanauAttention,   如代码11-10所示。为了方便\n\n后续运行，将此段代码写入 Seq2Seq.py 文件。\n\n代码11-10 定义 BahdanauAttention\n\n#Attention 机制\n\nclass       BahdanauAttention(tf.keras.Model):\n\n#设置参数\n\ndef    init   (self,units:int)->None:\n\nunits: tti\n\n神经元数据量\n\nsuper(BahdanauAttention,self)._init     ()\n\nself.W1                     =tf.keras.layers.Dense(units)   #全连接层\n\nself.W2                        =tf.keras.layers.Dense(units)# 全连接层\n\nself.V=tf.keras.layers.Dense(1)                      # 输出层\n\n# 设 置Attention       机制的计算方式\n\ndef                             call(self,query:tf.Tensor,values:tf.Tensor,**kwargs)-> typing.Tuple[tf.Tensor,tf.Tensor];\n\n1\n\nque ry:  上一层输出的特征值\n\nvalu  es:    上一层输出的计算结果\n\n#维度增加1维\n\n第 1 章 智能问答系统\n\n(3)定义 Decoder端。Encoder-Decoder模型中的 Decoder又称作解码器，它的作用是 求解数学问题，并将结果转化为现实世界的解决方案。定义 Decoder 端包括 Embedding 层、 GRU  模型和网络层数的设置，还有使用引入的 Attention  机制等。定义 Decoder  端如代码\n\n11-11 所示。为了方便后续运行，将此段代码写入 Seq2Seq.py 文件。\n\n代码11-11 定义 Decoder 端\n\n#解码\n\nclass                           Decoder(tf.keras.Model):\n\n#设置参数\n\ndef                                       init_(self,vocab_size:int,embedding_dim:int,dec_units:int):\n\n11\n\nvocab_size: 词库大小\n\n209\n\nembedding_dim\n\ndec_units:LS\n\n: 词向量维度\n\nTM层的神经元数量\n\n17\n\nsuper(Decoder,self)._init_()\n\nself.dec_units                       =dec_units\n\n#词嵌入层\n\nself.embedding              =tf.keras.layers.Embedding(vocab_size,embedding_dim)\n\n# 添 加 LSTM层\n\nTrue,retul=True)=tf.keras.layers.GRU(self.dec_units,return_sequences=\n\n层=tf.keras.layers.Dense(vocab_size)\n\n#添加Attention 机制\n\nself.attention                                        =BahdanauAttention(self.dec_units)\n\n#设置神经网络传输顺序\n\ndef                        call(self,x:tf.Tensor,hidden:tf.Tensor,enc_output:tf.Tensor)\\ ->typing.Tuple[tf.Tensor,tf.Tensor,tf.Tensor]:\n\n1*1\n\nx:  输入的文本\n\nhidden:     上一层输出的特征值\n\nenc_output:       上一层输出的计算结果\n\n111\n\n# 计 算 Attentio      n 机制层的结果\n\ncontext  vector,attention  weights =self.attention(hidden,enc\n\nPython 中文自然语言处理基础与实战\n\n(4)构建模型。调用定义好的 Encoder 和 Decoder 构建 Encoder-Decoder 模型，如代\n\n码11-12所示。为了方便后续运行，将此段代码写入 execute.py 文件。\n\n代码11-12 构建 Encoder-Decoder 模型\n\n4. 优化器和损失函数\n\n采用优化器和损失函数等方法对聊天机器人模型进行优化。其中，优化器可解决神经 网络中经常面对的非凸函数优化问题，损失函数用于度量神经网络输出的预测值与实际值\n\n之间的差距。\n\n(1)优化器。在神经网络中经常会面对非凸函数的优化问题，往往使用一些网络的优 化方法作为优化器。常见的优化器有梯度下降法 (SGD)  和自适应梯度法(AdaGrad) 等。 其中， AdaGrad 就是将每一维各自的历史梯度值的平方叠加起来，然后在更新的时候除以 该历史梯度值。AdaGrad 可以对低频的参数做较大的更新，对高频的参数做较小的更新， 因此，AdaGrad  对稀疏数据有较好的表现，它很好地提高了SGD 的鲁棒性，如识别视频里\n\n面的猫、训练 GloVe word embeddings等。AdaGrad 是机器学习和深度学习中用得最多的优\n\n化器。\n\n在聊天机器人模型计算图的搭建过程中，采用AdaGrad 进行模型优化，构建优化器如\n\n代码11-13所示。为了方便后续运行，将此段代码写入 execute.py 文件。\n\n代码11-13 构建优化器\n\n(2)损失函数。在计算损失函数之前，需要先指定目标函数，可以使用 Keras  中的 SparseCategoricalCrossentropy 函数，即交叉熵损失函数来实现。因为id 向量填充的时候填\n\n补了大量的占位符_PAD 以确保输入 id 向量的长度一致，而这部分内容在计算损失函数时\n\n210\n\n第  章 智能问答系统\n\n应不予考虑；所以在计算损失函数的时候需要添加权重以调整_PAD的对应位置都为0,不\n\n计算损失，而其余语料id向量正文部分的对应权重数值为1。\n\n计算模型的损失函数如代码11-14所示。其中，最后得到的 loss  function 就是模型的\n\n损失函数。为了方便后续运行", "metadata": {}}, {"content": "，而这部分内容在计算损失函数时\n\n210\n\n第  章 智能问答系统\n\n应不予考虑；所以在计算损失函数的时候需要添加权重以调整_PAD的对应位置都为0,不\n\n计算损失，而其余语料id向量正文部分的对应权重数值为1。\n\n计算模型的损失函数如代码11-14所示。其中，最后得到的 loss  function 就是模型的\n\n损失函数。为了方便后续运行，将此段代码写入execute.py 文件。\n\n代码11-14 计算模型的损失函数\n\n#设置损失函数\n\nprint(f'[{datetime.datetime.now()}]                           设置损失函数...')\n\n#损失值计算方式\n\nloss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n\nreduction='none')\n\n#损失函数\n\ndef      loss   function(loss   object,real:tf.Tensor,pred:tf.Tensor)->tf.Tensor;\n\nloss_object:          损失值计算方式\n\nreal:真实值\n\npred:  预测值\n\n111\n\n#计算真实值和预测值之间的误差\n\nloss  =loss   object(real,pred)\n\nk输=出t i，ca并l用n_oP_th.equal(real,CONST['_PAD']) k据m与a,t_.dtype)\n\nreturn                              tf.reduce_mean(loss_*mask)#返回平均误差\n\n(3)模型保存。因为当语料库数量增加时，模型训练速度将会变慢，而语料库的数量 和质量是聊天机器人中较为重要的因素，会对最后模型训练的效果产生影响；所以为了让 模型能够定期保存模型参数、数据等内容，需要实现下次训练时能够继续使用已训练完成\n\n的部分继续训练，可以添加 checkpoint 文件保存模型。\n\nTensorFlow 的 Checkpoint 机制将可追踪变量以二进制的方式储存成一个.ckpt 文件，用 于储存变量的名称和对应张量的值，在模型训练完成后能将训练好的参数(变量)保存起 来。在需要使用模型的其他地方载入模型和参数即可。tf.train.Checkpoint   类是一个强大的 变量保存与恢复类，可以使用其save 和restore 方法将TensorFlow 中所有包含 Checkpointable   State的对象保存和恢复。使用tftrain.Checkpoint类保存模型参数，如代码11-15所示。为\n\n了方便后续运行，将此段代码写入 execute.py文件。\n\n代码11- 15  使 用 tf.train.Checkpoint  类保存模型参数\n\n11.3.4   模型训练\n\n本案例设置模型训练次数为501 次，每次都输出模型损失率和结果。为了方便下一次\n\n模型调用，将训练后的模型保存在 checkpoint_path 中的路径下。\n\n211\n\nPython 中文自然语言处理基础与实战\n\n1. 设置训练步\n\n模型训练时，需要设置 tf.GradientTape,   即训练步，作用是在 eager 模式下计算梯 度。tf.GradientTape  的出现是 TensorFlow 2.0 最大的变化之一。训练步以一种简洁优雅 的方式，为 TensorFlow  的即时执行模式和图执行模式提供统一的自动求导 API 。训练 步中通过训练 Encoder-Decoder 模型得到loss,  再将其除以 tgt_length,  得到 batch_loss。\n\n设置模型的训练步，如代码11-16所示。为了方便后续运行，将此段代码写入 execute.py\n\n文件。\n\n代码11-16 设置模型的训练步\n\n#训练\n\ndef                                           train_step(src:tf.Tensor,tgt:tf.Tensor):\n\n1\n\nsrc:     输入的文本\n\ntgt:   标签\n\nTtt\n\n_取w标i,度t_length    =tgt.shape\n\nloss             =0\n\n#创建梯度带，用于反向计算导数\n\nwith                 tf.GradientTape()as                 tape:\n\n#对输入的文本编码\n\nenc   output,enc  hidden =encoder(src)\n\n设ec置_数c_的神经元数目相等\n\n#根据标签对数据解码\n\nfor       t        in       range(tgt_length        -1):\n\n#更新维度，新增1维\n\ndec_input                                   =tf.expand_dims(tgt[:,t],1)\n\n# 解 码\n\npredictions,dec  hidden,dec   out  =decoder(dec   input, dec_hidden,enc_output)\n\n#计算损失值\n\nloss         +=loss   function(loss   object,tgt[:,t+1],predictions) #计算一次训练的平均损失值\n\nbatch_loss              =loss             /tgt_length\n\ns值=encoder.trainable  variables  +decoder.trainable  variables\n\n_\n\n#反向求导\n\ngradients\n\n#利用优化器更新权重\n\noptimizer.apply_gradients(zip(gradients,variables))\n\nreturn         batch_loss          #  返回每次迭代训练的损失值\n\n2. 训练模型\n\n对模型进行循环训练，epoch为训练次数，本案例设置为501次。当 epoch 能被100整\n\n212\n\n第 ① 章  智能问答系统\n\n除时，则对模型进行保存，保存在 checkpoint_path 中的路径下。每次循环将每一步训练的 次数和损失值输出，如代码11-17所示。为了方便后续运行，将此段代码写入 execute.py\n\n文件。\n\n代码11-17 训练并保存模型\n\n运行代码11-17后，部分输出结果如下。\n\n3. 模型测试\n\n对训练后保存下来的聊天机器人模型进行测试，先读取相关的文件并加载计算图，再\n\n调用计算图进行模型测试。\n\n(1)结果预测。模型训练结束后，可以通过输入对话进行测试，观察模型的效果。 在对模型进行测试时，先调用之前训练好的模型，然后将需要预测的语句的前后分别加  上_BOS 和_EOS,    输入变量则需通过TensorFlow  中的 keras.preprocessing.sequence.pad      sequences 函数进行预处理操作。对模型进行测试，如代码11-18所示，为了方便后续运行，\n\n将此段代码写入 execute.py文件。\n\n213\n\nPython 中文自然语言处理基础与实战\n\n代码11- 18  对模型进行测试\n\nct(sentence='你好'):\n\n#导入训练参数\n\ncheckpoint.restore(tf.train.latest   checkpoint(checkpoint  path))\n\n#给句子添加开始和结束标记\n\nsentence    ='_B⁰S'+sentence     +'_EOS'\n\n#读取字段\n\nwith  all_d n=(ofs d.jn )ath,'all_dict.txt'),'r',encoding='utf-8')as\n\n#构建词 →id  的映射字典\n\nword2id={):i+len(CONST)for                 i,j                 in                 enumerate(all_dict)}\n\nword2id.update(CONST)\n\n# 构 建id→  词的映射字典\n\nid2word                                                           =dict(zip(word2id.values(),word2id.keys()))\n\n#分词时保留_EOS  和 _BOS\n\nfrom jieba import lcut,add  word\n\nfor  i  in  ['   EOS','  BOs']:\n\nadd_word(i)\n\ni NST['_UNK'])for i in Icut(sentence)]\n\n#长度填充\n\ninputs =tf.keras.preprocessing.sequence.pad   sequences(\n\n[inputs],maxlen=MAX_LENGTH,padding='post',value=CONST['_PAD'])\n\nto_的inputs)\n\n#空字符串", "metadata": {}}, {"content": "，用于保留预测结果\n\nresult=\n\n# 编 码\n\nenc_out,enc_hidden     =encoder(inputs)\n\ndec_hidden =enc_hidden\n\ndec_input              =tf.expand_dims([word2id['_BOS']],0)\n\nfor        t        in        range(MAX_LENGTH):\n\n# 解 码\n\npredictions,dec  hidden,attention  weights   =decoder(dec   input, dec_hidden,enc_out)\n\n#预测出词语对应的id\n\npredicted   id=tf.argmax(predictions[0]).numpy()\n\nrd遇_E'_o'_EOS':\n\nbreak\n\nlUt替(pedicted_id,'_UNK')\n\ndec   input  =tf.expand   dims([predicted   id],0)\n\nreturn       result       #  返回预测结果\n\n214\n\n第⑪章   智能问答系统\n\n运行代码11-18后，输出结果如下。\n\n预测示例：\n\n在的，请问有啥能帮你的吗\n\n通过预测示例可以看出，当前训练的模型具备较高的准确率，基本能够识别对话内容\n\n并返回应答结果。\n\n以上为智能问答系统的步骤拆分，在进行人机交互前，需要先调用data_utls.py 文件进行数\n\n据预处理，再调用Seq2Seq.py文件生成模型，最后调用execute.py 文件进行模型训练和预测。\n\n(2)调用 Flask 前端进行测试。模型测试阶段调用Flask 前端进行人机交互， chat 函数 用于从输入中返回聊天的应答结果， reply 应答函数返回的是一个jsonify  函数。jsonify  函数 是用于处理序列化 JSON 数据的函数，作用是将数据组装成 JSON 格式返回。调用 Flask 前\n\n端，如代码11-19所示，为了方便后续运行，将此段代码写入app.py 文件。\n\n代码11-19  调 用 Flask 前 端\n\nimport tensorflow as tf\n\nimport  os\n\nfrom Seq2Seq import Encoder,Decoder\n\nfrom jieba import Icut,add  word\n\nfrom   flask   import   Flask,render  template,request,jsonify\n\n#设置参数\n\ndata_path                              ='../data/ids'#  数据路径\n\nembedding_dim        =256        #  词嵌入维度\n\nhidden_dim       =512        # 隐藏层神经元个数\n\ncheckpoint_path                        ='../tmp/model'#   模型参数保存的路径\n\nMAX_LENGTH   =50   #  句子的最大词长\n\nCONST={'_BOS':0,'_EOS':1,'_PAD':2,'_UNK':3}\n\n#   聊天预测\n\ndef   chat(sentence='你好!):\n\n#初始化所有词语的哈希表\n\ntable                        =t up. Fl即e ntai希表\n\nos.path.join(data_path,'all_dict.txt'),\n\ntf.string,\n\ntf.lookup.TextFileIndex.WHOLE_LINE,\n\n     tf.int64,\n\ntf.lookup.TextFileIndex.LINE_NUMBER\n\n),#要使用的表初始化程序。有关支持的键和值类型请参见哈希表内核\n\ndefault_value=CONST[’_UNK']-len(CONST)#                       表中缺少键时使用的值\n\n)\n\n#实例化编码器和解码器\n\n215\n\nencoder\n\nhidden_dim)\n\n=Encoder(table.size().numpy()+len(CONST),embedding_dim,\n\nPython 中文自然语言处理基础与实战\n\n216\n\ndecoder\n\nhidden_dim)\n\noptimizer\n\n=Decoder(table.size().numpy()+len(CONST),embedding_dim,\n\n=tf.keras.optimizers.Adam()# 优化器\n\n=tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,\n\ndecoder=decoder)\n\n#导入训练参数\n\ncheckpoint.restore(tf.train.latest   checkpoint(checkpoint  path)) #给句子添加开始和结束标记\n\nsentence                                      =!BOS!+sentence                                       +'EOS                                          \n\n#读取字段\n\nwith                                                                      open(os.path.join(data_path,'all_dict.txt'),'r',encoding='utf-8')\n\nas            f:                  all   dict =f.read().split()\n\n#构建词 →id  的映射字典\n\nword2id            ={j:i+len(CONST)for            i,j            in             enumerate(all_dict)}\n\nword2id.update(CONST)\n\n# 构 建id→  词的映射字典\n\nid2word=dict(zip(word2id.values(),word2id.keys()))\n\n#分词时保留_EOS  和 _BOS\n\nfor i in ['   EOS','  BOS']:\n\nadd_word(i)\n\ni ,NST['_UNK'])for  i  in  lcut(sentence)]\n\n#长度填充\n\ninputs                                                =tf.keras.preprocessing.sequence.pad_sequences(\n\n[inputs],maxlen=MAX_LENGTH,padding='post',value=CONST['_PAD']) #将数据转为TensorFlow    的数据类型\n\ninputs                                  =tf.convert_to_tensor(inputs)\n\n#空字符串，用于保留预测结果\n\nresult                  =!\n\n# 编 码\n\ncc___eiddeencoder(inputs)\n\ndec_input        =tf.expand_dims([word2id['_BOS']],0)\n\nfor     t      in     range(MAX_LENGTH):\n\nctions,dec_hidden,attention_weights    =decoder(dec_input,\n\ndec_hidden,enc_out)\n\nd语_rgmax(predictions[0]).numpy()\n\n#通过字典的映射用id  寻找词，遇到_EOS 停止输出\n\nif   id2word.get(predicted   id,'  UNK')=='   EOS':\n\nbreak\n\n出predicted_id,'_UNk')\n\ndec   input   =tf.expand   dims([predicted   id],0)\n\n第  章 智能问答系统\n\nreturn            result  # 返回预测结果\n\n#实例化APP\n\napp                          =Flask(    name     ,static_url_path='/static')\n\n@app.route('/message',methods=['POST'])\n\n#定义应答函数，用于获取输入信息并返回相应的答案\n\ndef reply():\n\ne从q请_form['msg']\n\n_句m使s.j _msg))\n\n# 调 用 chat   对生成回答信息\n\nres_msg                        =chat(req_msg)\n\ne将s__mUNs_用e('_UNK','^_^')\n\nres_msg                  =res_msg.strip()\n\n收_':则给出相应的回复\n\nres_msg      = '   我们来聊聊天吧’\n\nreturn    jsonify({'text':res  msg}\n\n@app.route(\"/\")\n\n#在网页上展示对话\n\ndef              index():\n\nreturn render  template('index.html')\n\n# 启 动APP\n\nif(__n ' 127.0.0.1',port=8808)\n\n运行代码11-19后，需要在浏览器中打开网址“http://127.0.0.1:8808/”(仅在运行代码 11-19后有效),之后会出现一个对话框，如图11-1所示，可以在此对话框中进行人机对话。 需要注意的地方是，在打开网址调用Flask 前端之前，需确保存储代码的路径下有 static 和\n\ntemplates 这两个文件夹和文件夹里面的相关文件。\n\n图11-1 人机对话\n\n217\n\nPython 中文自然语言处理基础与实战\n\n11.3.5    模型评价\n\n在模型训练过程中所使用的损失函数可以计算模型损失值，该值可用于衡量模型效果。 损失值的计算可以通过 tf.keras.losses.SparseCategoricalCrossentropy    函数实现", "metadata": {}}, {"content": "，可以在此对话框中进行人机对话。 需要注意的地方是，在打开网址调用Flask 前端之前，需确保存储代码的路径下有 static 和\n\ntemplates 这两个文件夹和文件夹里面的相关文件。\n\n图11-1 人机对话\n\n217\n\nPython 中文自然语言处理基础与实战\n\n11.3.5    模型评价\n\n在模型训练过程中所使用的损失函数可以计算模型损失值，该值可用于衡量模型效果。 损失值的计算可以通过 tf.keras.losses.SparseCategoricalCrossentropy    函数实现，使用的损失\n\n函数为序列分对数 (logits)的加权交叉熵。\n\n通过对代码11-16 中的模型训练过程结果和损失值进行整理得到的训练过程的损失值 如表11-2所示。可以看到随着训练次数的增加，损失值逐渐减小，同时Inference  推理得到 的回答也接近语料的回答，但回答后面仍包含了许多无意义字符。针对这类情况，用户可 以通过增加训练次数和优化语料库内容等途径优化训练结果。虽然模型的损失值已下降到 了0.0001,可以认为模型效果较好，但是为了得到更好的模型效果，可以在后续的优化中\n\n使用更大的中文对话语料库，同时增加模型训练次数。\n\n表11 - 2 模型训练损失值\n\n训练次数 0 100 200 300 400 500 损失值 0.8269 0.1501 0.0021 0.0005 0.0002 0.0001\n\n注：模型训练次数由0开始计数，共训练501次。\n\n小结\n\n本章主要介绍了智能问答系统的基本概念和主要组成部分。首先讲述了智能问答系 统的发展、分类和应用，接着介绍了问答系统的问题理解、知识检索和答案生成3 个主 要组成部分，以及问答系统的基本流程、原理和所需要使用的技术等，最后实现了基于 Seq2Seq  模型的聊天机器人，其中包括语料预处理、模型构建、模型训练与测试等步骤的\n\n实现和讲解。\n\n实训  基 于Seq2Seq 模型的聊天机器人\n\n1.  训练要点\n\n(1)了解智能问答系统的基本概念。\n\n(2)熟悉智能问答系统的主要组成部分。\n\n(3)通过编程，熟悉智能问答系统的构造流程。\n\n2. 需求说明\n\n需要通过 Seq2Seq  模型，在收集的对话语句的基础上构建聊天机器人。提供的语料数 据是一份 TXT 文件，文件中包含一些常用的对话语句，采用的是一问一答的形式，奇数句 为问句，偶数句为答句。对语料数据进行分词处理，并构建词典，利用基于 Seq2Seq 模型 的聊天机器人进行训练，达到一定训练次数且模型的损失值较小时停止训练，然后利用\n\nFlask 前端对模型进行人机对话测试。\n\n3. 实现思路与步骤\n\n(1)读取语料库文件，利用jieba 分词进行中文分词并构建词典。\n\n(2)将语料库文件中的语句拆分为问答形式，加载词典、数据。\n\n218\n\n第 1 章  智能问答系统\n\n(3)设置模型参数，构建 Seq2Seq模型。\n\n(4)在模型中添加优化器和损失函数，训练模型并保存模型。\n\n(5)调用Flask 前端进行人机对话测试。\n\n课后习题\n\n1. 选择题\n\n(1)问答系统流程由问题理解、(    )、答案生成3个部分组成。\n\nA.  词性标注    B.   关键词提取  C.   问题分类         D.   知识检索\n\n(2)关键词提取最简单、最直观的方法是(    )方法，用于识别定义类查询的句子。\n\nA.  词性标注    B.  命名实体识别 C.   规则的匹配方法  D.  文本分类\n\n(3)不属于基于Seq2Seq模型的聊天机器人的实现流程的是(    )。\n\nA.  读取语料库                  B.  绘画建模\n\nC.  抽取数据中的问答对话集合    D.  添加 Attention 机制\n\n(4)对原始的语料文件进行预处理时要进行(    )。\n\nA.  抽取数据中的问答对话集合    B.   构建词典\n\nC.   模型训练                     D.   模型测试\n\n(5)不是聊天机器人模型优化方法的是(    )。\n\nA.  损失函数     B.   优化器      C.   自适应梯度法     D.   神经网络\n\n2. 操作题\n\n使用公开数据集，实现一个基于Seq2Seq模型的聊天机器人。\n\n219\n\n基于 TipDM  大数据\n\n挖掘建模平台实现垃圾\n\n短信分类\n\n8.5 节中完成垃圾短信分类任务，本章将使用另一种工具——TipDM  大数据挖掘建模 平台实现垃圾短信分类。相较于传统 Python解析器， TipDM大数据挖掘建模平台具有流程\n\n化、去编程化等特点，能够满足不懂编程的用户使用数据分析技术的需求。\n\n学习目标\n\n(1)了解 TipDM 大数据挖掘建模平台的相关概念和特点。\n\n(2)熟悉使用TipDM 大数据挖掘建模平台实现垃圾短信分类的总体流程。\n\n(3)熟悉使用 TipDM 大数据挖掘建模平台进行数据去重、数据脱敏、数据筛选等\n\n操作。\n\n12.1  平台简介\n\nTipDM  大数据挖掘建模平台是由广东泰迪智能科技股份有限公司自主研发，面向大 数据挖掘项目的工具。平台使用Java 语言开发，采用 B/S结构，用户不需要下载客户端， 通过浏览器即可进行访问。平台提供了基于Python 、R 语言和 Hadoop/Spark 分布式引擎 的大数据分析功能。平台支持工作流，用户可在没有 Scala 、Python 、R 语言等编程语言 基础的情况下，通过拖曳的方式进行操作，以流程化的方式将数据输入与输出、统计分 析、数据预处理、分析与建模等环节进行连接，从而达成大数据分析的目的。平台的界\n\n面如图12-1所示。\n\n读者可通过访问平台查看具体的界面情况，访问平台的具体步骤如下。\n\n(1)微信搜索并关注公众号 “TipDataMining”。\n\n(2)回复“建模平台”,获取平台访问方式。\n\n本章将以垃圾短信分类案例为例，介绍使用平台实现案例的流程。在介绍之前，需要\n\n引入平台的几个概念。\n\n(1)算法。对建模过程涉及的输入/输出、数据探索与预处理、建模、模型评估等算法\n\n分别进行封装，每一个封装好的算法模块称为算法。\n\n第②章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n图12-1 平台的界面\n\n(2)实训。为实现某一数据分析目标，将各算法通过流程化的方式进行连接，整个数\n\n据分析流程称为一个实训。\n\n(3)模板。用户可以将配置好的实训通过模板的方式分享给其他用户，其他用户可以\n\n使用该模板创建一个无须配置算法便可运行的实训。\n\nTipDM 大数据挖掘建模平台主要有以下几个特点。\n\n(1)平台算法基于Python 、R  语言和 Hadoop/Spark  分布式引擎，用于数据分析。 Python 、R 语言和 Hadoop/Spark  是目前最为流行的用于数据分析的语言，高度契合行\n\n业需求。\n\n(2)用户可在没有Python、R 语言或者 Hadoop/Spark 编程基础的情况下，使用直观的\n\n拖曳式图形界面构建数据分析流程，无须编程。\n\n(3)提供公开可用的数据分析示例实训， 一键即可创建并快速运行实训。支持挖掘流\n\n程每个节点的结果的在线预览。\n\n(4)Python    算法包可分为10类：统计分析、预处理、脚本、分类、聚类、回归、时间 序列、关联规则、文本分析、绘图。Spark 算法包可分为6类：预处理、统计分析、分类、 聚类、回归、协同过滤。R 语言算法包可分为8类：统计分析、预处理、脚本、分类、聚\n\n类、回归、时间序列、关联分析。\n\n下面将对平台中的“实训库”“数据连接”“实训数据”“我的实训”“系统算法”“个人\n\n算法”6个模块进行介绍。\n\n12.1.1  实 训 库\n\n登录平台后，用户即可看到“实训库”模块提供的示例实训(模板),如图12-2所示。\n\n“实训库”模块主要用于标准大数据分析案例的快速创建和展示。通过“实训库”模块， 用户可以创建一个无须导入数据和配置参数就能够快速运行的实训。同时，每一个模板的 创建者都拥有模板的所有权，能够对模板进行管理。用户可以将自己搭建的数据分析实训\n\n生成为模板，并显示在“实训库”模块", "metadata": {}}, {"content": "，用户即可看到“实训库”模块提供的示例实训(模板),如图12-2所示。\n\n“实训库”模块主要用于标准大数据分析案例的快速创建和展示。通过“实训库”模块， 用户可以创建一个无须导入数据和配置参数就能够快速运行的实训。同时，每一个模板的 创建者都拥有模板的所有权，能够对模板进行管理。用户可以将自己搭建的数据分析实训\n\n生成为模板，并显示在“实训库”模块，供其他用户一键创建。\n\n221\n\nPython 中文自然语言处理基础与实战\n\n图12-2 示例实训\n\n12.1.2  数据连接\n\n“数据连接”模块支持从DB2 、SQL Server 、MySQL 、Oracle 、PostgreSQL 等常用关系\n\n数据库导入数据，如图12-3所示。\n\n图12-3 连接数据库\n\n12.1.3  实训数据\n\n“实训数据”模块主要用于数据分析实训的数据导入与管理，支持从本地导入任意类型\n\n数据，如图12-4所示。\n\n222\n\n第②章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n图12-4  新增数据集\n\n12.1.4  我的实训\n\n“我的实训”模块主要用于数据分析流程化的创建与管理，其示例实训如图12-5所 示。通过其中的“实训”模块，用户可以创建空白实训，进行数据分析实训的配置，将 数据输入与输出、数据预处理、挖掘建模、模型评估等环节通过流程化的方式进行连接， 从而达到数据分析的目的。对于完成的优秀实训，可以将其保存为模板，供其他使用者\n\n学习和借鉴。\n\n图12-5 “我的实训”模块的示例实训\n\n12.1.5  系统算法\n\n“系统算法”模块主要用于大数据分析内置常用算法的管理，提供Python 、R 语言、Spark\n\n223\n\nPython 中文自然语言处理基础与实战\n\n这3种算法包。\n\nPython算法包可分为10类，具体如下。\n\n(1)统计分析类提供对数据整体情况进行统计的常用算法，包括因子分析、全表统计、\n\n正态性检验、相关性分析、卡方检验、主成分分析和频数统计等。\n\n(2)预处理类提供对数据进行清洗的算法，包括数据标准化、缺失值处理、表堆叠、  数据筛选、行列转置、修改列名、衍生变量、数据拆分、主键合并、新增序列、数据排序、\n\n数据采样、记录去重和分组聚合等。\n\n(3)脚本类提供一个 Python代码编辑框。用户可以在代码编辑框中粘贴已经写好的程\n\n序代码并直接运行，无须再额外配置成算法等。\n\n(4)分类类提供常用的分类算法，包括朴素贝叶斯、支持向量机、CART 分类树、逻\n\n辑回归、神经网络和K 最近邻等。\n\n(5)聚类类提供常用的聚类算法，包括层次聚类、DBSCAN 密度聚类和 K-means等。\n\n(6)回归类提供常用的回归算法，包括 CART回归树、线性回归、支持向量回归和K\n\n最近邻回归等。\n\n(7)时间序列类提供常用的时间序列算法，包括ARIMA 等。\n\n(8)关联规则类提供常用的关联规则算法，包括 Apriori和 FP-Growth等。\n\n(9)文本分析类提供对文本数据进行清洗、特征提取与分析的常用算法，包括TextCNN 、 Seq2Seq、jieba分词、HanLP 分词与词性、TF-IDF、Doc2Vec、Word2Vec 、LDA 、TextRank、\n\n脱敏、去停用词、分句、正则匹配和 HanLP 实体提取等。\n\n(10)绘图类提供常用的画图算法，包括柱形图、折线图、散点图、饼图和词云图等。\n\nSpark算法包可分为6类，具体如下。\n\n(1)预处理类提供对数据进行清洗的算法，包括数据去重、数据过滤、数据映射、数  据反映射、数据拆分、数据排序、缺失值处理、数据标准化、衍生变量、表连接、表堆叠、\n\n哑变量和数据离散化等。\n\n(2)统计分析类提供对数据整体情况进行统计的常用算法，包括行列统计、全表统计、\n\n相关性分析和卡方检验等。\n\n(3)分类类提供常用的分类算法，包括逻辑回归、决策树、梯度提升树、朴素贝叶斯、\n\n随机森林、线性支持向量机和多层感知神经网络等。\n\n(4)聚类类提供常用的聚类算法，包括K-means聚类、二分K 均值聚类和混合高斯模型等。\n\n(5)回归类提供常用的回归算法，包括线性回归、广义线性回归、决策树回归、梯度\n\n提升树回归、随机森林回归和保序回归等。\n\n(6)协同过滤类提供常用的智能推荐算法，包括ALS 算法等。\n\nR 语言算法包可分为8大类，具体如下。\n\n(1)统计分析类提供对数据整体情况进行统计的常用算法，包括卡方检验、因子分析、\n\n主成分分析、相关性分析、正态性检验和全表统计等。\n\n(2)预处理类提供对数据进行清洗的算法，包括缺失值处理、异常值处理、表连接、 表堆叠、数据标准化、记录去重、数据离散化、排序、数据拆分、频数统计、新增序列、\n\n字符串拆分、字符串拼接、修改列名和衍生变量等。\n\n224\n\n第②章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n(3)脚本类提供一个R 语言代码编辑框。用户可以在代码编辑框中粘贴已经写好的程\n\n序代码并直接运行，无须再额外配置成算法。\n\n(4)分类类提供常用的分类算法，包括朴素贝叶斯、CART 分类树、C4.5 分类树、BP\n\n神经网络、KNN 、SVM  和逻辑回归等。\n\n(5)聚类类提供常用的聚类算法，包括K-means、DBSCAN    和系统聚类等。\n\n(6)回归类提供常用的回归算法，包括CART 回归树、C4.5 回归树、线性回归、岭回\n\n归和 KNN 回归等。\n\n(7)时间序列类提供常用的时间序列算法，包括ARIMA、GM(1,1)  和指数平滑等。\n\n(8)关联分析类提供常用的关联规则算法，包括Apriori 等。\n\n平台提供的系统算法如图12-6所示。\n\n图12-6 平台提供的系统算法\n\n12.1.6  个人算法\n\n“个人算法”模块主要用于满足用户的个性化需求。在使用过程中，用户可根据自己的需\n\n求定制算法，方便使用。目前支持通过Python 和 R 语言进行个人算法的定制，如图12-7所示。\n\n图12-7 定制个人算法\n\n225\n\nPython 中文自然语言处理基础与实战\n\n12.2 实现垃圾短信分类\n\n本节以垃圾短信分类案例为例，在 TipDM 大数据挖掘建模平台上配置对应工程，展示\n\n几个主要的配置过程。详细的配置过程可访问平台进行查看。\n\n在 TipDM 大数据挖掘建模平台上配置垃圾短信分类案例的总体流程如图12-8所示，\n\n主要包括以下4个步骤。\n\n(1)数据读取。在 TipDM 大数据挖掘建模平台导入并读取80万条短信数据。\n\n(2)文本预处理。对原始数据进行预处理，并进行缺失值检测、去重、脱敏、分词、\n\n去停用词、词频统计等操作。\n\n(3)模型构建与训练。采用自定义朴素贝叶斯函数，实现朴素贝叶斯分类，将最终结\n\n果与测试集进行比较，得到模型的分类情况和准确率。\n\n(4)模型评价。使用处理好的测试集进行预测，对比真实值与预测值，获得准确率并\n\n进行结果分析。\n\n图12- 8  配置垃圾短信分类案例的总体流程\n\n在平台上配置得到的最终流程如图12-9所示。\n\n图12-9 最终流程\n\n226\n\n第②章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n12.2.1  数据源配置\n\n案例的数据为一份短信数据 (CSV  文件)、两个自建词库(TXT   文件)和一张背景图 片 (JPG  文件)。在 TipDM 大数据挖掘建模平台中导入这些文件的方式类似，这里以 CSV\n\n文件为例", "metadata": {}}, {"content": "，这里以 CSV\n\n文件为例，步骤如下。\n\n(1)新增数据集。单击“实训数据”,在“我的数据集”选项卡下单击“新增数据集”,\n\n如图12-10所示。\n\n图12-10 单击“新增数据集”\n\n(2)配置新增数据集参数。随意选择一张封面图片，在“名称”中输入“自然语言处 理”,“有效期(天)”项选择为“永久”,“描述”中输入“自然语言处理”,“访问权限”项 选择为“私有”,单击“点击上传”选择 “message80W.csv”  文件，如图12-11 所示。单击\n\n“确定”按钮，即可上传。\n\n图12-11 配置新增数据集参数\n\n227\n\nPython 中文自然语言处理基础与实战\n\n数据上传完成后，新建一个名为“自然语言处理”的空白工程，配置“输入源”算法，\n\n步骤如下。\n\n(1)添加“输入源”算法。在“实训”下方的“算法”栏中，找到“系统算法”模块\n\n下的“输入/输出”类。拖曳“输入/输出”类中的“输入源”算法至工程画布中。\n\n(2)配置“输入源”算法。单击画布中的“输入源”算法，然后单击工程画布右侧“参 数配置”栏中的“数据集”框，输入“自然语言处理”,或在弹出的下拉列表框中选择“自 然语言处理”,并勾选 “message80W.csv” 项，如图12-12所示。右击“输入源”算法，选\n\n择“重命名”并输入 “message”。\n\n图12-12 配置“输入源”算法\n\n(3)预览短信数据。单击画布中的 “message”  算法，在工程画布右侧“参数配置”栏\n\n中，单击“文件列表”项下的●图标查看数据集明细，如图12-13所示。\n\n图12-13 预览短信数据\n\n可以发现，数据列名不符合要求，需要进行修改，步骤如下。\n\n(1)添加“修改列名”算法。拖曳“系统算法”模块下的“预处理”类中的“修改列\n\n名”算法至工程画布中，并与 “message”  算法相连接。\n\n(2)配置“修改列名”算法。在“列索引名”中输入 “index,类别，短信”,如图12-14\n\n所示。\n\n(3)运行“修改列名”算法。右击“修改列名”算法，选择“运行该节点”,如图12-15\n\n所示。\n\n228\n\n第②章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n图12-14 配置“修改列名”算法\n\n图12-15 运行“修改列名”算法\n\n12.2.2 文本预处理\n\n本案例的文本预处理主要是对短信数据进行数据采样、缺失值处理、数据去重、文本\n\n数据脱敏、jieba 分词、去停用词、表堆叠、数据筛选、词频统计等操作。\n\n1. 数据采样\n\n由于原始数据量过大，为了方便后续建模与分类，采用简单随机抽样的方式抽取1%\n\n的数据，步骤如下。\n\n(1)添加“数据采样”算法。拖曳“系统算法”模块下的“预处理”类中的“数据采\n\n样”算法至工程画布中，并与“修改列名”算法相连接。\n\n(2)配置“数据采样”算法。在“采样比例”中输入“0.01”,如图12-16所示。\n\n图12-16  配置“数据采样”算法\n\n(3)运行“数据采样”算法。\n\n2. 缺失值处理\n\n由于建模数据不允许存在缺失值，因此需要进行缺失值检测。在平台中可通过“缺失\n\n229\n\nPython 中文自然语言处理基础与实战\n\n值处理”算法实现缺失值的检测并进行缺失值处理，步骤如下。\n\n(1)添加“缺失值处理”算法。拖曳“系统算法”模块下的“预处理”类中的“缺失\n\n值处理”算法至工程画布中，并与“数据采样”算法相连接。\n\n(2)配置“缺失值处理”算法。在字段设置中，单击“特征”项旁的一图标，选 择全部字段，如图12-17所示；在“参数设置”中，选择“处理缺失值方式”为“按行\n\n删除”。\n\n图12-17 配置“缺失值处理”算法\n\n(3)运行“缺失值处理”算法。运行成功后，右击“缺失值处理”算法，选择“查看\n\n日志”,缺失值处理结果如图12-18所示。\n\n图12-18  缺失值处理结果\n\n3. 数据去重\n\n由于重复记录数会对模型的精度造成影响，因此需要对数据进行去重操作，步骤如下。\n\n(1)添加“记录去重”算法。拖曳“系统算法”模块下的“预处理”类中的“记录去\n\n重”算法至工程画布中，并与“缺失值处理”算法相连接，如图12-19所示。\n\n(2)配置“记录去重”算法。在“字段设置”中，单击“特征”项旁的<图标，选择 全部字段。单击“根据哪些特征去重”项旁的心图标，选择全部字段。在“参数设置”中，\n\n选择“去重方式”为 “False”。\n\n230\n\n第②章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n实训 算法 中 Q 数据采样 200%                  图       D 字段设置 参数设警 去重方式 False 请输入算法名搜索 清空文件天 创建文件夹 行列转置 记录去用 主键合并 表堆叠 缺失值处理 免   记录去重\n\n图12-19 添加“记录去重”算法\n\n(3)运行“记录去重”算法。运行成功后，右击“记录去重”算法，选择“查看日志”\n\n查看记录去重结果。\n\n4. 文本数据脱敏\n\n由于原始数据中的敏感信息已用统一字符替换，因此进行脱敏时只需去掉相应的字符\n\n即可，步骤如下。\n\n(1)添加“脱敏”算法。拖曳“系统算法”模块下的“文本分析”类中的“文本预处\n\n理”子类中的“脱敏”算法至工程画布中，并与“记录去重”算法相连接。\n\n(2)配置“脱敏”算法。单击“特征”项旁的一图标，选择“短信”字段，如图12-20\n\n所示。\n\n图12-20 配置“脱敏”算法\n\n(3)运行“脱敏”算法。\n\n5.jieba   分词\n\n采用jieba 分词来切分短信内容，由于分词的过程中会将部分有用信息切分开，因此需 要加载自定义词典newdic1.txt来避免过度分词，文件中包含短信内容的几个重要词汇。jieba\n\n分词步骤如下。\n\n(1)配置“输入源”算法。将“输入源”算法拖曳至工程画布中，并重命名为 “newdic1”,\n\n导入自定义词典。\n\n(2)添加 “jieba 分词”算法。拖曳“系统算法”模块下的“文本分析”类中的“文本 预处理”子类中的 “jicba 分词”算法至工程画布中，并与 “newdic1”  算法和“脱敏”算法\n\n相连接。\n\n231\n\nPython 中文自然语言处理基础与实战\n\n(3)配置 “jieba 分词”算法。单击“特征”项旁的心图标，选择“短信”字段，如\n\n图12-21所示。\n\n图12-21 配置 “jieba分词”算法\n\n(4)运行 “jieba 分词”算法。\n\n6. 去停用词\n\n对分词后的结果去停用词，步骤如下。\n\n(1)配置“输入源”算法。将“输入源”算法拖曳至工程画布中，并重命名为 “stopword”\n\n导入“停用词”数据。\n\n(2)添加“去停用词”算法。拖曳“系统算法”模块下的“文本分析”类中的“文本 预处理”子类的“去停用词”算法至工程画布中，并与“jieba 分词”算法和 “stopword”\n\n算法相连接。\n\n(3)配置“去停用词”算法。单击“选择需要去停用词的字段”旁的  图标", "metadata": {}}, {"content": "，并重命名为 “stopword”\n\n导入“停用词”数据。\n\n(2)添加“去停用词”算法。拖曳“系统算法”模块下的“文本分析”类中的“文本 预处理”子类的“去停用词”算法至工程画布中，并与“jieba 分词”算法和 “stopword”\n\n算法相连接。\n\n(3)配置“去停用词”算法。单击“选择需要去停用词的字段”旁的  图标，选择“短\n\n信”字段，如图12-22所示。\n\n图12-22  配置“去停用词”算法\n\n(4)运行“去停用词”算法。\n\n7. 表堆叠\n\n预览数据可以发现，分词后的结果不存在类别标签数据，需要进行数据合并，步骤\n\n如下。\n\n(1)添加“表堆叠”算法。拖曳“系统算法”模块下的“预处理”类中的“表堆叠”\n\n232\n\n第 2 章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n算法至工程画布中，并与“去停用词”和“记录去重”算法相连接。\n\n(2)配置“表堆叠”算法。单击“表1特征”项旁的～图标，选择“短信”字段。单 击“表2特征”项旁的～图标，选择“类别”字段。在参数设置中，选择“合并方式”为\n\n“按列合并”,如图12-23所示。\n\n实训             + 塑法 请输入算法名搜索 记录去重 主键合并 表底障 最分组聚合 新增序列 Q     200%       @          图 jieba分词                      stopword 记录去重 去停用词 表堆叠 字段设置 参数设露 合并方式 按列合并\n\n图12-23 配置“表堆叠”算法\n\n(3)运行“表堆叠”算法。\n\n8. 数据筛选\n\n对垃圾短信和非垃圾短信的特征进行分析，需要将数据根据类别进行筛选，步骤\n\n如下。\n\n(1)添加“数据筛选”算法。拖曳“系统算法”模块下的“预处理”类中的“数据筛\n\n选”算法至工程画布中，与“表堆叠”算法相连接，并重命名为“非垃圾短信数据”。\n\n(2)配置“非垃圾短信数据”算法。在“字段设置”中，单击“特征”项旁的一图标， 选择全部字段。在“过滤条件1”中，选择“过滤的列”为“类别”,设置“表达式”为“等\n\n于”,设置“过滤条件的比较值“为“0”,筛选非垃圾短信数据，如图12-24所示。\n\n实训 法 + 数据 数据编码化 数据采样 数据标准化 数据拆分 数的和选 数捆排序 Q 非垃圾短信数据 200% 表堆叠 图 字段设置 过涉氧件1 类别 表达式 等于 过滤条件的比较值 0 图像处理\n\n图12-24 配置“非垃圾短信数据”算法\n\n(3)添加“数据筛选”算法。再次将“数据筛选”算法拖曳至工程画布中，并重命名\n\n为“垃圾短信数据”。\n\n(4)配置“垃圾短信数据”算法。将“过滤条件的比较值”设置为“1”,其余设置与\n\n步骤(2)相同。\n\n(5)运行“垃圾短信数据”和“非垃圾短信数据”算法。\n\n233\n\nPython 中文自然语言处理基础与实战\n\n9. 词频统计\n\n这里通过自定义函数来统计词频，将空格作为词与词之间的分隔符，整合得到一个词 汇序列后进行切分，统计每个词出现的频次。垃圾短信和非垃圾短信均保留词频大于5的\n\n词。分别对垃圾短信数据与非垃圾短信数据绘制词云图，查看短信内容分布情况。\n\n绘制垃圾短信数据词云图的步骤如下。\n\n(1)配置“输入源”算法。将“输入源”算法拖曳至工程画布中，并重命名为“背景\n\n图片”,导入词云图背景图片数据。\n\n(2)添加“词云图”算法。拖曳“系统算法”模块下的“绘图”类中的“词云图”算\n\n法至工程画布中，与“非垃圾短信数据”算法相连接，并重命名为“词云图1”。\n\n(3)配置“词云图1”算法。单击“特征”项旁的一图标，选择“短信”字段。在“词 云图设置”中保留默认设置，在“图片模板设置”中，选择“是否使用图片中的颜色”为\n\n“是”,如图12-25所示。\n\n实训             + 算法 词 基于词向量/文 文本预处理 去停用词 数据可视化 词云图 R语言算法 V Jieba分词 去除停用词 Q        300% 表堆蟹 背景图片 非垃圾短信数据 词云图： 图 ○ 垃圾短信数据 参数配置 词云图设置 超片根板设裂 是否使用图片中的颜售 是\n\n图12-25 配置“词云图1”算法\n\n(4)运行“词云图1”算法。\n\n(5)配置“词云图2”算法。再次将“词云图”算法拖曳至工程画布中并重命名为“词\n\n云图2”,其余操作与绘制非垃圾短信词云图的操作相同。\n\n12.2.3  朴素贝叶斯分类模型\n\n自定义的朴素贝叶斯函数有5个步骤，其中包含5个自定义函数。\n\n(1)loadDataSet     函数用于加载数据。按照8:2的比例采用简单随机抽样来划分训练集 和测试集，将数据集中的标签和短信内容两列内容拆分开，生成训练集和测试集共4个变 量。函数包括3个输入参数，前两个参数是数据截取范围，范围在0到20000之间，可以 在采样后的数据中根据需要再截取一部分数据，也可以直接选取所有数据，最后一个参数\n\n则是待选取的数据集。\n\n(2)createVocabList      函数用于生成词库。它以空格为分隔符将训练集的短信内容拆分\n\n开，从而得到词汇并生成一个词库。\n\n(3)setWordsVec     函数用于生成词频向量矩阵。在词库的基础上统计训练集或测试集\n\n中每个词出现的频次，得到一个词频向量矩阵，矩阵中每个数字代表对应位置词语的出\n\n234\n\n第 ② 章  基于 TipDM 大数据挖掘建模平台实现垃圾短信分类\n\n现频次。\n\n(4)trainNB 和 classifyNB 函数根据朴素贝叶斯算法原理，计算每个样本的条件概率，\n\n定义判断所属分类的条件。\n\n(5)最后调用上述5个函数构建朴素贝叶斯分类器 testingNB。\n\n朴素贝叶斯分类可通过 TipDM 平台的文本分类组件实现，步骤如下。\n\n(1)添加“朴素贝叶斯分类”算法。拖曳“系统算法”模块下的“文本分析”类中 的“文本分类”子类中的“朴素贝叶斯分类”算法至工程画布中，并与“表堆叠”算法\n\n相连接。\n\n(2)查看“朴素贝叶斯分类”算法的描述，如图12-26所示，提示需输入固定格式的\n\n数据，数据输入要求为两列，第一列为经过分词后的文本列，第二列为目标列。\n\n灾瑚             + 骑法 请确入算法名接索 ▶文本增理 文本分类 参扑要负叶斯分 文本证移学习 图像处理 图像识别 获取图像颜色矩 图像切割 图片处理 拉警拉斯锐化 图片识别 滤液 理想低通滤波 Q 230%                  图 O 强法描述 数据输入为两列，第一列为经过分词后的 文本列，第二列为目标列 表堆盈 朴素贝叶斯分类 背魔图片 非垃圾短信数据                                     垃圾短信数据 词云图1 民 词云图2\n\n图12-26 查看“朴素贝叶斯分类”算法的描述\n\n(3)预览“表堆叠”算法输出的数据格式。预览后发现“表堆叠”算法输出的数据格\n\n式与“朴素贝叶斯分类”算法所要求的数据格式一致。\n\n(4)运行“朴素贝叶斯分类”算法。运行成功后，右击“朴素贝叶斯分类”算法，选\n\n择“查看日志”。\n\n小结\n\n本章介绍了如何在 TipDM 大数据挖掘建模平台上配置垃圾短信分类案例的过程，从获 取数据到数据预处理", "metadata": {}}, {"content": "，右击“朴素贝叶斯分类”算法，选\n\n择“查看日志”。\n\n小结\n\n本章介绍了如何在 TipDM 大数据挖掘建模平台上配置垃圾短信分类案例的过程，从获 取数据到数据预处理，再到数据建模，向读者展示了平台流程化的思维，使读者加深了对 数据分析流程的理解。同时，平台去编程、拖曳式的操作方便了没有 Python 、Spark  编程\n\n基础的用户轻松构建数据分析流程，从而完成数据分析任务。\n\n实训  实现基于朴素贝叶斯的新闻分类\n\n1.  训练要点\n\n掌握使用TipDM大数据挖掘建模平台实现文本分类的方法。\n\n235\n\nPython 中文自然语言处理基础与实战\n\n2. 需求说明\n\n参照第8章的实训1,在TipDM 大数据挖掘建模平台实现基于朴素贝叶斯的新闻分类。\n\n3. 实现思路与步骤\n\n(1)配置数据源，导入新闻文本数据。\n\n(2)对导入的新闻文本数据进行预处理。\n\n(3)使用自定义的朴素贝叶斯分类模型对新闻文本进行分类。\n\n课后习题\n\n操作题\n\n参考正文中垃圾短信分类的流程，在 TipDM 大数据挖掘建模平台上使用其他分类算法\n\n实现垃圾短信分类。\n\n236\n\n大数据技术精品系列教材\n\n人工智能导论\n\n数学基础           数据库基础\n\n人工智能技术基础\n\n自然语言处理\n\n数据标注\n\n机器学习\n\n深度学习\n\n计算机视觉\n\n语音识别\n\n扫此二维码下载 本书配套资源\n\n封面设计：董志桢\n\n教材服务热线：010-81055256\n\n反馈/投稿/推荐信箱：315@ptpress.com.cn\n\n人邮教育服务与资源下载社区： www.ryjiaoyu.com\n\n定价：59.80元", "metadata": {}}]